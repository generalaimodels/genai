{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Probability and Statistics for Generative AI\n",
        "\n",
        "## Table of Contents\n",
        "1. [Foundations of Probability Theory](#1-foundations-of-probability-theory)\n",
        "2. [Random Variables and Distributions](#2-random-variables-and-distributions)\n",
        "3. [Key Probability Distributions in Gen-AI](#3-key-probability-distributions-in-gen-ai)\n",
        "4. [Expectation, Moments, and Covariance](#4-expectation-moments-and-covariance)\n",
        "5. [Information Theory Foundations](#5-information-theory-foundations)\n",
        "6. [Parameter Estimation Methods](#6-parameter-estimation-methods)\n",
        "7. [Latent Variable Models](#7-latent-variable-models)\n",
        "8. [Sampling Methods for Generative Models](#8-sampling-methods-for-generative-models)\n",
        "9. [Probabilistic Framework of Gen-AI Architectures](#9-probabilistic-framework-of-gen-ai-architectures)\n",
        "10. [Statistical Learning Theory](#10-statistical-learning-theory)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Foundations of Probability Theory\n",
        "\n",
        "### 1.1 Definition of Probability\n",
        "\n",
        "**Definition:** Probability is a mathematical framework for quantifying uncertainty, assigning a numerical value between 0 and 1 to events, representing the likelihood of their occurrence.\n",
        "\n",
        "In Generative AI, probability provides the theoretical foundation for:\n",
        "- Modeling data distributions\n",
        "- Generating new samples\n",
        "- Quantifying uncertainty in predictions\n",
        "- Training models via likelihood maximization\n",
        "\n",
        "### 1.2 Probability Axioms (Kolmogorov Axioms)\n",
        "\n",
        "For a sample space $\\Omega$ and event $A$:\n",
        "\n",
        "$$P: \\mathcal{F} \\rightarrow [0,1]$$\n",
        "\n",
        "**Axiom 1 (Non-negativity):**\n",
        "$$P(A) \\geq 0 \\quad \\forall A \\in \\mathcal{F}$$\n",
        "\n",
        "**Axiom 2 (Normalization):**\n",
        "$$P(\\Omega) = 1$$\n",
        "\n",
        "**Axiom 3 (Countable Additivity):**\n",
        "$$P\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} P(A_i) \\quad \\text{for mutually exclusive } A_i$$\n",
        "\n",
        "### 1.3 Conditional Probability\n",
        "\n",
        "**Definition:** The probability of event $A$ occurring given that event $B$ has occurred.\n",
        "\n",
        "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) > 0$$\n",
        "\n",
        "**Relevance to Gen-AI:**\n",
        "- Autoregressive models (GPT, LLaMA) model $P(x_t | x_1, x_2, ..., x_{t-1})$\n",
        "- Conditional generation: $P(\\text{output}|\\text{prompt})$\n",
        "\n",
        "### 1.4 Chain Rule of Probability\n",
        "\n",
        "For sequence of random variables $X_1, X_2, ..., X_n$:\n",
        "\n",
        "$$P(X_1, X_2, ..., X_n) = P(X_1) \\prod_{i=2}^{n} P(X_i | X_1, ..., X_{i-1})$$\n",
        "\n",
        "**Application in Autoregressive Language Models:**\n",
        "$$P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t | w_1, ..., w_{t-1})$$\n",
        "\n",
        "### 1.5 Bayes' Theorem\n",
        "\n",
        "**Definition:** Relates conditional probabilities between events.\n",
        "\n",
        "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
        "\n",
        "**General Form:**\n",
        "$$P(\\theta | \\mathcal{D}) = \\frac{P(\\mathcal{D} | \\theta) \\cdot P(\\theta)}{P(\\mathcal{D})}$$\n",
        "\n",
        "| Component | Term | Role in Gen-AI |\n",
        "|-----------|------|----------------|\n",
        "| $P(\\theta \\| \\mathcal{D})$ | Posterior | Updated belief after seeing data |\n",
        "| $P(\\mathcal{D} \\| \\theta)$ | Likelihood | How well model explains data |\n",
        "| $P(\\theta)$ | Prior | Initial belief about parameters |\n",
        "| $P(\\mathcal{D})$ | Evidence | Normalization constant |\n",
        "\n",
        "### 1.6 Independence and Conditional Independence\n",
        "\n",
        "**Statistical Independence:**\n",
        "$$P(A \\cap B) = P(A) \\cdot P(B)$$\n",
        "\n",
        "**Conditional Independence:**\n",
        "$$P(A, B | C) = P(A|C) \\cdot P(B|C)$$\n",
        "\n",
        "**Notation:** $A \\perp B | C$\n",
        "\n",
        "**Application:** Markov assumption in diffusion models:\n",
        "$$P(x_{t-1} | x_t, x_0) = P(x_{t-1} | x_t)$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Random Variables and Distributions\n",
        "\n",
        "### 2.1 Random Variable Definition\n",
        "\n",
        "**Definition:** A random variable $X$ is a measurable function from sample space $\\Omega$ to real numbers $\\mathbb{R}$.\n",
        "\n",
        "$$X: \\Omega \\rightarrow \\mathbb{R}$$\n",
        "\n",
        "### 2.2 Discrete Random Variables\n",
        "\n",
        "**Probability Mass Function (PMF):**\n",
        "$$P_X(x) = P(X = x)$$\n",
        "\n",
        "**Properties:**\n",
        "- $P_X(x) \\geq 0 \\quad \\forall x$\n",
        "- $\\sum_x P_X(x) = 1$\n",
        "\n",
        "**Application in Gen-AI:**\n",
        "- Token probability distribution in language models\n",
        "- Categorical output selection\n",
        "\n",
        "### 2.3 Continuous Random Variables\n",
        "\n",
        "**Probability Density Function (PDF):**\n",
        "$$f_X(x) \\geq 0, \\quad \\int_{-\\infty}^{\\infty} f_X(x) dx = 1$$\n",
        "\n",
        "**Probability of interval:**\n",
        "$$P(a \\leq X \\leq b) = \\int_a^b f_X(x) dx$$\n",
        "\n",
        "**Application:**\n",
        "- Latent space representations in VAEs\n",
        "- Noise modeling in diffusion models\n",
        "\n",
        "### 2.4 Cumulative Distribution Function (CDF)\n",
        "\n",
        "**Definition:**\n",
        "$$F_X(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f_X(t) dt$$\n",
        "\n",
        "**Properties:**\n",
        "- $F_X(-\\infty) = 0$\n",
        "- $F_X(\\infty) = 1$\n",
        "- Non-decreasing function\n",
        "\n",
        "### 2.5 Joint, Marginal, and Conditional Distributions\n",
        "\n",
        "**Joint Distribution:**\n",
        "$$P(X, Y) \\text{ or } f_{X,Y}(x, y)$$\n",
        "\n",
        "**Marginal Distribution:**\n",
        "$$P(X) = \\sum_y P(X, Y=y) \\quad \\text{(discrete)}$$\n",
        "$$f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x, y) dy \\quad \\text{(continuous)}$$\n",
        "\n",
        "**Conditional Distribution:**\n",
        "$$f_{Y|X}(y|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)}$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Key Probability Distributions in Gen-AI\n",
        "\n",
        "### 3.1 Bernoulli Distribution\n",
        "\n",
        "**Definition:** Distribution over binary outcomes.\n",
        "\n",
        "$$P(X = x) = p^x (1-p)^{1-x}, \\quad x \\in \\{0, 1\\}$$\n",
        "\n",
        "**Parameters:** $p \\in [0, 1]$\n",
        "\n",
        "**Moments:**\n",
        "$$\\mathbb{E}[X] = p, \\quad \\text{Var}(X) = p(1-p)$$\n",
        "\n",
        "### 3.2 Categorical Distribution\n",
        "\n",
        "**Definition:** Generalization of Bernoulli to $K$ categories.\n",
        "\n",
        "$$P(X = k) = \\pi_k, \\quad \\sum_{k=1}^{K} \\pi_k = 1$$\n",
        "\n",
        "**One-hot encoding representation:**\n",
        "$$P(\\mathbf{x}) = \\prod_{k=1}^{K} \\pi_k^{x_k}$$\n",
        "\n",
        "**Application:** Token prediction in language models (softmax output)\n",
        "\n",
        "### 3.3 Multinomial Distribution\n",
        "\n",
        "**Definition:** Distribution over counts from $n$ trials with $K$ categories.\n",
        "\n",
        "$$P(X_1=x_1, ..., X_K=x_K) = \\frac{n!}{\\prod_{k=1}^{K} x_k!} \\prod_{k=1}^{K} \\pi_k^{x_k}$$\n",
        "\n",
        "**Constraint:** $\\sum_{k=1}^{K} x_k = n$\n",
        "\n",
        "### 3.4 Gaussian (Normal) Distribution\n",
        "\n",
        "**Definition:** Continuous distribution with bell-shaped curve.\n",
        "\n",
        "**Univariate:**\n",
        "$$\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
        "\n",
        "**Multivariate:**\n",
        "$$\\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{D/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\right)$$\n",
        "\n",
        "**Critical Importance in Gen-AI:**\n",
        "| Application | Role |\n",
        "|-------------|------|\n",
        "| VAE latent space | Prior distribution $P(\\mathbf{z}) = \\mathcal{N}(0, I)$ |\n",
        "| Diffusion models | Noise distribution at each step |\n",
        "| Weight initialization | Xavier/He initialization |\n",
        "| Dropout noise | Gaussian dropout variant |\n",
        "\n",
        "### 3.5 Exponential Family\n",
        "\n",
        "**Definition:** Unified framework for many distributions.\n",
        "\n",
        "$$P(x | \\boldsymbol{\\eta}) = h(x) \\exp\\left(\\boldsymbol{\\eta}^T \\mathbf{T}(x) - A(\\boldsymbol{\\eta})\\right)$$\n",
        "\n",
        "| Component | Description |\n",
        "|-----------|-------------|\n",
        "| $\\boldsymbol{\\eta}$ | Natural parameters |\n",
        "| $\\mathbf{T}(x)$ | Sufficient statistics |\n",
        "| $A(\\boldsymbol{\\eta})$ | Log-partition function |\n",
        "| $h(x)$ | Base measure |\n",
        "\n",
        "**Members:** Gaussian, Bernoulli, Categorical, Poisson, Gamma, Beta\n",
        "\n",
        "### 3.6 Mixture Models\n",
        "\n",
        "**Definition:** Weighted combination of component distributions.\n",
        "\n",
        "$$P(x) = \\sum_{k=1}^{K} \\pi_k \\cdot P_k(x | \\theta_k)$$\n",
        "\n",
        "**Gaussian Mixture Model (GMM):**\n",
        "$$P(x) = \\sum_{k=1}^{K} \\pi_k \\cdot \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
        "\n",
        "**Latent variable formulation:**\n",
        "$$P(x) = \\sum_{z} P(z) \\cdot P(x|z)$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Expectation, Moments, and Covariance\n",
        "\n",
        "### 4.1 Expected Value\n",
        "\n",
        "**Definition:** The mean or average value of a random variable.\n",
        "\n",
        "**Discrete:**\n",
        "$$\\mathbb{E}[X] = \\sum_x x \\cdot P(X=x)$$\n",
        "\n",
        "**Continuous:**\n",
        "$$\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f_X(x) dx$$\n",
        "\n",
        "**Properties:**\n",
        "- Linearity: $\\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]$\n",
        "- For function $g(X)$: $\\mathbb{E}[g(X)] = \\int g(x) f_X(x) dx$\n",
        "\n",
        "### 4.2 Variance and Standard Deviation\n",
        "\n",
        "**Variance:**\n",
        "$$\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$$\n",
        "\n",
        "**Standard Deviation:**\n",
        "$$\\sigma_X = \\sqrt{\\text{Var}(X)}$$\n",
        "\n",
        "**Properties:**\n",
        "- $\\text{Var}(aX + b) = a^2 \\text{Var}(X)$\n",
        "- For independent $X, Y$: $\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)$\n",
        "\n",
        "### 4.3 Covariance and Correlation\n",
        "\n",
        "**Covariance:**\n",
        "$$\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$$\n",
        "\n",
        "**Covariance Matrix:**\n",
        "$$\\boldsymbol{\\Sigma} = \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T]$$\n",
        "\n",
        "$$\\boldsymbol{\\Sigma}_{ij} = \\text{Cov}(X_i, X_j)$$\n",
        "\n",
        "**Correlation Coefficient:**\n",
        "$$\\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}, \\quad -1 \\leq \\rho \\leq 1$$\n",
        "\n",
        "### 4.4 Higher-Order Moments\n",
        "\n",
        "**$n$-th Moment:**\n",
        "$$\\mu_n = \\mathbb{E}[X^n]$$\n",
        "\n",
        "**$n$-th Central Moment:**\n",
        "$$\\mu'_n = \\mathbb{E}[(X - \\mathbb{E}[X])^n]$$\n",
        "\n",
        "**Skewness (3rd standardized moment):**\n",
        "$$\\gamma_1 = \\frac{\\mathbb{E}[(X-\\mu)^3]}{\\sigma^3}$$\n",
        "\n",
        "**Kurtosis (4th standardized moment):**\n",
        "$$\\gamma_2 = \\frac{\\mathbb{E}[(X-\\mu)^4]}{\\sigma^4}$$\n",
        "\n",
        "### 4.5 Moment Generating Function\n",
        "\n",
        "**Definition:**\n",
        "$$M_X(t) = \\mathbb{E}[e^{tX}]$$\n",
        "\n",
        "**Property:**\n",
        "$$\\mathbb{E}[X^n] = M_X^{(n)}(0) = \\left.\\frac{d^n M_X(t)}{dt^n}\\right|_{t=0}$$\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Information Theory Foundations\n",
        "\n",
        "### 5.1 Entropy\n",
        "\n",
        "**Definition:** Measure of uncertainty or information content in a distribution.\n",
        "\n",
        "**Discrete Entropy:**\n",
        "$$H(X) = -\\sum_x P(x) \\log P(x) = \\mathbb{E}[-\\log P(X)]$$\n",
        "\n",
        "**Differential Entropy (Continuous):**\n",
        "$$H(X) = -\\int f(x) \\log f(x) dx$$\n",
        "\n",
        "**Properties:**\n",
        "- $H(X) \\geq 0$ (discrete case)\n",
        "- Maximum entropy for uniform distribution\n",
        "- $H(X, Y) \\leq H(X) + H(Y)$ (equality iff independent)\n",
        "\n",
        "**Entropy of Common Distributions:**\n",
        "| Distribution | Entropy |\n",
        "|--------------|---------|\n",
        "| Bernoulli($p$) | $-p\\log p - (1-p)\\log(1-p)$ |\n",
        "| Categorical($K$) | $\\leq \\log K$ (max when uniform) |\n",
        "| Gaussian($\\mu, \\sigma^2$) | $\\frac{1}{2}\\log(2\\pi e \\sigma^2)$ |\n",
        "\n",
        "### 5.2 Cross-Entropy\n",
        "\n",
        "**Definition:** Measures the average number of bits needed to encode data from distribution $P$ using a code optimized for distribution $Q$.\n",
        "\n",
        "$$H(P, Q) = -\\sum_x P(x) \\log Q(x) = \\mathbb{E}_{x \\sim P}[-\\log Q(x)]$$\n",
        "\n",
        "**Continuous:**\n",
        "$$H(P, Q) = -\\int p(x) \\log q(x) dx$$\n",
        "\n",
        "**Application in Gen-AI - Cross-Entropy Loss:**\n",
        "$$\\mathcal{L}_{CE} = -\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log \\hat{y}_{ik}$$\n",
        "\n",
        "Where:\n",
        "- $y_{ik}$: true one-hot label\n",
        "- $\\hat{y}_{ik}$: predicted probability\n",
        "\n",
        "### 5.3 Kullback-Leibler (KL) Divergence\n",
        "\n",
        "**Definition:** Measures the difference between two probability distributions.\n",
        "\n",
        "$$D_{KL}(P \\| Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} = \\mathbb{E}_{x \\sim P}\\left[\\log \\frac{P(x)}{Q(x)}\\right]$$\n",
        "\n",
        "**Continuous:**\n",
        "$$D_{KL}(P \\| Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx$$\n",
        "\n",
        "**Properties:**\n",
        "- $D_{KL}(P \\| Q) \\geq 0$ (Gibbs' inequality)\n",
        "- $D_{KL}(P \\| Q) = 0 \\iff P = Q$\n",
        "- **Asymmetric:** $D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$\n",
        "\n",
        "**Relationship:**\n",
        "$$D_{KL}(P \\| Q) = H(P, Q) - H(P)$$\n",
        "\n",
        "**KL Divergence for Gaussians:**\n",
        "$$D_{KL}(\\mathcal{N}(\\mu_1, \\sigma_1^2) \\| \\mathcal{N}(\\mu_2, \\sigma_2^2)) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$$\n",
        "\n",
        "**Multivariate Gaussian:**\n",
        "$$D_{KL}(\\mathcal{N}_1 \\| \\mathcal{N}_2) = \\frac{1}{2}\\left[\\text{tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\mu_2-\\mu_1)^T\\Sigma_2^{-1}(\\mu_2-\\mu_1) - k + \\log\\frac{|\\Sigma_2|}{|\\Sigma_1|}\\right]$$\n",
        "\n",
        "### 5.4 Jensen-Shannon Divergence\n",
        "\n",
        "**Definition:** Symmetric measure of divergence.\n",
        "\n",
        "$$D_{JS}(P \\| Q) = \\frac{1}{2}D_{KL}(P \\| M) + \\frac{1}{2}D_{KL}(Q \\| M)$$\n",
        "\n",
        "Where $M = \\frac{1}{2}(P + Q)$\n",
        "\n",
        "**Properties:**\n",
        "- Symmetric: $D_{JS}(P \\| Q) = D_{JS}(Q \\| P)$\n",
        "- Bounded: $0 \\leq D_{JS} \\leq \\log 2$\n",
        "- $\\sqrt{D_{JS}}$ is a metric\n",
        "\n",
        "**Application:** Original GAN training objective\n",
        "\n",
        "### 5.5 Mutual Information\n",
        "\n",
        "**Definition:** Measures shared information between two random variables.\n",
        "\n",
        "$$I(X; Y) = D_{KL}(P(X,Y) \\| P(X)P(Y))$$\n",
        "\n",
        "$$I(X; Y) = \\sum_x \\sum_y P(x, y) \\log \\frac{P(x, y)}{P(x)P(y)}$$\n",
        "\n",
        "**Alternative Forms:**\n",
        "$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X, Y)$$\n",
        "\n",
        "**Properties:**\n",
        "- $I(X; Y) \\geq 0$\n",
        "- $I(X; Y) = 0 \\iff X \\perp Y$\n",
        "- $I(X; X) = H(X)$\n",
        "\n",
        "**Application in Gen-AI:**\n",
        "- InfoGAN: Maximizing mutual information for disentanglement\n",
        "- Contrastive learning objectives\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Parameter Estimation Methods\n",
        "\n",
        "### 6.1 Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "**Definition:** Find parameters that maximize the probability of observed data.\n",
        "\n",
        "$$\\hat{\\theta}_{MLE} = \\arg\\max_\\theta P(\\mathcal{D} | \\theta) = \\arg\\max_\\theta \\mathcal{L}(\\theta; \\mathcal{D})$$\n",
        "\n",
        "**For i.i.d. data:**\n",
        "$$\\mathcal{L}(\\theta; \\mathcal{D}) = \\prod_{i=1}^{N} P(x_i | \\theta)$$\n",
        "\n",
        "**Log-Likelihood (computationally preferred):**\n",
        "$$\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^{N} \\log P(x_i | \\theta)$$\n",
        "\n",
        "**MLE Equation:**\n",
        "$$\\nabla_\\theta \\ell(\\theta) = 0$$\n",
        "\n",
        "### 6.2 MLE for Common Distributions\n",
        "\n",
        "**Bernoulli:**\n",
        "$$\\hat{p}_{MLE} = \\frac{1}{N}\\sum_{i=1}^{N} x_i$$\n",
        "\n",
        "**Gaussian:**\n",
        "$$\\hat{\\mu}_{MLE} = \\frac{1}{N}\\sum_{i=1}^{N} x_i$$\n",
        "$$\\hat{\\sigma}^2_{MLE} = \\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\hat{\\mu})^2$$\n",
        "\n",
        "**Categorical:**\n",
        "$$\\hat{\\pi}_k = \\frac{N_k}{N}$$\n",
        "\n",
        "Where $N_k$ is count of category $k$\n",
        "\n",
        "### 6.3 MLE in Neural Networks\n",
        "\n",
        "**Negative Log-Likelihood (NLL) Loss:**\n",
        "$$\\mathcal{L}_{NLL} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log P_\\theta(x_i)$$\n",
        "\n",
        "**For classification (Cross-Entropy):**\n",
        "$$\\mathcal{L}_{CE} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log P_\\theta(y_i | x_i)$$\n",
        "\n",
        "**For language models:**\n",
        "$$\\mathcal{L}_{LM} = -\\frac{1}{T}\\sum_{t=1}^{T} \\log P_\\theta(w_t | w_{<t})$$\n",
        "\n",
        "### 6.4 Maximum A Posteriori (MAP) Estimation\n",
        "\n",
        "**Definition:** Incorporates prior knowledge through Bayesian framework.\n",
        "\n",
        "$$\\hat{\\theta}_{MAP} = \\arg\\max_\\theta P(\\theta | \\mathcal{D}) = \\arg\\max_\\theta P(\\mathcal{D} | \\theta) P(\\theta)$$\n",
        "\n",
        "**Log form:**\n",
        "$$\\hat{\\theta}_{MAP} = \\arg\\max_\\theta \\left[\\log P(\\mathcal{D} | \\theta) + \\log P(\\theta)\\right]$$\n",
        "\n",
        "### 6.5 Connection to Regularization\n",
        "\n",
        "**Gaussian Prior → L2 Regularization:**\n",
        "$$P(\\theta) = \\mathcal{N}(0, \\sigma^2 I) \\implies \\log P(\\theta) = -\\frac{\\|\\theta\\|^2}{2\\sigma^2} + \\text{const}$$\n",
        "\n",
        "$$\\hat{\\theta}_{MAP} = \\arg\\max_\\theta \\left[\\log P(\\mathcal{D}|\\theta) - \\frac{\\lambda}{2}\\|\\theta\\|^2\\right]$$\n",
        "\n",
        "**Laplace Prior → L1 Regularization:**\n",
        "$$P(\\theta) \\propto \\exp(-\\lambda|\\theta|) \\implies \\hat{\\theta}_{MAP} = \\arg\\max_\\theta \\left[\\log P(\\mathcal{D}|\\theta) - \\lambda\\|\\theta\\|_1\\right]$$\n",
        "\n",
        "### 6.6 Full Bayesian Inference\n",
        "\n",
        "**Posterior Distribution:**\n",
        "$$P(\\theta | \\mathcal{D}) = \\frac{P(\\mathcal{D} | \\theta) P(\\theta)}{P(\\mathcal{D})}$$\n",
        "\n",
        "**Predictive Distribution:**\n",
        "$$P(x_{new} | \\mathcal{D}) = \\int P(x_{new} | \\theta) P(\\theta | \\mathcal{D}) d\\theta$$\n",
        "\n",
        "**Challenge:** Integral often intractable → requires approximate inference\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Latent Variable Models\n",
        "\n",
        "### 7.1 Definition and Motivation\n",
        "\n",
        "**Latent Variable Model:** A probabilistic model with observed variables $\\mathbf{x}$ and unobserved (hidden) variables $\\mathbf{z}$.\n",
        "\n",
        "$$P_\\theta(\\mathbf{x}) = \\int P_\\theta(\\mathbf{x}, \\mathbf{z}) d\\mathbf{z} = \\int P_\\theta(\\mathbf{x} | \\mathbf{z}) P(\\mathbf{z}) d\\mathbf{z}$$\n",
        "\n",
        "**Purpose in Gen-AI:**\n",
        "- Learn compressed representations\n",
        "- Enable controllable generation\n",
        "- Capture data structure\n",
        "\n",
        "### 7.2 Marginal Likelihood (Evidence)\n",
        "\n",
        "$$P_\\theta(\\mathbf{x}) = \\int P_\\theta(\\mathbf{x} | \\mathbf{z}) P(\\mathbf{z}) d\\mathbf{z}$$\n",
        "\n",
        "**Problem:** This integral is often intractable because:\n",
        "- High-dimensional integration\n",
        "- Complex likelihood functions\n",
        "- No closed-form solution\n",
        "\n",
        "### 7.3 Evidence Lower Bound (ELBO)\n",
        "\n",
        "**Derivation:**\n",
        "Starting from log marginal likelihood:\n",
        "$$\\log P_\\theta(\\mathbf{x}) = \\log \\int P_\\theta(\\mathbf{x}, \\mathbf{z}) d\\mathbf{z}$$\n",
        "\n",
        "Introducing variational distribution $Q_\\phi(\\mathbf{z}|\\mathbf{x})$:\n",
        "$$\\log P_\\theta(\\mathbf{x}) = \\log \\int Q_\\phi(\\mathbf{z}|\\mathbf{x}) \\frac{P_\\theta(\\mathbf{x}, \\mathbf{z})}{Q_\\phi(\\mathbf{z}|\\mathbf{x})} d\\mathbf{z}$$\n",
        "\n",
        "Applying Jensen's inequality:\n",
        "$$\\log P_\\theta(\\mathbf{x}) \\geq \\int Q_\\phi(\\mathbf{z}|\\mathbf{x}) \\log \\frac{P_\\theta(\\mathbf{x}, \\mathbf{z})}{Q_\\phi(\\mathbf{z}|\\mathbf{x})} d\\mathbf{z}$$\n",
        "\n",
        "**ELBO Definition:**\n",
        "$$\\mathcal{L}_{ELBO}(\\theta, \\phi; \\mathbf{x}) = \\mathbb{E}_{Q_\\phi(\\mathbf{z}|\\mathbf{x})}\\left[\\log \\frac{P_\\theta(\\mathbf{x}, \\mathbf{z})}{Q_\\phi(\\mathbf{z}|\\mathbf{x})}\\right]$$\n",
        "\n",
        "**Decomposition:**\n",
        "$$\\mathcal{L}_{ELBO} = \\underbrace{\\mathbb{E}_{Q_\\phi(\\mathbf{z}|\\mathbf{x})}[\\log P_\\theta(\\mathbf{x}|\\mathbf{z})]}_{\\text{Reconstruction Term}} - \\underbrace{D_{KL}(Q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| P(\\mathbf{z}))}_{\\text{Regularization Term}}$$\n",
        "\n",
        "**Relationship to Log-Likelihood:**\n",
        "$$\\log P_\\theta(\\mathbf{x}) = \\mathcal{L}_{ELBO} + D_{KL}(Q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| P_\\theta(\\mathbf{z}|\\mathbf{x}))$$\n",
        "\n",
        "### 7.4 Expectation-Maximization (EM) Algorithm\n",
        "\n",
        "**Objective:** Maximum likelihood for latent variable models.\n",
        "\n",
        "**E-Step (Expectation):**\n",
        "$$Q(\\theta | \\theta^{(t)}) = \\mathbb{E}_{\\mathbf{z} \\sim P(\\mathbf{z}|\\mathbf{x}, \\theta^{(t)})}[\\log P(\\mathbf{x}, \\mathbf{z} | \\theta)]$$\n",
        "\n",
        "**M-Step (Maximization):**\n",
        "$$\\theta^{(t+1)} = \\arg\\max_\\theta Q(\\theta | \\theta^{(t)})$$\n",
        "\n",
        "**Convergence Guarantee:**\n",
        "$$\\log P(\\mathbf{x} | \\theta^{(t+1)}) \\geq \\log P(\\mathbf{x} | \\theta^{(t)})$$\n",
        "\n",
        "### 7.5 Variational Inference\n",
        "\n",
        "**Goal:** Approximate intractable posterior $P(\\mathbf{z}|\\mathbf{x})$ with tractable $Q_\\phi(\\mathbf{z}|\\mathbf{x})$.\n",
        "\n",
        "**Optimization:**\n",
        "$$\\phi^* = \\arg\\min_\\phi D_{KL}(Q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| P(\\mathbf{z}|\\mathbf{x}))$$\n",
        "\n",
        "Equivalently:\n",
        "$$\\phi^* = \\arg\\max_\\phi \\mathcal{L}_{ELBO}(\\phi)$$\n",
        "\n",
        "**Mean-Field Approximation:**\n",
        "$$Q(\\mathbf{z}) = \\prod_{i=1}^{M} Q_i(z_i)$$\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Sampling Methods for Generative Models\n",
        "\n",
        "### 8.1 Importance of Sampling in Gen-AI\n",
        "\n",
        "Sampling is fundamental for:\n",
        "- Generating new data points\n",
        "- Computing expectations\n",
        "- Training stochastic models\n",
        "- Monte Carlo estimation\n",
        "\n",
        "### 8.2 Ancestral Sampling\n",
        "\n",
        "**Definition:** Sample from joint distribution using factorization.\n",
        "\n",
        "For graphical model:\n",
        "$$P(x_1, ..., x_n) = \\prod_{i=1}^{n} P(x_i | \\text{parents}(x_i))$$\n",
        "\n",
        "**Procedure:**\n",
        "1. Sample $x_1 \\sim P(x_1)$\n",
        "2. Sample $x_2 \\sim P(x_2 | x_1)$\n",
        "3. Continue: $x_i \\sim P(x_i | x_1, ..., x_{i-1})$\n",
        "\n",
        "**Application:** Autoregressive generation in GPT\n",
        "\n",
        "### 8.3 Inverse Transform Sampling\n",
        "\n",
        "**Definition:** Generate samples from any distribution using uniform random variable.\n",
        "\n",
        "For CDF $F_X$:\n",
        "1. Sample $u \\sim \\text{Uniform}(0, 1)$\n",
        "2. Compute $x = F_X^{-1}(u)$\n",
        "\n",
        "$$X = F_X^{-1}(U) \\sim F_X$$\n",
        "\n",
        "### 8.4 Rejection Sampling\n",
        "\n",
        "**Setup:** Target distribution $p(x)$, proposal distribution $q(x)$, constant $M$ where $Mq(x) \\geq p(x)$ for all $x$.\n",
        "\n",
        "**Algorithm:**\n",
        "1. Sample $x \\sim q(x)$\n",
        "2. Sample $u \\sim \\text{Uniform}(0, 1)$\n",
        "3. Accept $x$ if $u < \\frac{p(x)}{Mq(x)}$; else reject and repeat\n",
        "\n",
        "### 8.5 Markov Chain Monte Carlo (MCMC)\n",
        "\n",
        "**Definition:** Construct Markov chain with stationary distribution equal to target distribution.\n",
        "\n",
        "**Metropolis-Hastings Algorithm:**\n",
        "1. Initialize $x^{(0)}$\n",
        "2. For $t = 1, 2, ...$:\n",
        "   - Propose $x' \\sim q(x' | x^{(t-1)})$\n",
        "   - Compute acceptance ratio:\n",
        "   $$\\alpha = \\min\\left(1, \\frac{p(x') q(x^{(t-1)} | x')}{p(x^{(t-1)}) q(x' | x^{(t-1)})}\\right)$$\n",
        "   - Accept with probability $\\alpha$:\n",
        "   $$x^{(t)} = \\begin{cases} x' & \\text{with prob } \\alpha \\\\ x^{(t-1)} & \\text{otherwise} \\end{cases}$$\n",
        "\n",
        "**Gibbs Sampling:**\n",
        "For multivariate distribution, sample each variable conditioned on others:\n",
        "$$x_i^{(t+1)} \\sim P(x_i | x_1^{(t+1)}, ..., x_{i-1}^{(t+1)}, x_{i+1}^{(t)}, ..., x_n^{(t)})$$\n",
        "\n",
        "### 8.6 Reparameterization Trick\n",
        "\n",
        "**Problem:** Cannot backpropagate through stochastic sampling.\n",
        "\n",
        "**Solution:** Express random variable as deterministic function of parameters and noise.\n",
        "\n",
        "**For Gaussian:**\n",
        "$$\\mathbf{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2)$$\n",
        "$$\\mathbf{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, I)$$\n",
        "\n",
        "**Gradient Flow:**\n",
        "$$\\nabla_{\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}} \\mathbb{E}_{z \\sim \\mathcal{N}(\\mu, \\sigma^2)}[f(z)] = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,1)}[\\nabla_{\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}} f(\\mu + \\sigma \\epsilon)]$$\n",
        "\n",
        "**Application:** Training VAEs with backpropagation\n",
        "\n",
        "### 8.7 Temperature Sampling\n",
        "\n",
        "**Definition:** Control randomness in sampling from categorical distribution.\n",
        "\n",
        "**Softmax with temperature:**\n",
        "$$P(x_i) = \\frac{\\exp(z_i / \\tau)}{\\sum_j \\exp(z_j / \\tau)}$$\n",
        "\n",
        "| Temperature $\\tau$ | Effect |\n",
        "|-------------------|--------|\n",
        "| $\\tau \\rightarrow 0$ | Deterministic (argmax) |\n",
        "| $\\tau = 1$ | Standard softmax |\n",
        "| $\\tau \\rightarrow \\infty$ | Uniform distribution |\n",
        "\n",
        "### 8.8 Top-k and Top-p (Nucleus) Sampling\n",
        "\n",
        "**Top-k Sampling:**\n",
        "1. Keep only top $k$ highest probability tokens\n",
        "2. Renormalize probabilities\n",
        "3. Sample from truncated distribution\n",
        "\n",
        "**Top-p (Nucleus) Sampling:**\n",
        "1. Sort tokens by probability\n",
        "2. Keep smallest set where cumulative probability $\\geq p$\n",
        "3. Renormalize and sample\n",
        "\n",
        "$$V^{(p)} = \\min\\{V' \\subseteq V : \\sum_{x \\in V'} P(x) \\geq p\\}$$\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Probabilistic Framework of Gen-AI Architectures\n",
        "\n",
        "### 9.1 Taxonomy of Generative Models\n",
        "\n",
        "```\n",
        "Generative Models\n",
        "├── Explicit Density\n",
        "│   ├── Tractable Density\n",
        "│   │   ├── Autoregressive Models (GPT)\n",
        "│   │   └── Flow-based Models (Normalizing Flows)\n",
        "│   └── Approximate Density\n",
        "│       ├── Variational Autoencoders (VAE)\n",
        "│       └── Diffusion Models\n",
        "└── Implicit Density\n",
        "    └── Generative Adversarial Networks (GAN)\n",
        "```\n",
        "\n",
        "### 9.2 Autoregressive Models\n",
        "\n",
        "**Probabilistic Foundation:**\n",
        "$$P_\\theta(\\mathbf{x}) = \\prod_{t=1}^{T} P_\\theta(x_t | x_1, ..., x_{t-1})$$\n",
        "\n",
        "**Training Objective (NLL):**\n",
        "$$\\mathcal{L}_{AR} = -\\mathbb{E}_{\\mathbf{x} \\sim p_{data}}\\left[\\sum_{t=1}^{T} \\log P_\\theta(x_t | x_{<t})\\right]$$\n",
        "\n",
        "**GPT-style Transformer:**\n",
        "$$P_\\theta(x_t | x_{<t}) = \\text{softmax}(\\mathbf{W}_v \\cdot \\mathbf{h}_t)$$\n",
        "\n",
        "Where $\\mathbf{h}_t$ is the hidden state from transformer.\n",
        "\n",
        "**Perplexity (Evaluation Metric):**\n",
        "$$\\text{PPL} = \\exp\\left(-\\frac{1}{T}\\sum_{t=1}^{T} \\log P_\\theta(x_t | x_{<t})\\right)$$\n",
        "\n",
        "### 9.3 Variational Autoencoders (VAE)\n",
        "\n",
        "**Generative Model:**\n",
        "$$P_\\theta(\\mathbf{x}) = \\int P_\\theta(\\mathbf{x} | \\mathbf{z}) P(\\mathbf{z}) d\\mathbf{z}$$\n",
        "\n",
        "**Components:**\n",
        "- Prior: $P(\\mathbf{z}) = \\mathcal{N}(0, I)$\n",
        "- Decoder: $P_\\theta(\\mathbf{x} | \\mathbf{z})$ (neural network)\n",
        "- Encoder (approximate posterior): $Q_\\phi(\\mathbf{z} | \\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu}_\\phi(\\mathbf{x}), \\text{diag}(\\boldsymbol{\\sigma}^2_\\phi(\\mathbf{x})))$\n",
        "\n",
        "**VAE Loss (Negative ELBO):**\n",
        "$$\\mathcal{L}_{VAE}(\\theta, \\phi; \\mathbf{x}) = -\\mathbb{E}_{Q_\\phi(\\mathbf{z}|\\mathbf{x})}[\\log P_\\theta(\\mathbf{x}|\\mathbf{z})] + D_{KL}(Q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| P(\\mathbf{z}))$$\n",
        "\n",
        "**KL Term (Closed Form for Gaussians):**\n",
        "$$D_{KL}(Q_\\phi \\| P) = -\\frac{1}{2}\\sum_{j=1}^{J}\\left(1 + \\log\\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)$$\n",
        "\n",
        "**Reconstruction Term:**\n",
        "$$\\mathbb{E}_{Q_\\phi}[\\log P_\\theta(\\mathbf{x}|\\mathbf{z})] \\approx \\frac{1}{L}\\sum_{l=1}^{L} \\log P_\\theta(\\mathbf{x} | \\mathbf{z}^{(l)})$$\n",
        "\n",
        "Where $\\mathbf{z}^{(l)} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon}^{(l)}$, $\\boldsymbol{\\epsilon}^{(l)} \\sim \\mathcal{N}(0, I)$\n",
        "\n",
        "### 9.4 Diffusion Models\n",
        "\n",
        "**Forward Process (Fixed):**\n",
        "$$q(\\mathbf{x}_{1:T} | \\mathbf{x}_0) = \\prod_{t=1}^{T} q(\\mathbf{x}_t | \\mathbf{x}_{t-1})$$\n",
        "\n",
        "$$q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, \\beta_t \\mathbf{I})$$\n",
        "\n",
        "**Closed-form for arbitrary timestep:**\n",
        "$$q(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0, (1-\\bar{\\alpha}_t)\\mathbf{I})$$\n",
        "\n",
        "Where:\n",
        "- $\\alpha_t = 1 - \\beta_t$\n",
        "- $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$\n",
        "\n",
        "**Reverse Process (Learned):**\n",
        "$$P_\\theta(\\mathbf{x}_{0:T}) = P(\\mathbf{x}_T) \\prod_{t=1}^{T} P_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t)$$\n",
        "\n",
        "$$P_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\sigma_t^2 \\mathbf{I})$$\n",
        "\n",
        "**Training Objective (Simplified):**\n",
        "$$\\mathcal{L}_{simple} = \\mathbb{E}_{t, \\mathbf{x}_0, \\boldsymbol{\\epsilon}}\\left[\\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2\\right]$$\n",
        "\n",
        "Where $\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1-\\bar{\\alpha}_t}\\boldsymbol{\\epsilon}$\n",
        "\n",
        "**Score Matching Perspective:**\n",
        "$$\\nabla_{\\mathbf{x}} \\log P(\\mathbf{x}) \\approx -\\frac{\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)}{\\sqrt{1-\\bar{\\alpha}_t}}$$\n",
        "\n",
        "### 9.5 Normalizing Flows\n",
        "\n",
        "**Definition:** Transform simple distribution to complex one through invertible functions.\n",
        "\n",
        "$$\\mathbf{x} = f_\\theta(\\mathbf{z}), \\quad \\mathbf{z} \\sim P_Z(\\mathbf{z})$$\n",
        "\n",
        "**Change of Variables Formula:**\n",
        "$$P_X(\\mathbf{x}) = P_Z(f_\\theta^{-1}(\\mathbf{x})) \\left|\\det\\left(\\frac{\\partial f_\\theta^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)\\right|$$\n",
        "\n",
        "$$\\log P_X(\\mathbf{x}) = \\log P_Z(\\mathbf{z}) - \\log\\left|\\det\\left(\\frac{\\partial f_\\theta(\\mathbf{z})}{\\partial \\mathbf{z}}\\right)\\right|$$\n",
        "\n",
        "**Composition of Flows:**\n",
        "$$\\mathbf{x} = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(\\mathbf{z})$$\n",
        "\n",
        "$$\\log P_X(\\mathbf{x}) = \\log P_Z(\\mathbf{z}) - \\sum_{k=1}^{K} \\log\\left|\\det\\left(\\frac{\\partial f_k}{\\partial \\mathbf{h}_{k-1}}\\right)\\right|$$\n",
        "\n",
        "**Training:**\n",
        "$$\\mathcal{L}_{NF} = -\\mathbb{E}_{\\mathbf{x} \\sim p_{data}}[\\log P_X(\\mathbf{x})]$$\n",
        "\n",
        "### 9.6 Generative Adversarial Networks (GANs)\n",
        "\n",
        "**Implicit Density:** No explicit density computation; generates samples directly.\n",
        "\n",
        "**Min-Max Objective:**\n",
        "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{\\mathbf{x} \\sim p_{data}}[\\log D(\\mathbf{x})] + \\mathbb{E}_{\\mathbf{z} \\sim p_z}[\\log(1 - D(G(\\mathbf{z})))]$$\n",
        "\n",
        "**Optimal Discriminator:**\n",
        "$$D^*(\\mathbf{x}) = \\frac{p_{data}(\\mathbf{x})}{p_{data}(\\mathbf{x}) + p_g(\\mathbf{x})}$$\n",
        "\n",
        "**Generator Objective (at optimal D):**\n",
        "$$\\min_G V(D^*, G) = 2 \\cdot D_{JS}(p_{data} \\| p_g) - \\log 4$$\n",
        "\n",
        "**Wasserstein GAN:**\n",
        "$$\\min_G \\max_{D \\in \\mathcal{D}} \\mathbb{E}_{\\mathbf{x} \\sim p_{data}}[D(\\mathbf{x})] - \\mathbb{E}_{\\mathbf{z} \\sim p_z}[D(G(\\mathbf{z}))]$$\n",
        "\n",
        "Where $\\mathcal{D}$ is set of 1-Lipschitz functions.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Statistical Learning Theory\n",
        "\n",
        "### 10.1 Bias-Variance Tradeoff\n",
        "\n",
        "**Definition:** Decomposition of expected prediction error.\n",
        "\n",
        "For estimator $\\hat{f}(\\mathbf{x})$ predicting $y$:\n",
        "$$\\mathbb{E}[(y - \\hat{f}(\\mathbf{x}))^2] = \\underbrace{\\text{Bias}^2[\\hat{f}(\\mathbf{x})]}_{\\text{Systematic Error}} + \\underbrace{\\text{Var}[\\hat{f}(\\mathbf{x})]}_{\\text{Sensitivity to Data}} + \\underbrace{\\sigma^2}_{\\text{Irreducible Error}}$$\n",
        "\n",
        "Where:\n",
        "$$\\text{Bias}[\\hat{f}(\\mathbf{x})] = \\mathbb{E}[\\hat{f}(\\mathbf{x})] - f(\\mathbf{x})$$\n",
        "$$\\text{Var}[\\hat{f}(\\mathbf{x})] = \\mathbb{E}[(\\hat{f}(\\mathbf{x}) - \\mathbb{E}[\\hat{f}(\\mathbf{x})])^2]$$\n",
        "\n",
        "**In Gen-AI Context:**\n",
        "\n",
        "| Regime | Characteristic | Gen-AI Example |\n",
        "|--------|---------------|----------------|\n",
        "| High Bias | Underfitting | Too small model capacity |\n",
        "| High Variance | Overfitting | Memorizing training data |\n",
        "| Optimal | Balance | Proper regularization |\n",
        "\n",
        "### 10.2 Generalization Theory\n",
        "\n",
        "**Empirical Risk:**\n",
        "$$\\hat{R}(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N} \\mathcal{L}(f_\\theta(\\mathbf{x}_i), y_i)$$\n",
        "\n",
        "**True Risk:**\n",
        "$$R(\\theta) = \\mathbb{E}_{(\\mathbf{x}, y) \\sim P_{data}}[\\mathcal{L}(f_\\theta(\\mathbf{x}), y)]$$\n",
        "\n",
        "**Generalization Gap:**\n",
        "$$\\epsilon_{gen} = R(\\theta) - \\hat{R}(\\theta)$$\n",
        "\n",
        "**PAC Bound:**\n",
        "With probability at least $1 - \\delta$:\n",
        "$$R(\\theta) \\leq \\hat{R}(\\theta) + \\sqrt{\\frac{2\\log(2|\\mathcal{H}|/\\delta)}{N}}$$\n",
        "\n",
        "### 10.3 Regularization Techniques\n",
        "\n",
        "**L2 Regularization (Weight Decay):**\n",
        "$$\\mathcal{L}_{reg} = \\mathcal{L}_{data} + \\lambda \\|\\theta\\|_2^2$$\n",
        "\n",
        "**L1 Regularization (Sparsity):**\n",
        "$$\\mathcal{L}_{reg} = \\mathcal{L}_{data} + \\lambda \\|\\theta\\|_1$$\n",
        "\n",
        "**Dropout:**\n",
        "$$\\tilde{\\mathbf{h}} = \\mathbf{m} \\odot \\mathbf{h}, \\quad m_i \\sim \\text{Bernoulli}(p)$$\n",
        "\n",
        "**Variational Dropout (Bayesian Interpretation):**\n",
        "$$P(\\theta | \\mathcal{D}) \\approx \\prod_i \\mathcal{N}(\\theta_i | \\mu_i, \\sigma_i^2)$$\n",
        "\n",
        "### 10.4 Law of Large Numbers\n",
        "\n",
        "**Weak LLN:**\n",
        "$$\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i \\xrightarrow{P} \\mathbb{E}[X] \\text{ as } n \\rightarrow \\infty$$\n",
        "\n",
        "**Application:** Justifies Monte Carlo estimation:\n",
        "$$\\mathbb{E}[f(X)] \\approx \\frac{1}{N}\\sum_{i=1}^{N} f(x_i)$$\n",
        "\n",
        "### 10.5 Central Limit Theorem\n",
        "\n",
        "**Statement:** For i.i.d. random variables with mean $\\mu$ and variance $\\sigma^2$:\n",
        "$$\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0, 1)$$\n",
        "\n",
        "**Implication:** Sum of many independent random effects → Gaussian distribution\n",
        "\n",
        "**Application in Gen-AI:**\n",
        "- Justifies Gaussian assumptions in many models\n",
        "- Explains convergence of gradient averaging\n",
        "\n",
        "### 10.6 Concentration Inequalities\n",
        "\n",
        "**Markov's Inequality:**\n",
        "$$P(X \\geq a) \\leq \\frac{\\mathbb{E}[X]}{a}, \\quad \\text{for } X \\geq 0, a > 0$$\n",
        "\n",
        "**Chebyshev's Inequality:**\n",
        "$$P(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}$$\n",
        "\n",
        "**Hoeffding's Inequality:**\n",
        "For bounded i.i.d. $X_i \\in [a_i, b_i]$:\n",
        "$$P\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n} X_i - \\mathbb{E}[X]\\right| \\geq \\epsilon\\right) \\leq 2\\exp\\left(-\\frac{2n^2\\epsilon^2}{\\sum_{i=1}^{n}(b_i - a_i)^2}\\right)$$\n",
        "\n",
        "**Application:** Bounds on generalization error, confidence intervals\n",
        "\n",
        "---\n",
        "\n",
        "## Summary Table: Probability Concepts in Gen-AI Architectures\n",
        "\n",
        "| Gen-AI Model | Key Probabilistic Concepts |\n",
        "|--------------|---------------------------|\n",
        "| **GPT/LLaMA** | Chain rule, Conditional probability, Cross-entropy, Autoregressive factorization |\n",
        "| **VAE** | Latent variables, ELBO, KL divergence, Reparameterization, Variational inference |\n",
        "| **Diffusion** | Markov chains, Gaussian noise, Score matching, Reverse process |\n",
        "| **GAN** | Implicit density, JS divergence, Adversarial training, Wasserstein distance |\n",
        "| **Flow** | Change of variables, Jacobian determinant, Invertible transformations |\n",
        "| **RLHF** | Reward modeling, KL constraints, Policy optimization |\n",
        "\n",
        "---\n",
        "\n",
        "## Key Mathematical Relationships\n",
        "\n",
        "```\n",
        "Cross-Entropy = Entropy + KL Divergence\n",
        "       ↓              ↓           ↓\n",
        "    H(P,Q)    =     H(P)   +   D_KL(P||Q)\n",
        "\n",
        "Log-Likelihood = ELBO + KL(Posterior)\n",
        "       ↓           ↓          ↓\n",
        "    log P(x)   =  L_ELBO  +  D_KL(Q(z|x)||P(z|x))\n",
        "```"
      ],
      "metadata": {
        "id": "85_P_gg2W9ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Foundations of Probability Theory\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction and Motivation\n",
        "\n",
        "**Definition:** Probability theory is the mathematical framework for quantifying uncertainty, randomness, and stochastic phenomena. In Generative AI, probability theory provides the foundational language for modeling data distributions, learning latent representations, and generating new samples from learned distributions.\n",
        "\n",
        "**Relevance to Generative AI:**\n",
        "- Generative models learn probability distributions $p(x)$ over data $x$\n",
        "- Sampling mechanisms rely on probabilistic foundations\n",
        "- Loss functions derived from probabilistic principles (likelihood, KL divergence)\n",
        "- Latent variable models (VAEs, diffusion models) require marginalization and conditioning\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Axiomatic Foundations of Probability\n",
        "\n",
        "### 2.1 Sample Space and Events\n",
        "\n",
        "**Definition (Sample Space):** The sample space $\\Omega$ is the set of all possible outcomes of a random experiment.\n",
        "\n",
        "**Definition (Event):** An event $A$ is a subset of the sample space, $A \\subseteq \\Omega$.\n",
        "\n",
        "**Definition (Sigma-Algebra):** A $\\sigma$-algebra $\\mathcal{F}$ on $\\Omega$ is a collection of subsets satisfying:\n",
        "1. $\\Omega \\in \\mathcal{F}$\n",
        "2. If $A \\in \\mathcal{F}$, then $A^c \\in \\mathcal{F}$ (closure under complementation)\n",
        "3. If $A_1, A_2, \\ldots \\in \\mathcal{F}$, then $\\bigcup_{i=1}^{\\infty} A_i \\in \\mathcal{F}$ (closure under countable unions)\n",
        "\n",
        "### 2.2 Kolmogorov Axioms\n",
        "\n",
        "**Definition (Probability Measure):** A probability measure $P$ is a function $P: \\mathcal{F} \\rightarrow [0, 1]$ satisfying:\n",
        "\n",
        "**Axiom 1 (Non-negativity):**\n",
        "$$P(A) \\geq 0 \\quad \\forall A \\in \\mathcal{F}$$\n",
        "\n",
        "**Axiom 2 (Normalization):**\n",
        "$$P(\\Omega) = 1$$\n",
        "\n",
        "**Axiom 3 (Countable Additivity):** For mutually disjoint events $A_1, A_2, \\ldots$ (i.e., $A_i \\cap A_j = \\emptyset$ for $i \\neq j$):\n",
        "$$P\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} P(A_i)$$\n",
        "\n",
        "**Probability Space:** The triplet $(\\Omega, \\mathcal{F}, P)$ constitutes a probability space.\n",
        "\n",
        "### 2.3 Fundamental Properties Derived from Axioms\n",
        "\n",
        "**Complement Rule:**\n",
        "$$P(A^c) = 1 - P(A)$$\n",
        "\n",
        "**Addition Rule:**\n",
        "$$P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$$\n",
        "\n",
        "**Monotonicity:**\n",
        "$$A \\subseteq B \\Rightarrow P(A) \\leq P(B)$$\n",
        "\n",
        "**Boole's Inequality (Union Bound):**\n",
        "$$P\\left(\\bigcup_{i=1}^{n} A_i\\right) \\leq \\sum_{i=1}^{n} P(A_i)$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Conditional Probability and Independence\n",
        "\n",
        "### 3.1 Conditional Probability\n",
        "\n",
        "**Definition:** The conditional probability of event $A$ given event $B$ (where $P(B) > 0$) is:\n",
        "\n",
        "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
        "\n",
        "**Chain Rule (Product Rule):** For events $A_1, A_2, \\ldots, A_n$:\n",
        "\n",
        "$$P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n) = P(A_1) \\cdot P(A_2|A_1) \\cdot P(A_3|A_1 \\cap A_2) \\cdots P(A_n|A_1 \\cap \\cdots \\cap A_{n-1})$$\n",
        "\n",
        "**Compact Form:**\n",
        "$$P\\left(\\bigcap_{i=1}^{n} A_i\\right) = \\prod_{i=1}^{n} P\\left(A_i \\Big| \\bigcap_{j=1}^{i-1} A_j\\right)$$\n",
        "\n",
        "**Gen-AI Application:** Autoregressive models (GPT, language models) factorize joint distributions using the chain rule:\n",
        "$$p(x_1, x_2, \\ldots, x_T) = \\prod_{t=1}^{T} p(x_t | x_1, \\ldots, x_{t-1})$$\n",
        "\n",
        "### 3.2 Law of Total Probability\n",
        "\n",
        "**Definition:** If $\\{B_1, B_2, \\ldots, B_n\\}$ is a partition of $\\Omega$ (mutually exclusive and exhaustive), then:\n",
        "\n",
        "$$P(A) = \\sum_{i=1}^{n} P(A|B_i) P(B_i)$$\n",
        "\n",
        "**Continuous Version:**\n",
        "$$p(x) = \\int p(x|z) p(z) \\, dz$$\n",
        "\n",
        "**Gen-AI Application:** This is the marginalization principle fundamental to latent variable models (VAEs):\n",
        "$$p_\\theta(x) = \\int p_\\theta(x|z) p(z) \\, dz$$\n",
        "\n",
        "### 3.3 Bayes' Theorem\n",
        "\n",
        "**Definition:** Bayes' theorem relates conditional probabilities:\n",
        "\n",
        "$$P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$$\n",
        "\n",
        "**Extended Form with Partition:**\n",
        "$$P(B_i|A) = \\frac{P(A|B_i) P(B_i)}{\\sum_{j=1}^{n} P(A|B_j) P(B_j)}$$\n",
        "\n",
        "**Continuous Form:**\n",
        "$$p(\\theta|x) = \\frac{p(x|\\theta) p(\\theta)}{p(x)} = \\frac{p(x|\\theta) p(\\theta)}{\\int p(x|\\theta') p(\\theta') \\, d\\theta'}$$\n",
        "\n",
        "**Terminology:**\n",
        "| Term | Symbol | Description |\n",
        "|------|--------|-------------|\n",
        "| Posterior | $p(\\theta\\|x)$ | Updated belief after observing data |\n",
        "| Likelihood | $p(x\\|\\theta)$ | Probability of data given parameters |\n",
        "| Prior | $p(\\theta)$ | Initial belief before data |\n",
        "| Evidence/Marginal | $p(x)$ | Normalizing constant |\n",
        "\n",
        "**Gen-AI Application:**\n",
        "- VAE inference: $p(z|x) = \\frac{p(x|z)p(z)}{p(x)}$\n",
        "- Diffusion models: posterior $q(x_{t-1}|x_t, x_0)$\n",
        "\n",
        "### 3.4 Independence\n",
        "\n",
        "**Definition (Independence of Two Events):** Events $A$ and $B$ are independent if:\n",
        "$$P(A \\cap B) = P(A) P(B)$$\n",
        "\n",
        "Equivalently:\n",
        "$$P(A|B) = P(A) \\quad \\text{and} \\quad P(B|A) = P(B)$$\n",
        "\n",
        "**Definition (Mutual Independence):** Events $A_1, \\ldots, A_n$ are mutually independent if for every subset $S \\subseteq \\{1, \\ldots, n\\}$:\n",
        "$$P\\left(\\bigcap_{i \\in S} A_i\\right) = \\prod_{i \\in S} P(A_i)$$\n",
        "\n",
        "**Definition (Conditional Independence):** $A$ and $B$ are conditionally independent given $C$:\n",
        "$$P(A \\cap B | C) = P(A|C) P(B|C)$$\n",
        "\n",
        "**Notation:** $A \\perp\\!\\!\\!\\perp B \\mid C$\n",
        "\n",
        "**Gen-AI Application:** Conditional independence assumptions are crucial in:\n",
        "- Naive Bayes models\n",
        "- Graphical models\n",
        "- VAE assumption: $p(x_1, \\ldots, x_n | z) = \\prod_i p(x_i | z)$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Random Variables and Distributions\n",
        "\n",
        "### 4.1 Random Variables\n",
        "\n",
        "**Definition:** A random variable $X$ is a measurable function $X: \\Omega \\rightarrow \\mathbb{R}$ mapping outcomes to real numbers.\n",
        "\n",
        "**Types:**\n",
        "- **Discrete Random Variable:** Takes countable values $\\{x_1, x_2, \\ldots\\}$\n",
        "- **Continuous Random Variable:** Takes uncountably infinite values in $\\mathbb{R}$\n",
        "\n",
        "### 4.2 Probability Mass Function (PMF)\n",
        "\n",
        "**Definition:** For discrete random variable $X$, the PMF is:\n",
        "$$p_X(x) = P(X = x)$$\n",
        "\n",
        "**Properties:**\n",
        "1. $p_X(x) \\geq 0$ for all $x$\n",
        "2. $\\sum_{x} p_X(x) = 1$\n",
        "\n",
        "### 4.3 Probability Density Function (PDF)\n",
        "\n",
        "**Definition:** For continuous random variable $X$, the PDF $f_X(x)$ satisfies:\n",
        "$$P(a \\leq X \\leq b) = \\int_a^b f_X(x) \\, dx$$\n",
        "\n",
        "**Properties:**\n",
        "1. $f_X(x) \\geq 0$ for all $x$\n",
        "2. $\\int_{-\\infty}^{\\infty} f_X(x) \\, dx = 1$\n",
        "\n",
        "**Note:** $f_X(x)$ is NOT a probability; it can exceed 1.\n",
        "\n",
        "### 4.4 Cumulative Distribution Function (CDF)\n",
        "\n",
        "**Definition:** The CDF of random variable $X$ is:\n",
        "$$F_X(x) = P(X \\leq x)$$\n",
        "\n",
        "**Properties:**\n",
        "1. $\\lim_{x \\to -\\infty} F_X(x) = 0$\n",
        "2. $\\lim_{x \\to +\\infty} F_X(x) = 1$\n",
        "3. $F_X$ is non-decreasing\n",
        "4. $F_X$ is right-continuous\n",
        "\n",
        "**Relationship to PDF:**\n",
        "$$f_X(x) = \\frac{dF_X(x)}{dx}$$\n",
        "\n",
        "$$F_X(x) = \\int_{-\\infty}^{x} f_X(t) \\, dt$$\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Joint, Marginal, and Conditional Distributions\n",
        "\n",
        "### 5.1 Joint Distribution\n",
        "\n",
        "**Definition (Joint PMF):** For discrete random variables $X$ and $Y$:\n",
        "$$p_{X,Y}(x, y) = P(X = x, Y = y)$$\n",
        "\n",
        "**Definition (Joint PDF):** For continuous random variables:\n",
        "$$P((X, Y) \\in A) = \\iint_A f_{X,Y}(x, y) \\, dx \\, dy$$\n",
        "\n",
        "### 5.2 Marginal Distribution\n",
        "\n",
        "**Definition:** Obtained by summing/integrating out other variables.\n",
        "\n",
        "**Discrete:**\n",
        "$$p_X(x) = \\sum_{y} p_{X,Y}(x, y)$$\n",
        "\n",
        "**Continuous:**\n",
        "$$f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x, y) \\, dy$$\n",
        "\n",
        "**Gen-AI Application:** Computing marginal likelihood in VAEs:\n",
        "$$p_\\theta(x) = \\int p_\\theta(x, z) \\, dz = \\int p_\\theta(x|z) p(z) \\, dz$$\n",
        "\n",
        "### 5.3 Conditional Distribution\n",
        "\n",
        "**Definition:** For continuous variables with $f_Y(y) > 0$:\n",
        "$$f_{X|Y}(x|y) = \\frac{f_{X,Y}(x, y)}{f_Y(y)}$$\n",
        "\n",
        "**Relationship:**\n",
        "$$f_{X,Y}(x, y) = f_{X|Y}(x|y) \\cdot f_Y(y) = f_{Y|X}(y|x) \\cdot f_X(x)$$\n",
        "\n",
        "### 5.4 Independence of Random Variables\n",
        "\n",
        "**Definition:** $X$ and $Y$ are independent ($X \\perp\\!\\!\\!\\perp Y$) if:\n",
        "$$f_{X,Y}(x, y) = f_X(x) \\cdot f_Y(y) \\quad \\forall x, y$$\n",
        "\n",
        "Equivalently:\n",
        "$$f_{X|Y}(x|y) = f_X(x) \\quad \\forall x, y$$\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Expectation, Variance, and Moments\n",
        "\n",
        "### 6.1 Expectation (Mean)\n",
        "\n",
        "**Definition (Discrete):**\n",
        "$$\\mathbb{E}[X] = \\sum_{x} x \\cdot p_X(x)$$\n",
        "\n",
        "**Definition (Continuous):**\n",
        "$$\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f_X(x) \\, dx$$\n",
        "\n",
        "**Expectation of Function:**\n",
        "$$\\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) \\cdot f_X(x) \\, dx$$\n",
        "\n",
        "**Properties:**\n",
        "1. **Linearity:** $\\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]$\n",
        "2. **Monotonicity:** $X \\leq Y \\Rightarrow \\mathbb{E}[X] \\leq \\mathbb{E}[Y]$\n",
        "3. **Independence:** $X \\perp\\!\\!\\!\\perp Y \\Rightarrow \\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$\n",
        "\n",
        "### 6.2 Variance\n",
        "\n",
        "**Definition:**\n",
        "$$\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$$\n",
        "\n",
        "**Properties:**\n",
        "1. $\\text{Var}(X) \\geq 0$\n",
        "2. $\\text{Var}(aX + b) = a^2 \\text{Var}(X)$\n",
        "3. If $X \\perp\\!\\!\\!\\perp Y$: $\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)$\n",
        "\n",
        "**Standard Deviation:**\n",
        "$$\\sigma_X = \\sqrt{\\text{Var}(X)}$$\n",
        "\n",
        "### 6.3 Covariance and Correlation\n",
        "\n",
        "**Covariance:**\n",
        "$$\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$$\n",
        "\n",
        "**Correlation Coefficient:**\n",
        "$$\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} \\in [-1, 1]$$\n",
        "\n",
        "**Properties:**\n",
        "- $\\text{Cov}(X, X) = \\text{Var}(X)$\n",
        "- $X \\perp\\!\\!\\!\\perp Y \\Rightarrow \\text{Cov}(X, Y) = 0$ (converse not always true)\n",
        "\n",
        "### 6.4 Moments and Moment Generating Functions\n",
        "\n",
        "**Definition (k-th Moment):**\n",
        "$$\\mu_k = \\mathbb{E}[X^k]$$\n",
        "\n",
        "**Definition (k-th Central Moment):**\n",
        "$$\\mu'_k = \\mathbb{E}[(X - \\mathbb{E}[X])^k]$$\n",
        "\n",
        "**Moment Generating Function (MGF):**\n",
        "$$M_X(t) = \\mathbb{E}[e^{tX}] = \\int_{-\\infty}^{\\infty} e^{tx} f_X(x) \\, dx$$\n",
        "\n",
        "**Property:** The k-th moment is:\n",
        "$$\\mathbb{E}[X^k] = \\left. \\frac{d^k M_X(t)}{dt^k} \\right|_{t=0}$$\n",
        "\n",
        "### 6.5 Conditional Expectation\n",
        "\n",
        "**Definition:**\n",
        "$$\\mathbb{E}[X|Y=y] = \\int_{-\\infty}^{\\infty} x \\cdot f_{X|Y}(x|y) \\, dx$$\n",
        "\n",
        "**Law of Total Expectation (Tower Property):**\n",
        "$$\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X|Y]]$$\n",
        "\n",
        "**Law of Total Variance:**\n",
        "$$\\text{Var}(X) = \\mathbb{E}[\\text{Var}(X|Y)] + \\text{Var}(\\mathbb{E}[X|Y])$$\n",
        "\n",
        "**Gen-AI Application:** In VAEs, the reconstruction term involves:\n",
        "$$\\mathbb{E}_{z \\sim q(z|x)}[\\log p(x|z)]$$\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Important Probability Distributions for Generative AI\n",
        "\n",
        "### 7.1 Discrete Distributions\n",
        "\n",
        "#### 7.1.1 Bernoulli Distribution\n",
        "\n",
        "**Definition:** Models binary outcomes.\n",
        "$$X \\sim \\text{Bernoulli}(p)$$\n",
        "\n",
        "$$P(X = x) = p^x (1-p)^{1-x}, \\quad x \\in \\{0, 1\\}$$\n",
        "\n",
        "**Parameters:** $p \\in [0, 1]$\n",
        "\n",
        "**Moments:**\n",
        "- $\\mathbb{E}[X] = p$\n",
        "- $\\text{Var}(X) = p(1-p)$\n",
        "\n",
        "#### 7.1.2 Categorical Distribution\n",
        "\n",
        "**Definition:** Generalization of Bernoulli to $K$ categories.\n",
        "$$X \\sim \\text{Categorical}(\\pi_1, \\ldots, \\pi_K)$$\n",
        "\n",
        "$$P(X = k) = \\pi_k, \\quad \\sum_{k=1}^{K} \\pi_k = 1$$\n",
        "\n",
        "**Gen-AI Application:** Output distribution for classification, token prediction in language models.\n",
        "\n",
        "#### 7.1.3 Multinomial Distribution\n",
        "\n",
        "**Definition:** Multiple independent categorical trials.\n",
        "$$(X_1, \\ldots, X_K) \\sim \\text{Multinomial}(n, \\pi_1, \\ldots, \\pi_K)$$\n",
        "\n",
        "$$P(X_1 = x_1, \\ldots, X_K = x_k) = \\frac{n!}{x_1! \\cdots x_K!} \\prod_{k=1}^{K} \\pi_k^{x_k}$$\n",
        "\n",
        "where $\\sum_{k=1}^{K} x_k = n$\n",
        "\n",
        "### 7.2 Continuous Distributions\n",
        "\n",
        "#### 7.2.1 Uniform Distribution\n",
        "\n",
        "**Definition:**\n",
        "$$X \\sim \\text{Uniform}(a, b)$$\n",
        "\n",
        "$$f_X(x) = \\frac{1}{b-a}, \\quad x \\in [a, b]$$\n",
        "\n",
        "**Moments:**\n",
        "- $\\mathbb{E}[X] = \\frac{a+b}{2}$\n",
        "- $\\text{Var}(X) = \\frac{(b-a)^2}{12}$\n",
        "\n",
        "#### 7.2.2 Gaussian (Normal) Distribution\n",
        "\n",
        "**Definition (Univariate):**\n",
        "$$X \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
        "\n",
        "$$f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
        "\n",
        "**Standard Normal:** $Z \\sim \\mathcal{N}(0, 1)$\n",
        "\n",
        "$$\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$$\n",
        "\n",
        "**Properties:**\n",
        "- $\\mathbb{E}[X] = \\mu$\n",
        "- $\\text{Var}(X) = \\sigma^2$\n",
        "- Linear combination of Gaussians is Gaussian\n",
        "\n",
        "**Definition (Multivariate):**\n",
        "$$\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$$\n",
        "\n",
        "$$f_{\\mathbf{X}}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right)$$\n",
        "\n",
        "where:\n",
        "- $\\boldsymbol{\\mu} \\in \\mathbb{R}^d$ is the mean vector\n",
        "- $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d}$ is the positive semi-definite covariance matrix\n",
        "- $|\\boldsymbol{\\Sigma}|$ is the determinant\n",
        "\n",
        "**Gen-AI Application:**\n",
        "- VAE latent prior: $p(z) = \\mathcal{N}(0, I)$\n",
        "- Diffusion noise: $q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)$\n",
        "- Reparameterization trick\n",
        "\n",
        "#### 7.2.3 Exponential Distribution\n",
        "\n",
        "**Definition:**\n",
        "$$X \\sim \\text{Exponential}(\\lambda)$$\n",
        "\n",
        "$$f_X(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0$$\n",
        "\n",
        "**Moments:**\n",
        "- $\\mathbb{E}[X] = 1/\\lambda$\n",
        "- $\\text{Var}(X) = 1/\\lambda^2$\n",
        "\n",
        "#### 7.2.4 Beta Distribution\n",
        "\n",
        "**Definition:**\n",
        "$$X \\sim \\text{Beta}(\\alpha, \\beta)$$\n",
        "\n",
        "$$f_X(x) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha-1}(1-x)^{\\beta-1}, \\quad x \\in [0, 1]$$\n",
        "\n",
        "**Gen-AI Application:** Prior for probabilities, noise scheduling in diffusion.\n",
        "\n",
        "#### 7.2.5 Dirichlet Distribution\n",
        "\n",
        "**Definition:** Multivariate generalization of Beta.\n",
        "$$\\boldsymbol{\\pi} \\sim \\text{Dirichlet}(\\alpha_1, \\ldots, \\alpha_K)$$\n",
        "\n",
        "$$f(\\boldsymbol{\\pi}) = \\frac{\\Gamma(\\sum_{k=1}^{K} \\alpha_k)}{\\prod_{k=1}^{K} \\Gamma(\\alpha_k)} \\prod_{k=1}^{K} \\pi_k^{\\alpha_k - 1}$$\n",
        "\n",
        "where $\\sum_{k=1}^{K} \\pi_k = 1$ and $\\pi_k \\geq 0$\n",
        "\n",
        "**Gen-AI Application:** Prior for topic distributions in LDA.\n",
        "\n",
        "### 7.3 Distributions Crucial for Generative Models\n",
        "\n",
        "#### 7.3.1 Mixture Models\n",
        "\n",
        "**Gaussian Mixture Model (GMM):**\n",
        "$$p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$\n",
        "\n",
        "where $\\sum_{k=1}^{K} \\pi_k = 1$\n",
        "\n",
        "**Latent Variable Formulation:**\n",
        "$$z \\sim \\text{Categorical}(\\pi_1, \\ldots, \\pi_K)$$\n",
        "$$\\mathbf{x}|z=k \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$\n",
        "\n",
        "#### 7.3.2 Exponential Family\n",
        "\n",
        "**Definition:** A distribution belongs to the exponential family if:\n",
        "$$p(x|\\boldsymbol{\\eta}) = h(x) \\exp\\left(\\boldsymbol{\\eta}^\\top \\mathbf{T}(x) - A(\\boldsymbol{\\eta})\\right)$$\n",
        "\n",
        "where:\n",
        "- $\\boldsymbol{\\eta}$: natural parameters\n",
        "- $\\mathbf{T}(x)$: sufficient statistics\n",
        "- $A(\\boldsymbol{\\eta})$: log-partition function (normalizer)\n",
        "- $h(x)$: base measure\n",
        "\n",
        "**Key Property:**\n",
        "$$\\nabla_{\\boldsymbol{\\eta}} A(\\boldsymbol{\\eta}) = \\mathbb{E}[\\mathbf{T}(x)]$$\n",
        "$$\\nabla^2_{\\boldsymbol{\\eta}} A(\\boldsymbol{\\eta}) = \\text{Cov}[\\mathbf{T}(x)]$$\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Information Theory Foundations\n",
        "\n",
        "### 8.1 Entropy\n",
        "\n",
        "**Definition (Discrete):** Shannon entropy measures uncertainty:\n",
        "$$H(X) = -\\sum_{x} p(x) \\log p(x) = -\\mathbb{E}_{p}[\\log p(X)]$$\n",
        "\n",
        "**Definition (Continuous):** Differential entropy:\n",
        "$$h(X) = -\\int f(x) \\log f(x) \\, dx$$\n",
        "\n",
        "**Properties:**\n",
        "- $H(X) \\geq 0$ for discrete distributions\n",
        "- $H(X)$ is maximized by uniform distribution (discrete)\n",
        "- For continuous: $h(X)$ can be negative\n",
        "\n",
        "**Gaussian Entropy:**\n",
        "$$h(X) = \\frac{1}{2}\\log(2\\pi e \\sigma^2) \\quad \\text{for } X \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
        "\n",
        "**Multivariate Gaussian:**\n",
        "$$h(\\mathbf{X}) = \\frac{d}{2}\\log(2\\pi e) + \\frac{1}{2}\\log|\\boldsymbol{\\Sigma}|$$\n",
        "\n",
        "### 8.2 Cross-Entropy\n",
        "\n",
        "**Definition:**\n",
        "$$H(p, q) = -\\sum_{x} p(x) \\log q(x) = -\\mathbb{E}_{p}[\\log q(X)]$$\n",
        "\n",
        "**Relationship:**\n",
        "$$H(p, q) = H(p) + D_{KL}(p \\| q)$$\n",
        "\n",
        "**Gen-AI Application:** Cross-entropy loss for classification:\n",
        "$$\\mathcal{L}_{CE} = -\\sum_{c=1}^{C} y_c \\log \\hat{y}_c$$\n",
        "\n",
        "### 8.3 Kullback-Leibler (KL) Divergence\n",
        "\n",
        "**Definition:**\n",
        "$$D_{KL}(p \\| q) = \\sum_{x} p(x) \\log \\frac{p(x)}{q(x)} = \\mathbb{E}_{p}\\left[\\log \\frac{p(X)}{q(X)}\\right]$$\n",
        "\n",
        "**Continuous Version:**\n",
        "$$D_{KL}(p \\| q) = \\int p(x) \\log \\frac{p(x)}{q(x)} \\, dx$$\n",
        "\n",
        "**Properties:**\n",
        "1. $D_{KL}(p \\| q) \\geq 0$ (Gibbs' inequality)\n",
        "2. $D_{KL}(p \\| q) = 0 \\Leftrightarrow p = q$ (almost everywhere)\n",
        "3. **Asymmetric:** $D_{KL}(p \\| q) \\neq D_{KL}(q \\| p)$\n",
        "4. Not a metric (doesn't satisfy triangle inequality)\n",
        "\n",
        "**KL Divergence Between Gaussians:**\n",
        "$$D_{KL}(\\mathcal{N}(\\mu_1, \\sigma_1^2) \\| \\mathcal{N}(\\mu_2, \\sigma_2^2)) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$$\n",
        "\n",
        "**Multivariate Gaussians:**\n",
        "$$D_{KL}(\\mathcal{N}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_1) \\| \\mathcal{N}(\\boldsymbol{\\mu}_2, \\boldsymbol{\\Sigma}_2)) = \\frac{1}{2}\\left[\\log\\frac{|\\boldsymbol{\\Sigma}_2|}{|\\boldsymbol{\\Sigma}_1|} - d + \\text{tr}(\\boldsymbol{\\Sigma}_2^{-1}\\boldsymbol{\\Sigma}_1) + (\\boldsymbol{\\mu}_2-\\boldsymbol{\\mu}_1)^\\top\\boldsymbol{\\Sigma}_2^{-1}(\\boldsymbol{\\mu}_2-\\boldsymbol{\\mu}_1)\\right]$$\n",
        "\n",
        "**Special Case (VAE KL term):** When $q = \\mathcal{N}(\\mu, \\sigma^2)$ and $p = \\mathcal{N}(0, 1)$:\n",
        "$$D_{KL}(q \\| p) = -\\frac{1}{2}\\sum_{j=1}^{d}\\left(1 + \\log\\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)$$\n",
        "\n",
        "### 8.4 Mutual Information\n",
        "\n",
        "**Definition:**\n",
        "$$I(X; Y) = D_{KL}(p(x, y) \\| p(x)p(y)) = \\mathbb{E}_{p(x,y)}\\left[\\log \\frac{p(x, y)}{p(x)p(y)}\\right]$$\n",
        "\n",
        "**Equivalent Forms:**\n",
        "$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X, Y)$$\n",
        "\n",
        "**Properties:**\n",
        "- $I(X; Y) \\geq 0$\n",
        "- $I(X; Y) = 0 \\Leftrightarrow X \\perp\\!\\!\\!\\perp Y$\n",
        "- $I(X; Y) = I(Y; X)$ (symmetric)\n",
        "\n",
        "**Gen-AI Application:** InfoGAN, contrastive learning objectives.\n",
        "\n",
        "### 8.5 Jensen-Shannon Divergence\n",
        "\n",
        "**Definition:**\n",
        "$$D_{JS}(p \\| q) = \\frac{1}{2}D_{KL}(p \\| m) + \\frac{1}{2}D_{KL}(q \\| m)$$\n",
        "\n",
        "where $m = \\frac{1}{2}(p + q)$\n",
        "\n",
        "**Properties:**\n",
        "- Symmetric: $D_{JS}(p \\| q) = D_{JS}(q \\| p)$\n",
        "- Bounded: $0 \\leq D_{JS}(p \\| q) \\leq \\log 2$\n",
        "- $\\sqrt{D_{JS}}$ is a metric\n",
        "\n",
        "**Gen-AI Application:** Original GAN objective is related to JS divergence:\n",
        "$$\\mathcal{L}_{GAN} = 2 \\cdot D_{JS}(p_{data} \\| p_g) - \\log 4$$\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "### 9.1 Definition and Formulation\n",
        "\n",
        "**Definition:** MLE finds parameters $\\theta$ that maximize the likelihood of observed data:\n",
        "\n",
        "$$\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} p(x_1, \\ldots, x_n | \\theta)$$\n",
        "\n",
        "**For i.i.d. data:**\n",
        "$$\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} \\prod_{i=1}^{n} p(x_i | \\theta)$$\n",
        "\n",
        "### 9.2 Log-Likelihood\n",
        "\n",
        "**Definition:** Taking logarithm for numerical stability and mathematical convenience:\n",
        "\n",
        "$$\\mathcal{L}(\\theta) = \\log p(\\mathcal{D}|\\theta) = \\sum_{i=1}^{n} \\log p(x_i | \\theta)$$\n",
        "\n",
        "**MLE Objective:**\n",
        "$$\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} \\mathcal{L}(\\theta) = \\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log p(x_i | \\theta)$$\n",
        "\n",
        "### 9.3 MLE as KL Minimization\n",
        "\n",
        "**Theorem:** MLE is equivalent to minimizing KL divergence between empirical distribution and model:\n",
        "\n",
        "$$\\hat{\\theta}_{MLE} = \\arg\\min_{\\theta} D_{KL}(\\hat{p}_{data} \\| p_\\theta)$$\n",
        "\n",
        "where $\\hat{p}_{data}(x) = \\frac{1}{n}\\sum_{i=1}^{n} \\delta(x - x_i)$\n",
        "\n",
        "**Proof:**\n",
        "$$D_{KL}(\\hat{p}_{data} \\| p_\\theta) = -H(\\hat{p}_{data}) - \\mathbb{E}_{\\hat{p}_{data}}[\\log p_\\theta(x)]$$\n",
        "\n",
        "Since $H(\\hat{p}_{data})$ is constant:\n",
        "$$\\arg\\min_\\theta D_{KL}(\\hat{p}_{data} \\| p_\\theta) = \\arg\\max_\\theta \\mathbb{E}_{\\hat{p}_{data}}[\\log p_\\theta(x)] = \\arg\\max_\\theta \\frac{1}{n}\\sum_{i=1}^{n} \\log p_\\theta(x_i)$$\n",
        "\n",
        "### 9.4 Properties of MLE\n",
        "\n",
        "**Consistency:** As $n \\rightarrow \\infty$, $\\hat{\\theta}_{MLE} \\xrightarrow{p} \\theta_{true}$\n",
        "\n",
        "**Asymptotic Normality:**\n",
        "$$\\sqrt{n}(\\hat{\\theta}_{MLE} - \\theta_{true}) \\xrightarrow{d} \\mathcal{N}(0, I(\\theta)^{-1})$$\n",
        "\n",
        "where $I(\\theta)$ is the Fisher Information Matrix.\n",
        "\n",
        "**Asymptotic Efficiency:** Achieves Cramér-Rao lower bound asymptotically.\n",
        "\n",
        "### 9.5 Fisher Information\n",
        "\n",
        "**Definition (Scalar):**\n",
        "$$I(\\theta) = \\mathbb{E}\\left[\\left(\\frac{\\partial \\log p(X|\\theta)}{\\partial \\theta}\\right)^2\\right] = -\\mathbb{E}\\left[\\frac{\\partial^2 \\log p(X|\\theta)}{\\partial \\theta^2}\\right]$$\n",
        "\n",
        "**Definition (Matrix Form):**\n",
        "$$[I(\\theta)]_{ij} = \\mathbb{E}\\left[\\frac{\\partial \\log p(X|\\theta)}{\\partial \\theta_i} \\frac{\\partial \\log p(X|\\theta)}{\\partial \\theta_j}\\right]$$\n",
        "\n",
        "**Cramér-Rao Bound:**\n",
        "$$\\text{Var}(\\hat{\\theta}) \\geq I(\\theta)^{-1}$$\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Bayesian Inference\n",
        "\n",
        "### 10.1 Bayesian Framework\n",
        "\n",
        "**Prior Distribution:** $p(\\theta)$ - encodes prior belief about parameters\n",
        "\n",
        "**Likelihood:** $p(\\mathcal{D}|\\theta)$ - probability of data given parameters\n",
        "\n",
        "**Posterior Distribution:** Using Bayes' theorem:\n",
        "$$p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})} = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{\\int p(\\mathcal{D}|\\theta') p(\\theta') d\\theta'}$$\n",
        "\n",
        "### 10.2 Posterior Predictive Distribution\n",
        "\n",
        "**Definition:** Distribution over new data point $x^*$:\n",
        "$$p(x^*|\\mathcal{D}) = \\int p(x^*|\\theta) p(\\theta|\\mathcal{D}) \\, d\\theta$$\n",
        "\n",
        "### 10.3 Maximum A Posteriori (MAP) Estimation\n",
        "\n",
        "**Definition:**\n",
        "$$\\hat{\\theta}_{MAP} = \\arg\\max_{\\theta} p(\\theta|\\mathcal{D}) = \\arg\\max_{\\theta} [p(\\mathcal{D}|\\theta) p(\\theta)]$$\n",
        "\n",
        "**Log Form:**\n",
        "$$\\hat{\\theta}_{MAP} = \\arg\\max_{\\theta} [\\log p(\\mathcal{D}|\\theta) + \\log p(\\theta)]$$\n",
        "\n",
        "**Relationship to Regularization:**\n",
        "- Gaussian prior $p(\\theta) \\propto e^{-\\frac{\\lambda}{2}\\|\\theta\\|^2}$ → L2 regularization\n",
        "- Laplace prior $p(\\theta) \\propto e^{-\\lambda\\|\\theta\\|_1}$ → L1 regularization\n",
        "\n",
        "### 10.4 Conjugate Priors\n",
        "\n",
        "**Definition:** A prior $p(\\theta)$ is conjugate to likelihood $p(x|\\theta)$ if the posterior $p(\\theta|x)$ belongs to the same family as the prior.\n",
        "\n",
        "| Likelihood | Conjugate Prior | Posterior |\n",
        "|------------|-----------------|-----------|\n",
        "| Bernoulli | Beta | Beta |\n",
        "| Multinomial | Dirichlet | Dirichlet |\n",
        "| Gaussian (known $\\sigma$) | Gaussian | Gaussian |\n",
        "| Gaussian (known $\\mu$) | Inverse-Gamma | Inverse-Gamma |\n",
        "| Poisson | Gamma | Gamma |\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Latent Variable Models\n",
        "\n",
        "### 11.1 Definition and Motivation\n",
        "\n",
        "**Definition:** Latent variable models introduce unobserved variables $z$ to capture hidden structure:\n",
        "$$p_\\theta(x) = \\int p_\\theta(x, z) \\, dz = \\int p_\\theta(x|z) p(z) \\, dz$$\n",
        "\n",
        "**Components:**\n",
        "- $z$: latent variables (hidden factors)\n",
        "- $p(z)$: prior over latent space\n",
        "- $p_\\theta(x|z)$: decoder/generator\n",
        "- $p_\\theta(x)$: marginal likelihood (evidence)\n",
        "\n",
        "### 11.2 The Inference Problem\n",
        "\n",
        "**Goal:** Compute posterior $p(z|x)$ for inference:\n",
        "$$p(z|x) = \\frac{p(x|z)p(z)}{p(x)} = \\frac{p(x|z)p(z)}{\\int p(x|z')p(z') \\, dz'}$$\n",
        "\n",
        "**Problem:** The integral $p(x) = \\int p(x|z)p(z) \\, dz$ is often intractable.\n",
        "\n",
        "### 11.3 Evidence Lower Bound (ELBO)\n",
        "\n",
        "**Derivation:** For any distribution $q(z|x)$:\n",
        "\n",
        "$$\\log p(x) = \\log \\int p(x, z) \\, dz = \\log \\int \\frac{p(x, z)}{q(z|x)} q(z|x) \\, dz$$\n",
        "\n",
        "Using Jensen's inequality:\n",
        "$$\\log p(x) \\geq \\int q(z|x) \\log \\frac{p(x, z)}{q(z|x)} \\, dz = \\mathcal{L}(x; \\theta, \\phi)$$\n",
        "\n",
        "**ELBO Formulation:**\n",
        "$$\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p(z))$$\n",
        "\n",
        "where:\n",
        "- First term: Reconstruction term\n",
        "- Second term: Regularization term (KL to prior)\n",
        "\n",
        "**Alternative Form:**\n",
        "$$\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)]$$\n",
        "\n",
        "**Gap to Evidence:**\n",
        "$$\\log p(x) = \\mathcal{L}(x; \\theta, \\phi) + D_{KL}(q_\\phi(z|x) \\| p_\\theta(z|x))$$\n",
        "\n",
        "Since $D_{KL} \\geq 0$, ELBO is a lower bound, with equality when $q_\\phi(z|x) = p_\\theta(z|x)$.\n",
        "\n",
        "### 11.4 Variational Inference\n",
        "\n",
        "**Objective:** Approximate intractable posterior with tractable family:\n",
        "$$q^*(z|x) = \\arg\\min_{q \\in \\mathcal{Q}} D_{KL}(q(z|x) \\| p(z|x))$$\n",
        "\n",
        "**Equivalently:** Maximize ELBO:\n",
        "$$q^*(z|x) = \\arg\\max_{q \\in \\mathcal{Q}} \\mathcal{L}(x; \\theta, q)$$\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Sampling and Monte Carlo Methods\n",
        "\n",
        "### 12.1 Law of Large Numbers\n",
        "\n",
        "**Theorem (Strong LLN):** For i.i.d. random variables $X_1, X_2, \\ldots$ with $\\mathbb{E}[X_i] = \\mu$:\n",
        "$$\\frac{1}{n}\\sum_{i=1}^{n} X_i \\xrightarrow{a.s.} \\mu \\quad \\text{as } n \\rightarrow \\infty$$\n",
        "\n",
        "### 12.2 Central Limit Theorem\n",
        "\n",
        "**Theorem:** For i.i.d. random variables with mean $\\mu$ and variance $\\sigma^2$:\n",
        "$$\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0, 1)$$\n",
        "\n",
        "Equivalently:\n",
        "$$\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2)$$\n",
        "\n",
        "### 12.3 Monte Carlo Estimation\n",
        "\n",
        "**Goal:** Estimate expectation $\\mathbb{E}_{p(x)}[f(x)]$\n",
        "\n",
        "**Monte Carlo Estimator:** Draw samples $x_1, \\ldots, x_n \\sim p(x)$:\n",
        "$$\\hat{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^{n} f(x_i)$$\n",
        "\n",
        "**Properties:**\n",
        "- Unbiased: $\\mathbb{E}[\\hat{\\mu}_n] = \\mathbb{E}_{p}[f(x)]$\n",
        "- Variance: $\\text{Var}(\\hat{\\mu}_n) = \\frac{\\text{Var}(f(X))}{n}$\n",
        "- Convergence rate: $O(1/\\sqrt{n})$ regardless of dimension\n",
        "\n",
        "### 12.4 Importance Sampling\n",
        "\n",
        "**Motivation:** Sample from proposal $q(x)$ instead of target $p(x)$.\n",
        "\n",
        "**Identity:**\n",
        "$$\\mathbb{E}_{p}[f(x)] = \\int f(x) p(x) \\, dx = \\int f(x) \\frac{p(x)}{q(x)} q(x) \\, dx = \\mathbb{E}_{q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]$$\n",
        "\n",
        "**Importance Weight:**\n",
        "$$w(x) = \\frac{p(x)}{q(x)}$$\n",
        "\n",
        "**Importance Sampling Estimator:**\n",
        "$$\\hat{\\mu}_{IS} = \\frac{1}{n}\\sum_{i=1}^{n} f(x_i) w(x_i), \\quad x_i \\sim q$$\n",
        "\n",
        "**Self-Normalized Importance Sampling:** When $p(x)$ known up to constant:\n",
        "$$\\hat{\\mu}_{SNIS} = \\frac{\\sum_{i=1}^{n} f(x_i) w(x_i)}{\\sum_{i=1}^{n} w(x_i)}$$\n",
        "\n",
        "### 12.5 Reparameterization Trick\n",
        "\n",
        "**Problem:** Need gradients through sampling operation for VAE training.\n",
        "\n",
        "**Solution:** Express $z \\sim q_\\phi(z|x)$ as deterministic transformation of noise.\n",
        "\n",
        "**For Gaussian $q_\\phi(z|x) = \\mathcal{N}(\\mu_\\phi(x), \\sigma^2_\\phi(x))$:**\n",
        "$$z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
        "\n",
        "**Gradient Computation:**\n",
        "$$\\nabla_\\phi \\mathbb{E}_{q_\\phi(z|x)}[f(z)] = \\nabla_\\phi \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)}[f(\\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon)]$$\n",
        "$$= \\mathbb{E}_{\\epsilon}[\\nabla_\\phi f(\\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon)]$$\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Probability Theory in Major Generative Models\n",
        "\n",
        "### 13.1 Variational Autoencoders (VAEs)\n",
        "\n",
        "**Generative Model:**\n",
        "$$p(z) = \\mathcal{N}(0, I)$$\n",
        "$$p_\\theta(x|z) = \\mathcal{N}(\\mu_\\theta(z), \\sigma^2_\\theta(z))$$\n",
        "\n",
        "**Inference Model:**\n",
        "$$q_\\phi(z|x) = \\mathcal{N}(\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))$$\n",
        "\n",
        "**Loss Function:**\n",
        "$$\\mathcal{L}_{VAE} = -\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] + D_{KL}(q_\\phi(z|x) \\| p(z))$$\n",
        "\n",
        "### 13.2 Generative Adversarial Networks (GANs)\n",
        "\n",
        "**Generator:** $G_\\theta: z \\rightarrow x$, where $z \\sim p(z)$\n",
        "\n",
        "**Discriminator Objective:**\n",
        "$$\\max_D \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log(1 - D(G(z)))]$$\n",
        "\n",
        "**Generator Objective:**\n",
        "$$\\min_G \\mathbb{E}_{z \\sim p(z)}[\\log(1 - D(G(z)))]$$\n",
        "\n",
        "**Optimal Discriminator:**\n",
        "$$D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$$\n",
        "\n",
        "### 13.3 Diffusion Models\n",
        "\n",
        "**Forward Process (Noising):**\n",
        "$$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)$$\n",
        "\n",
        "$$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1-\\bar{\\alpha}_t)I)$$\n",
        "\n",
        "where $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$\n",
        "\n",
        "**Reverse Process (Denoising):**\n",
        "$$p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$$\n",
        "\n",
        "**Training Objective (Simplified):**\n",
        "$$\\mathcal{L}_{simple} = \\mathbb{E}_{t, x_0, \\epsilon}\\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right]$$\n",
        "\n",
        "### 13.4 Autoregressive Models\n",
        "\n",
        "**Factorization:**\n",
        "$$p(x_1, \\ldots, x_T) = \\prod_{t=1}^{T} p(x_t | x_{<t})$$\n",
        "\n",
        "**Language Model (Token Prediction):**\n",
        "$$p(x_t | x_{<t}) = \\text{softmax}(W h_t)$$\n",
        "\n",
        "**Training Objective (Cross-Entropy):**\n",
        "$$\\mathcal{L}_{AR} = -\\sum_{t=1}^{T} \\log p_\\theta(x_t | x_{<t})$$\n",
        "\n",
        "### 13.5 Normalizing Flows\n",
        "\n",
        "**Transformation:** $z = f_\\theta(x)$ where $f_\\theta$ is invertible\n",
        "\n",
        "**Change of Variables:**\n",
        "$$p_X(x) = p_Z(f_\\theta(x)) \\left|\\det\\left(\\frac{\\partial f_\\theta(x)}{\\partial x}\\right)\\right|$$\n",
        "\n",
        "**Log-Likelihood:**\n",
        "$$\\log p_X(x) = \\log p_Z(f_\\theta(x)) + \\log\\left|\\det(J_{f_\\theta}(x))\\right|$$\n",
        "\n",
        "---\n",
        "\n",
        "## 14. Summary: Probability Foundations for Gen-AI\n",
        "\n",
        "| Concept | Mathematical Form | Gen-AI Application |\n",
        "|---------|-------------------|-------------------|\n",
        "| Bayes' Theorem | $p(\\theta\\|x) \\propto p(x\\|\\theta)p(\\theta)$ | Posterior inference in VAEs |\n",
        "| Chain Rule | $p(x_{1:T}) = \\prod_t p(x_t\\|x_{<t})$ | Autoregressive models |\n",
        "| Marginalization | $p(x) = \\int p(x\\|z)p(z)dz$ | Latent variable models |\n",
        "| KL Divergence | $D_{KL}(q\\|\\|p) = \\mathbb{E}_q[\\log q/p]$ | VAE loss, divergence minimization |\n",
        "| ELBO | $\\log p(x) \\geq \\mathbb{E}_q[\\log p(x,z)/q(z)]$ | VAE objective |\n",
        "| MLE | $\\arg\\max_\\theta \\sum_i \\log p_\\theta(x_i)$ | Training objective |\n",
        "| Reparameterization | $z = \\mu + \\sigma \\odot \\epsilon$ | Gradient through sampling |\n",
        "| Change of Variables | $p_x(x) = p_z(f(x))\\|\\det J_f\\|$ | Normalizing flows |\n",
        "\n",
        "---\n",
        "\n",
        "## 15. Key Takeaways\n",
        "\n",
        "1. **Generative AI = Learning Probability Distributions:** All generative models aim to learn $p_{data}(x)$ either explicitly or implicitly.\n",
        "\n",
        "2. **Latent Variables Enable Structure:** Introducing $z$ allows modeling complex distributions via simpler conditional distributions.\n",
        "\n",
        "3. **Intractability Drives Methodology:** Intractable integrals/posteriors motivate variational inference, MCMC, and amortized inference.\n",
        "\n",
        "4. **Information Theory Provides Objectives:** KL divergence, cross-entropy, and mutual information form the basis of training losses.\n",
        "\n",
        "5. **Sampling is Fundamental:** Both training (Monte Carlo gradients) and inference (generation) rely on efficient sampling."
      ],
      "metadata": {
        "id": "4VfR19oXaFSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probability and Statistics for Generative AI\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "1. [Foundational Concepts](#1-foundational-concepts)\n",
        "2. [Random Variables](#2-random-variables)\n",
        "3. [Probability Distributions](#3-probability-distributions)\n",
        "4. [Higher-Order Probability Distributions](#4-higher-order-probability-distributions)\n",
        "5. [Stochastic Processes](#5-stochastic-processes)\n",
        "6. [Non-Stochastic Processes](#6-non-stochastic-processes)\n",
        "7. [Principles of Generative AI](#7-principles-of-generative-ai)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Foundational Concepts\n",
        "\n",
        "### 1.1 Probability Space\n",
        "\n",
        "**Definition:** A probability space is a mathematical construct $(\\Omega, \\mathcal{F}, P)$ that provides a formal model for randomness.\n",
        "\n",
        "| Component | Symbol | Description |\n",
        "|-----------|--------|-------------|\n",
        "| Sample Space | $\\Omega$ | Set of all possible outcomes |\n",
        "| Event Space | $\\mathcal{F}$ | $\\sigma$-algebra of subsets of $\\Omega$ |\n",
        "| Probability Measure | $P$ | Function mapping events to $[0,1]$ |\n",
        "\n",
        "**Kolmogorov Axioms:**\n",
        "\n",
        "$$P(\\Omega) = 1$$\n",
        "\n",
        "$$P(A) \\geq 0 \\quad \\forall A \\in \\mathcal{F}$$\n",
        "\n",
        "$$P\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} P(A_i) \\quad \\text{for mutually exclusive } A_i$$\n",
        "\n",
        "### 1.2 Conditional Probability\n",
        "\n",
        "**Definition:** The probability of event $A$ given event $B$ has occurred:\n",
        "\n",
        "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) > 0$$\n",
        "\n",
        "### 1.3 Bayes' Theorem\n",
        "\n",
        "**Definition:** Fundamental theorem for posterior inference in generative models:\n",
        "\n",
        "$$P(\\theta|X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)}$$\n",
        "\n",
        "where:\n",
        "- $P(\\theta|X)$ = **Posterior** distribution\n",
        "- $P(X|\\theta)$ = **Likelihood** function\n",
        "- $P(\\theta)$ = **Prior** distribution\n",
        "- $P(X)$ = **Evidence** (marginal likelihood)\n",
        "\n",
        "**Marginal Likelihood Computation:**\n",
        "\n",
        "$$P(X) = \\int P(X|\\theta)P(\\theta)d\\theta$$\n",
        "\n",
        "### 1.4 Chain Rule of Probability\n",
        "\n",
        "**Definition:** Decomposition of joint probability into conditional factors:\n",
        "\n",
        "$$P(X_1, X_2, \\ldots, X_n) = \\prod_{i=1}^{n} P(X_i | X_1, X_2, \\ldots, X_{i-1})$$\n",
        "\n",
        "**Gen-AI Application:** Autoregressive language models (GPT) directly model this factorization:\n",
        "\n",
        "$$P(\\text{sequence}) = \\prod_{t=1}^{T} P(x_t | x_{<t})$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Random Variables\n",
        "\n",
        "### 2.1 Definition\n",
        "\n",
        "**Definition:** A random variable is a measurable function $X: \\Omega \\rightarrow \\mathbb{R}$ that maps outcomes from a sample space to real numbers.\n",
        "\n",
        "$$X: \\Omega \\rightarrow \\mathbb{R}$$\n",
        "\n",
        "### 2.2 Types of Random Variables\n",
        "\n",
        "#### 2.2.1 Discrete Random Variables\n",
        "\n",
        "**Definition:** Takes countable values with associated probability mass function (PMF):\n",
        "\n",
        "$$P(X = x_i) = p_i, \\quad \\sum_{i} p_i = 1$$\n",
        "\n",
        "**Properties:**\n",
        "- Cumulative Distribution Function (CDF):\n",
        "$$F_X(x) = P(X \\leq x) = \\sum_{x_i \\leq x} P(X = x_i)$$\n",
        "\n",
        "#### 2.2.2 Continuous Random Variables\n",
        "\n",
        "**Definition:** Takes uncountably infinite values with probability density function (PDF):\n",
        "\n",
        "$$P(a \\leq X \\leq b) = \\int_a^b f_X(x)dx$$\n",
        "\n",
        "**Normalization Constraint:**\n",
        "\n",
        "$$\\int_{-\\infty}^{\\infty} f_X(x)dx = 1$$\n",
        "\n",
        "**CDF-PDF Relationship:**\n",
        "\n",
        "$$F_X(x) = \\int_{-\\infty}^{x} f_X(t)dt$$\n",
        "\n",
        "$$f_X(x) = \\frac{dF_X(x)}{dx}$$\n",
        "\n",
        "### 2.3 Moments of Random Variables\n",
        "\n",
        "#### 2.3.1 Expected Value (First Moment)\n",
        "\n",
        "**Discrete:**\n",
        "$$\\mathbb{E}[X] = \\sum_{i} x_i P(X = x_i)$$\n",
        "\n",
        "**Continuous:**\n",
        "$$\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f_X(x)dx$$\n",
        "\n",
        "#### 2.3.2 Variance (Second Central Moment)\n",
        "\n",
        "$$\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$$\n",
        "\n",
        "#### 2.3.3 Higher-Order Moments\n",
        "\n",
        "**$n$-th Moment:**\n",
        "$$\\mu_n = \\mathbb{E}[X^n]$$\n",
        "\n",
        "**$n$-th Central Moment:**\n",
        "$$\\mu'_n = \\mathbb{E}[(X - \\mu)^n]$$\n",
        "\n",
        "**Skewness (3rd standardized moment):**\n",
        "$$\\gamma_1 = \\frac{\\mathbb{E}[(X-\\mu)^3]}{\\sigma^3}$$\n",
        "\n",
        "**Kurtosis (4th standardized moment):**\n",
        "$$\\gamma_2 = \\frac{\\mathbb{E}[(X-\\mu)^4]}{\\sigma^4}$$\n",
        "\n",
        "### 2.4 Moment Generating Function (MGF)\n",
        "\n",
        "**Definition:**\n",
        "$$M_X(t) = \\mathbb{E}[e^{tX}] = \\int_{-\\infty}^{\\infty} e^{tx} f_X(x)dx$$\n",
        "\n",
        "**Moment Extraction:**\n",
        "$$\\mathbb{E}[X^n] = \\frac{d^n M_X(t)}{dt^n}\\bigg|_{t=0}$$\n",
        "\n",
        "### 2.5 Characteristic Function\n",
        "\n",
        "**Definition:**\n",
        "$$\\phi_X(t) = \\mathbb{E}[e^{itX}] = \\int_{-\\infty}^{\\infty} e^{itx} f_X(x)dx$$\n",
        "\n",
        "**Properties:**\n",
        "- Always exists (unlike MGF)\n",
        "- $\\phi_X(0) = 1$\n",
        "- $|\\phi_X(t)| \\leq 1$\n",
        "- Uniquely determines distribution\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Probability Distributions\n",
        "\n",
        "### 3.1 Discrete Distributions\n",
        "\n",
        "#### 3.1.1 Bernoulli Distribution\n",
        "\n",
        "**Definition:** Models single binary trial.\n",
        "\n",
        "$$X \\sim \\text{Bernoulli}(p)$$\n",
        "\n",
        "$$P(X = k) = p^k(1-p)^{1-k}, \\quad k \\in \\{0, 1\\}$$\n",
        "\n",
        "| Statistic | Value |\n",
        "|-----------|-------|\n",
        "| Mean | $\\mu = p$ |\n",
        "| Variance | $\\sigma^2 = p(1-p)$ |\n",
        "\n",
        "#### 3.1.2 Binomial Distribution\n",
        "\n",
        "**Definition:** Number of successes in $n$ independent Bernoulli trials.\n",
        "\n",
        "$$X \\sim \\text{Binomial}(n, p)$$\n",
        "\n",
        "$$P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
        "\n",
        "| Statistic | Value |\n",
        "|-----------|-------|\n",
        "| Mean | $\\mu = np$ |\n",
        "| Variance | $\\sigma^2 = np(1-p)$ |\n",
        "\n",
        "#### 3.1.3 Categorical Distribution\n",
        "\n",
        "**Definition:** Generalization of Bernoulli to $K$ categories.\n",
        "\n",
        "$$X \\sim \\text{Categorical}(\\pi_1, \\pi_2, \\ldots, \\pi_K)$$\n",
        "\n",
        "$$P(X = k) = \\pi_k, \\quad \\sum_{k=1}^{K} \\pi_k = 1$$\n",
        "\n",
        "**Gen-AI Application:** Output layer of classification/language models uses softmax to parameterize categorical distribution:\n",
        "\n",
        "$$\\pi_k = \\frac{\\exp(z_k)}{\\sum_{j=1}^{K} \\exp(z_j)}$$\n",
        "\n",
        "#### 3.1.4 Multinomial Distribution\n",
        "\n",
        "**Definition:** Multiple draws from categorical distribution.\n",
        "\n",
        "$$\\mathbf{X} \\sim \\text{Multinomial}(n, \\boldsymbol{\\pi})$$\n",
        "\n",
        "$$P(X_1 = x_1, \\ldots, X_K = x_K) = \\frac{n!}{\\prod_{k=1}^{K} x_k!} \\prod_{k=1}^{K} \\pi_k^{x_k}$$\n",
        "\n",
        "#### 3.1.5 Poisson Distribution\n",
        "\n",
        "**Definition:** Models count of events in fixed interval.\n",
        "\n",
        "$$X \\sim \\text{Poisson}(\\lambda)$$\n",
        "\n",
        "$$P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n",
        "\n",
        "| Statistic | Value |\n",
        "|-----------|-------|\n",
        "| Mean | $\\mu = \\lambda$ |\n",
        "| Variance | $\\sigma^2 = \\lambda$ |\n",
        "\n",
        "#### 3.1.6 Geometric Distribution\n",
        "\n",
        "**Definition:** Number of trials until first success.\n",
        "\n",
        "$$P(X = k) = (1-p)^{k-1}p, \\quad k \\in \\{1, 2, 3, \\ldots\\}$$\n",
        "\n",
        "### 3.2 Continuous Distributions\n",
        "\n",
        "#### 3.2.1 Uniform Distribution\n",
        "\n",
        "**Definition:**\n",
        "$$X \\sim \\text{Uniform}(a, b)$$\n",
        "\n",
        "$$f_X(x) = \\frac{1}{b-a}, \\quad x \\in [a, b]$$\n",
        "\n",
        "| Statistic | Value |\n",
        "|-----------|-------|\n",
        "| Mean | $\\mu = \\frac{a+b}{2}$ |\n",
        "| Variance | $\\sigma^2 = \\frac{(b-a)^2}{12}$ |\n",
        "\n",
        "#### 3.2.2 Gaussian (Normal) Distribution\n",
        "\n",
        "**Definition:** The most fundamental distribution in generative AI.\n",
        "\n",
        "$$X \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
        "\n",
        "$$f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
        "\n",
        "**Standard Normal ($\\mu=0$, $\\sigma=1$):**\n",
        "\n",
        "$$\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$$\n",
        "\n",
        "**Properties:**\n",
        "- Maximum entropy distribution for fixed mean and variance\n",
        "- Closed under linear transformations\n",
        "- Central Limit Theorem convergence target\n",
        "\n",
        "**Moment Generating Function:**\n",
        "$$M_X(t) = \\exp\\left(\\mu t + \\frac{\\sigma^2 t^2}{2}\\right)$$\n",
        "\n",
        "#### 3.2.3 Exponential Distribution\n",
        "\n",
        "**Definition:**\n",
        "$$X \\sim \\text{Exponential}(\\lambda)$$\n",
        "\n",
        "$$f_X(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0$$\n",
        "\n",
        "**Memoryless Property:**\n",
        "$$P(X > s + t | X > s) = P(X > t)$$\n",
        "\n",
        "#### 3.2.4 Gamma Distribution\n",
        "\n",
        "**Definition:**\n",
        "$$X \\sim \\text{Gamma}(\\alpha, \\beta)$$\n",
        "\n",
        "$$f_X(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}, \\quad x > 0$$\n",
        "\n",
        "where $\\Gamma(\\alpha) = \\int_0^{\\infty} t^{\\alpha-1}e^{-t}dt$\n",
        "\n",
        "#### 3.2.5 Beta Distribution\n",
        "\n",
        "**Definition:** Distribution over $[0,1]$, conjugate prior for Bernoulli.\n",
        "\n",
        "$$X \\sim \\text{Beta}(\\alpha, \\beta)$$\n",
        "\n",
        "$$f_X(x) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha-1}(1-x)^{\\beta-1}$$\n",
        "\n",
        "| Statistic | Value |\n",
        "|-----------|-------|\n",
        "| Mean | $\\mu = \\frac{\\alpha}{\\alpha + \\beta}$ |\n",
        "| Variance | $\\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$ |\n",
        "\n",
        "#### 3.2.6 Dirichlet Distribution\n",
        "\n",
        "**Definition:** Multivariate generalization of Beta, distribution over probability simplices.\n",
        "\n",
        "$$\\boldsymbol{\\pi} \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha})$$\n",
        "\n",
        "$$f(\\pi_1, \\ldots, \\pi_K) = \\frac{\\Gamma(\\sum_{k=1}^{K} \\alpha_k)}{\\prod_{k=1}^{K} \\Gamma(\\alpha_k)} \\prod_{k=1}^{K} \\pi_k^{\\alpha_k - 1}$$\n",
        "\n",
        "**Constraint:** $\\sum_{k=1}^{K} \\pi_k = 1$\n",
        "\n",
        "**Gen-AI Application:** Prior for topic models (LDA).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Higher-Order Probability Distributions\n",
        "\n",
        "### 4.1 Joint Distributions\n",
        "\n",
        "#### 4.1.1 Bivariate Joint Distribution\n",
        "\n",
        "**Definition:** Distribution over two random variables simultaneously.\n",
        "\n",
        "**Discrete:**\n",
        "$$P(X = x_i, Y = y_j) = p_{ij}$$\n",
        "\n",
        "**Continuous:**\n",
        "$$P(a \\leq X \\leq b, c \\leq Y \\leq d) = \\int_a^b \\int_c^d f_{X,Y}(x,y) \\, dy \\, dx$$\n",
        "\n",
        "#### 4.1.2 Marginal Distribution\n",
        "\n",
        "**Definition:** Distribution of single variable from joint.\n",
        "\n",
        "$$f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy$$\n",
        "\n",
        "$$f_Y(y) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dx$$\n",
        "\n",
        "#### 4.1.3 Conditional Distribution\n",
        "\n",
        "$$f_{Y|X}(y|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)}$$\n",
        "\n",
        "### 4.2 Multivariate Gaussian Distribution\n",
        "\n",
        "**Definition:** The cornerstone distribution for continuous latent space models.\n",
        "\n",
        "$$\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$$\n",
        "\n",
        "$$f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right)$$\n",
        "\n",
        "where:\n",
        "- $\\boldsymbol{\\mu} \\in \\mathbb{R}^d$ = Mean vector\n",
        "- $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d}$ = Covariance matrix (symmetric positive definite)\n",
        "- $|\\boldsymbol{\\Sigma}|$ = Determinant of covariance matrix\n",
        "\n",
        "#### 4.2.1 Covariance Matrix Structure\n",
        "\n",
        "$$\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\sigma_1^2 & \\sigma_{12} & \\cdots & \\sigma_{1d} \\\\ \\sigma_{21} & \\sigma_2^2 & \\cdots & \\sigma_{2d} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sigma_{d1} & \\sigma_{d2} & \\cdots & \\sigma_d^2 \\end{pmatrix}$$\n",
        "\n",
        "**Correlation Coefficient:**\n",
        "$$\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sigma_i \\sigma_j}$$\n",
        "\n",
        "#### 4.2.2 Mahalanobis Distance\n",
        "\n",
        "**Definition:** Generalized distance accounting for correlations.\n",
        "\n",
        "$$D_M(\\mathbf{x}) = \\sqrt{(\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})}$$\n",
        "\n",
        "#### 4.2.3 Conditional Multivariate Gaussian\n",
        "\n",
        "For partition $\\mathbf{X} = [\\mathbf{X}_1, \\mathbf{X}_2]^T$:\n",
        "\n",
        "$$\\boldsymbol{\\mu} = \\begin{pmatrix} \\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2 \\end{pmatrix}, \\quad \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\boldsymbol{\\Sigma}_{11} & \\boldsymbol{\\Sigma}_{12} \\\\ \\boldsymbol{\\Sigma}_{21} & \\boldsymbol{\\Sigma}_{22} \\end{pmatrix}$$\n",
        "\n",
        "**Conditional Distribution:**\n",
        "$$\\mathbf{X}_1 | \\mathbf{X}_2 = \\mathbf{x}_2 \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{1|2}, \\boldsymbol{\\Sigma}_{1|2})$$\n",
        "\n",
        "where:\n",
        "$$\\boldsymbol{\\mu}_{1|2} = \\boldsymbol{\\mu}_1 + \\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}(\\mathbf{x}_2 - \\boldsymbol{\\mu}_2)$$\n",
        "\n",
        "$$\\boldsymbol{\\Sigma}_{1|2} = \\boldsymbol{\\Sigma}_{11} - \\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\\boldsymbol{\\Sigma}_{21}$$\n",
        "\n",
        "#### 4.2.4 Precision Matrix (Inverse Covariance)\n",
        "\n",
        "$$\\boldsymbol{\\Lambda} = \\boldsymbol{\\Sigma}^{-1}$$\n",
        "\n",
        "**Gen-AI Application:** Sparse precision matrices encode conditional independence (Gaussian Graphical Models).\n",
        "\n",
        "### 4.3 Mixture Models\n",
        "\n",
        "#### 4.3.1 Gaussian Mixture Model (GMM)\n",
        "\n",
        "**Definition:** Weighted combination of Gaussian components.\n",
        "\n",
        "$$p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$\n",
        "\n",
        "**Constraints:**\n",
        "$$\\sum_{k=1}^{K} \\pi_k = 1, \\quad \\pi_k \\geq 0$$\n",
        "\n",
        "**Latent Variable Formulation:**\n",
        "$$p(\\mathbf{x}, z) = p(z)p(\\mathbf{x}|z) = \\pi_z \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_z, \\boldsymbol{\\Sigma}_z)$$\n",
        "\n",
        "#### 4.3.2 EM Algorithm for GMM\n",
        "\n",
        "**E-Step (Responsibility Computation):**\n",
        "$$\\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}$$\n",
        "\n",
        "**M-Step (Parameter Update):**\n",
        "$$N_k = \\sum_{n=1}^{N} \\gamma_{nk}$$\n",
        "\n",
        "$$\\boldsymbol{\\mu}_k^{\\text{new}} = \\frac{1}{N_k} \\sum_{n=1}^{N} \\gamma_{nk} \\mathbf{x}_n$$\n",
        "\n",
        "$$\\boldsymbol{\\Sigma}_k^{\\text{new}} = \\frac{1}{N_k} \\sum_{n=1}^{N} \\gamma_{nk} (\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}})(\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}})^T$$\n",
        "\n",
        "$$\\pi_k^{\\text{new}} = \\frac{N_k}{N}$$\n",
        "\n",
        "### 4.4 Exponential Family Distributions\n",
        "\n",
        "**Definition:** Unified form capturing most common distributions.\n",
        "\n",
        "$$p(\\mathbf{x}|\\boldsymbol{\\eta}) = h(\\mathbf{x}) \\exp\\left(\\boldsymbol{\\eta}^T \\mathbf{T}(\\mathbf{x}) - A(\\boldsymbol{\\eta})\\right)$$\n",
        "\n",
        "| Component | Description |\n",
        "|-----------|-------------|\n",
        "| $\\boldsymbol{\\eta}$ | Natural parameters |\n",
        "| $\\mathbf{T}(\\mathbf{x})$ | Sufficient statistics |\n",
        "| $A(\\boldsymbol{\\eta})$ | Log-partition function (normalizer) |\n",
        "| $h(\\mathbf{x})$ | Base measure |\n",
        "\n",
        "**Properties of Log-Partition Function:**\n",
        "\n",
        "$$\\nabla_{\\boldsymbol{\\eta}} A(\\boldsymbol{\\eta}) = \\mathbb{E}[\\mathbf{T}(\\mathbf{x})]$$\n",
        "\n",
        "$$\\nabla^2_{\\boldsymbol{\\eta}} A(\\boldsymbol{\\eta}) = \\text{Cov}[\\mathbf{T}(\\mathbf{x})]$$\n",
        "\n",
        "### 4.5 Copulas\n",
        "\n",
        "**Definition:** Functions that couple marginal distributions to form joint distributions.\n",
        "\n",
        "$$C: [0,1]^d \\rightarrow [0,1]$$\n",
        "\n",
        "**Sklar's Theorem:**\n",
        "$$F(x_1, \\ldots, x_d) = C(F_1(x_1), \\ldots, F_d(x_d))$$\n",
        "\n",
        "**Gaussian Copula:**\n",
        "$$C_{\\boldsymbol{\\Sigma}}(\\mathbf{u}) = \\Phi_{\\boldsymbol{\\Sigma}}(\\Phi^{-1}(u_1), \\ldots, \\Phi^{-1}(u_d))$$\n",
        "\n",
        "where $\\Phi_{\\boldsymbol{\\Sigma}}$ is multivariate standard normal CDF and $\\Phi^{-1}$ is univariate standard normal inverse CDF.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Stochastic Processes\n",
        "\n",
        "### 5.1 Definition\n",
        "\n",
        "**Definition:** A stochastic process is a collection of random variables $\\{X_t\\}_{t \\in T}$ indexed by a parameter set $T$ (typically time), defined on a probability space $(\\Omega, \\mathcal{F}, P)$.\n",
        "\n",
        "$$\\{X_t : t \\in T\\}$$\n",
        "\n",
        "**Types by Index Set:**\n",
        "- **Discrete-time:** $T = \\{0, 1, 2, \\ldots\\}$\n",
        "- **Continuous-time:** $T = [0, \\infty)$\n",
        "\n",
        "**Types by State Space:**\n",
        "- **Discrete state:** $X_t \\in \\{s_1, s_2, \\ldots\\}$\n",
        "- **Continuous state:** $X_t \\in \\mathbb{R}^d$\n",
        "\n",
        "### 5.2 Markov Chains\n",
        "\n",
        "#### 5.2.1 Definition\n",
        "\n",
        "**Definition:** A stochastic process satisfying the Markov property (memoryless).\n",
        "\n",
        "$$P(X_{t+1} | X_t, X_{t-1}, \\ldots, X_0) = P(X_{t+1} | X_t)$$\n",
        "\n",
        "#### 5.2.2 Transition Matrix\n",
        "\n",
        "**Definition:** For discrete state space with $n$ states:\n",
        "\n",
        "$$\\mathbf{P} = \\begin{pmatrix} p_{11} & p_{12} & \\cdots & p_{1n} \\\\ p_{21} & p_{22} & \\cdots & p_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ p_{n1} & p_{n2} & \\cdots & p_{nn} \\end{pmatrix}$$\n",
        "\n",
        "where $p_{ij} = P(X_{t+1} = j | X_t = i)$\n",
        "\n",
        "**Row Stochastic Property:**\n",
        "$$\\sum_{j=1}^{n} p_{ij} = 1 \\quad \\forall i$$\n",
        "\n",
        "#### 5.2.3 Chapman-Kolmogorov Equation\n",
        "\n",
        "$$P(X_{t+s} = j | X_0 = i) = \\sum_{k} P(X_t = k | X_0 = i) P(X_{t+s} = j | X_t = k)$$\n",
        "\n",
        "$$\\mathbf{P}^{(t+s)} = \\mathbf{P}^{(t)} \\mathbf{P}^{(s)}$$\n",
        "\n",
        "#### 5.2.4 Stationary Distribution\n",
        "\n",
        "**Definition:** Distribution $\\boldsymbol{\\pi}$ satisfying:\n",
        "\n",
        "$$\\boldsymbol{\\pi}^T \\mathbf{P} = \\boldsymbol{\\pi}^T$$\n",
        "\n",
        "$$\\boldsymbol{\\pi} = \\lim_{t \\rightarrow \\infty} \\mathbf{P}^t \\boldsymbol{\\pi}_0$$\n",
        "\n",
        "**Detailed Balance (Reversibility):**\n",
        "$$\\pi_i p_{ij} = \\pi_j p_{ji}$$\n",
        "\n",
        "### 5.3 Markov Chain Monte Carlo (MCMC)\n",
        "\n",
        "#### 5.3.1 Metropolis-Hastings Algorithm\n",
        "\n",
        "**Objective:** Sample from target distribution $p(\\mathbf{x})$.\n",
        "\n",
        "**Acceptance Probability:**\n",
        "$$\\alpha(\\mathbf{x}' | \\mathbf{x}) = \\min\\left(1, \\frac{p(\\mathbf{x}')q(\\mathbf{x}|\\mathbf{x}')}{p(\\mathbf{x})q(\\mathbf{x}'|\\mathbf{x})}\\right)$$\n",
        "\n",
        "where $q(\\mathbf{x}'|\\mathbf{x})$ is proposal distribution.\n",
        "\n",
        "**Algorithm:**\n",
        "1. Initialize $\\mathbf{x}_0$\n",
        "2. For $t = 0, 1, 2, \\ldots$:\n",
        "   - Sample $\\mathbf{x}' \\sim q(\\mathbf{x}'|\\mathbf{x}_t)$\n",
        "   - Compute $\\alpha = \\alpha(\\mathbf{x}' | \\mathbf{x}_t)$\n",
        "   - Sample $u \\sim \\text{Uniform}(0,1)$\n",
        "   - If $u < \\alpha$: $\\mathbf{x}_{t+1} = \\mathbf{x}'$, else: $\\mathbf{x}_{t+1} = \\mathbf{x}_t$\n",
        "\n",
        "#### 5.3.2 Gibbs Sampling\n",
        "\n",
        "**Definition:** Special case of Metropolis-Hastings with component-wise updates.\n",
        "\n",
        "For $\\mathbf{x} = (x_1, x_2, \\ldots, x_d)$:\n",
        "\n",
        "$$x_i^{(t+1)} \\sim p(x_i | x_1^{(t+1)}, \\ldots, x_{i-1}^{(t+1)}, x_{i+1}^{(t)}, \\ldots, x_d^{(t)})$$\n",
        "\n",
        "**Acceptance Probability:** Always 1.\n",
        "\n",
        "### 5.4 Gaussian Processes\n",
        "\n",
        "#### 5.4.1 Definition\n",
        "\n",
        "**Definition:** A collection of random variables, any finite subset of which has a joint Gaussian distribution.\n",
        "\n",
        "$$f(\\mathbf{x}) \\sim \\mathcal{GP}(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}'))$$\n",
        "\n",
        "where:\n",
        "- $m(\\mathbf{x}) = \\mathbb{E}[f(\\mathbf{x})]$ = Mean function\n",
        "- $k(\\mathbf{x}, \\mathbf{x}') = \\mathbb{E}[(f(\\mathbf{x}) - m(\\mathbf{x}))(f(\\mathbf{x}') - m(\\mathbf{x}'))]$ = Covariance (kernel) function\n",
        "\n",
        "#### 5.4.2 Common Kernel Functions\n",
        "\n",
        "**Squared Exponential (RBF):**\n",
        "$$k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2 \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}'\\|^2}{2l^2}\\right)$$\n",
        "\n",
        "**Matérn Kernel:**\n",
        "$$k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\frac{\\sqrt{2\\nu}r}{l}\\right)^\\nu K_\\nu\\left(\\frac{\\sqrt{2\\nu}r}{l}\\right)$$\n",
        "\n",
        "where $r = \\|\\mathbf{x} - \\mathbf{x}'\\|$ and $K_\\nu$ is modified Bessel function.\n",
        "\n",
        "#### 5.4.3 GP Posterior Inference\n",
        "\n",
        "Given observations $\\mathbf{y} = f(\\mathbf{X}) + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$:\n",
        "\n",
        "**Posterior Mean:**\n",
        "$$\\bar{f}(\\mathbf{x}_*) = \\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}$$\n",
        "\n",
        "**Posterior Variance:**\n",
        "$$\\text{Var}(f(\\mathbf{x}_*)) = k(\\mathbf{x}_*, \\mathbf{x}_*) - \\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*$$\n",
        "\n",
        "where $\\mathbf{K}$ is kernel matrix and $\\mathbf{k}_* = [k(\\mathbf{x}_*, \\mathbf{x}_1), \\ldots, k(\\mathbf{x}_*, \\mathbf{x}_n)]^T$.\n",
        "\n",
        "### 5.5 Wiener Process (Brownian Motion)\n",
        "\n",
        "#### 5.5.1 Definition\n",
        "\n",
        "**Definition:** Continuous-time stochastic process $\\{W_t\\}_{t \\geq 0}$ with properties:\n",
        "\n",
        "1. $W_0 = 0$\n",
        "2. Independent increments: $W_t - W_s$ is independent of $\\{W_u : u \\leq s\\}$\n",
        "3. Gaussian increments: $W_t - W_s \\sim \\mathcal{N}(0, t-s)$ for $t > s$\n",
        "4. Continuous paths\n",
        "\n",
        "**Properties:**\n",
        "$$\\mathbb{E}[W_t] = 0$$\n",
        "$$\\text{Cov}(W_s, W_t) = \\min(s, t)$$\n",
        "\n",
        "### 5.6 Stochastic Differential Equations (SDEs)\n",
        "\n",
        "#### 5.6.1 Itô SDE\n",
        "\n",
        "**Definition:**\n",
        "$$d\\mathbf{X}_t = \\boldsymbol{\\mu}(\\mathbf{X}_t, t)dt + \\boldsymbol{\\sigma}(\\mathbf{X}_t, t)d\\mathbf{W}_t$$\n",
        "\n",
        "where:\n",
        "- $\\boldsymbol{\\mu}(\\mathbf{X}_t, t)$ = Drift coefficient\n",
        "- $\\boldsymbol{\\sigma}(\\mathbf{X}_t, t)$ = Diffusion coefficient\n",
        "- $d\\mathbf{W}_t$ = Wiener process increment\n",
        "\n",
        "#### 5.6.2 Itô's Lemma\n",
        "\n",
        "For function $f(\\mathbf{X}_t, t)$:\n",
        "\n",
        "$$df = \\frac{\\partial f}{\\partial t}dt + \\nabla_{\\mathbf{x}} f \\cdot d\\mathbf{X}_t + \\frac{1}{2} \\text{tr}\\left(\\boldsymbol{\\sigma}\\boldsymbol{\\sigma}^T \\nabla^2_{\\mathbf{x}} f\\right)dt$$\n",
        "\n",
        "#### 5.6.3 Fokker-Planck Equation\n",
        "\n",
        "**Definition:** Evolution equation for probability density $p(\\mathbf{x}, t)$:\n",
        "\n",
        "$$\\frac{\\partial p}{\\partial t} = -\\nabla \\cdot (\\boldsymbol{\\mu} p) + \\frac{1}{2} \\nabla \\cdot \\nabla \\cdot (\\boldsymbol{\\sigma}\\boldsymbol{\\sigma}^T p)$$\n",
        "\n",
        "### 5.7 Ornstein-Uhlenbeck Process\n",
        "\n",
        "**Definition:** Mean-reverting Gaussian process:\n",
        "\n",
        "$$d\\mathbf{X}_t = \\theta(\\boldsymbol{\\mu} - \\mathbf{X}_t)dt + \\sigma d\\mathbf{W}_t$$\n",
        "\n",
        "**Solution:**\n",
        "$$\\mathbf{X}_t = \\boldsymbol{\\mu} + e^{-\\theta t}(\\mathbf{X}_0 - \\boldsymbol{\\mu}) + \\sigma \\int_0^t e^{-\\theta(t-s)} d\\mathbf{W}_s$$\n",
        "\n",
        "**Stationary Distribution:**\n",
        "$$\\mathbf{X}_\\infty \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}, \\frac{\\sigma^2}{2\\theta}\\mathbf{I}\\right)$$\n",
        "\n",
        "### 5.8 Diffusion Processes (Critical for Diffusion Models)\n",
        "\n",
        "#### 5.8.1 Forward Diffusion Process\n",
        "\n",
        "**Definition:** Gradually adds noise to data:\n",
        "\n",
        "$$d\\mathbf{x}_t = -\\frac{1}{2}\\beta(t)\\mathbf{x}_t dt + \\sqrt{\\beta(t)} d\\mathbf{W}_t$$\n",
        "\n",
        "**Marginal Distribution:**\n",
        "$$q(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0, (1-\\bar{\\alpha}_t)\\mathbf{I})$$\n",
        "\n",
        "where $\\bar{\\alpha}_t = \\exp\\left(-\\int_0^t \\beta(s)ds\\right)$\n",
        "\n",
        "#### 5.8.2 Reverse Diffusion Process\n",
        "\n",
        "**Definition:** Denoising process (Anderson, 1982):\n",
        "\n",
        "$$d\\mathbf{x}_t = \\left[-\\frac{1}{2}\\beta(t)\\mathbf{x}_t - \\beta(t)\\nabla_{\\mathbf{x}} \\log p_t(\\mathbf{x}_t)\\right]dt + \\sqrt{\\beta(t)} d\\bar{\\mathbf{W}}_t$$\n",
        "\n",
        "**Score Function:**\n",
        "$$\\nabla_{\\mathbf{x}} \\log p_t(\\mathbf{x}_t) = -\\frac{\\mathbf{x}_t - \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0}{1 - \\bar{\\alpha}_t}$$\n",
        "\n",
        "### 5.9 Discrete-Time Diffusion (DDPM)\n",
        "\n",
        "**Forward Process:**\n",
        "$$q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, \\beta_t \\mathbf{I})$$\n",
        "\n",
        "**Closed-Form Marginal:**\n",
        "$$q(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0, (1-\\bar{\\alpha}_t)\\mathbf{I})$$\n",
        "\n",
        "where $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$\n",
        "\n",
        "**Reverse Process:**\n",
        "$$p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\sigma_t^2 \\mathbf{I})$$\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Non-Stochastic Processes\n",
        "\n",
        "### 6.1 Definition\n",
        "\n",
        "**Definition:** Deterministic processes where future states are completely determined by current state without randomness.\n",
        "\n",
        "$$\\mathbf{x}_{t+1} = f(\\mathbf{x}_t)$$\n",
        "\n",
        "### 6.2 Deterministic Dynamical Systems\n",
        "\n",
        "#### 6.2.1 Continuous-Time Systems\n",
        "\n",
        "$$\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}, t)$$\n",
        "\n",
        "**Solution:** Flow map $\\phi_t: \\mathbf{x}_0 \\mapsto \\mathbf{x}_t$\n",
        "\n",
        "#### 6.2.2 Fixed Points and Stability\n",
        "\n",
        "**Fixed Point:** $\\mathbf{x}^*$ such that $\\mathbf{f}(\\mathbf{x}^*) = 0$\n",
        "\n",
        "**Linearization:**\n",
        "$$\\frac{d\\boldsymbol{\\delta}}{dt} = \\mathbf{J}|_{\\mathbf{x}^*} \\boldsymbol{\\delta}$$\n",
        "\n",
        "where $\\mathbf{J} = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}$ is Jacobian matrix.\n",
        "\n",
        "**Stability Criterion:** All eigenvalues of $\\mathbf{J}|_{\\mathbf{x}^*}$ have negative real parts.\n",
        "\n",
        "### 6.3 Neural Network Forward Pass as Deterministic Process\n",
        "\n",
        "**Layer-wise Transformation:**\n",
        "$$\\mathbf{h}^{(l+1)} = \\sigma(\\mathbf{W}^{(l)}\\mathbf{h}^{(l)} + \\mathbf{b}^{(l)})$$\n",
        "\n",
        "**Neural ODE Interpretation:**\n",
        "$$\\frac{d\\mathbf{h}(t)}{dt} = f_\\theta(\\mathbf{h}(t), t)$$\n",
        "\n",
        "### 6.4 Gradient Descent as Deterministic Dynamics\n",
        "\n",
        "**Continuous-Time Gradient Flow:**\n",
        "$$\\frac{d\\boldsymbol{\\theta}}{dt} = -\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta})$$\n",
        "\n",
        "**Discrete Update:**\n",
        "$$\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\eta \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_t)$$\n",
        "\n",
        "### 6.5 Normalizing Flows (Deterministic Bijections)\n",
        "\n",
        "**Definition:** Deterministic, invertible transformations for density estimation.\n",
        "\n",
        "$$\\mathbf{z}_K = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(\\mathbf{z}_0)$$\n",
        "\n",
        "**Change of Variables:**\n",
        "$$\\log p(\\mathbf{x}) = \\log p(\\mathbf{z}_0) - \\sum_{k=1}^{K} \\log\\left|\\det\\frac{\\partial f_k}{\\partial \\mathbf{z}_{k-1}}\\right|$$\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Principles of Generative AI\n",
        "\n",
        "### 7.1 Fundamental Objective\n",
        "\n",
        "**Definition:** Learn data distribution $p_{\\text{data}}(\\mathbf{x})$ and generate new samples $\\mathbf{x} \\sim p_\\theta(\\mathbf{x})$ that approximate the true data distribution.\n",
        "\n",
        "$$\\min_\\theta D(p_{\\text{data}} \\| p_\\theta)$$\n",
        "\n",
        "where $D$ is some divergence measure.\n",
        "\n",
        "### 7.2 Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "**Definition:** Find parameters that maximize probability of observed data.\n",
        "\n",
        "$$\\theta^* = \\arg\\max_\\theta \\prod_{i=1}^{N} p_\\theta(\\mathbf{x}_i) = \\arg\\max_\\theta \\sum_{i=1}^{N} \\log p_\\theta(\\mathbf{x}_i)$$\n",
        "\n",
        "**Equivalence to KL Minimization:**\n",
        "$$\\max_\\theta \\mathbb{E}_{p_{\\text{data}}}[\\log p_\\theta(\\mathbf{x})] \\equiv \\min_\\theta D_{KL}(p_{\\text{data}} \\| p_\\theta)$$\n",
        "\n",
        "### 7.3 Kullback-Leibler Divergence\n",
        "\n",
        "**Definition:** Asymmetric measure of distribution difference.\n",
        "\n",
        "**Forward KL:**\n",
        "$$D_{KL}(p \\| q) = \\int p(\\mathbf{x}) \\log \\frac{p(\\mathbf{x})}{q(\\mathbf{x})} d\\mathbf{x} = \\mathbb{E}_p\\left[\\log \\frac{p(\\mathbf{x})}{q(\\mathbf{x})}\\right]$$\n",
        "\n",
        "**Reverse KL:**\n",
        "$$D_{KL}(q \\| p) = \\int q(\\mathbf{x}) \\log \\frac{q(\\mathbf{x})}{p(\\mathbf{x})} d\\mathbf{x}$$\n",
        "\n",
        "**Properties:**\n",
        "- $D_{KL}(p \\| q) \\geq 0$\n",
        "- $D_{KL}(p \\| q) = 0 \\iff p = q$\n",
        "- Asymmetric: $D_{KL}(p \\| q) \\neq D_{KL}(q \\| p)$\n",
        "\n",
        "**Behavior:**\n",
        "| Divergence | Property | Application |\n",
        "|------------|----------|-------------|\n",
        "| Forward KL | Mean-seeking, mode covering | MLE training |\n",
        "| Reverse KL | Mode-seeking, zero-avoiding | Variational inference |\n",
        "\n",
        "### 7.4 Latent Variable Models\n",
        "\n",
        "#### 7.4.1 Framework\n",
        "\n",
        "**Generative Process:**\n",
        "1. Sample latent: $\\mathbf{z} \\sim p_\\theta(\\mathbf{z})$\n",
        "2. Generate data: $\\mathbf{x} \\sim p_\\theta(\\mathbf{x}|\\mathbf{z})$\n",
        "\n",
        "**Marginal Likelihood:**\n",
        "$$p_\\theta(\\mathbf{x}) = \\int p_\\theta(\\mathbf{x}|\\mathbf{z})p_\\theta(\\mathbf{z})d\\mathbf{z}$$\n",
        "\n",
        "**Intractability:** This integral is typically intractable.\n",
        "\n",
        "#### 7.4.2 Evidence Lower Bound (ELBO)\n",
        "\n",
        "**Derivation:**\n",
        "$$\\log p_\\theta(\\mathbf{x}) = \\log \\int p_\\theta(\\mathbf{x}, \\mathbf{z}) d\\mathbf{z}$$\n",
        "\n",
        "$$= \\log \\int \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} q_\\phi(\\mathbf{z}|\\mathbf{x}) d\\mathbf{z}$$\n",
        "\n",
        "$$\\geq \\int q_\\phi(\\mathbf{z}|\\mathbf{x}) \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} d\\mathbf{z} = \\mathcal{L}(\\theta, \\phi; \\mathbf{x})$$\n",
        "\n",
        "**ELBO Decomposition:**\n",
        "$$\\mathcal{L}(\\theta, \\phi; \\mathbf{x}) = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p_\\theta(\\mathbf{z}))$$\n",
        "\n",
        "| Term | Name | Interpretation |\n",
        "|------|------|----------------|\n",
        "| $\\mathbb{E}_{q_\\phi}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})]$ | Reconstruction | Data fit |\n",
        "| $D_{KL}(q_\\phi \\| p_\\theta)$ | Regularization | Prior matching |\n",
        "\n",
        "**Gap Analysis:**\n",
        "$$\\log p_\\theta(\\mathbf{x}) = \\mathcal{L}(\\theta, \\phi; \\mathbf{x}) + D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p_\\theta(\\mathbf{z}|\\mathbf{x}))$$\n",
        "\n",
        "### 7.5 Variational Autoencoders (VAEs)\n",
        "\n",
        "#### 7.5.1 Architecture\n",
        "\n",
        "```\n",
        "Encoder: q_φ(z|x) → μ_φ(x), σ_φ(x)\n",
        "         ↓\n",
        "Latent:  z = μ + σ ⊙ ε, ε ~ N(0,I)  [Reparameterization]\n",
        "         ↓\n",
        "Decoder: p_θ(x|z) → reconstruction\n",
        "```\n",
        "\n",
        "#### 7.5.2 Reparameterization Trick\n",
        "\n",
        "**Problem:** Gradient cannot flow through sampling operation.\n",
        "\n",
        "**Solution:** Transform random variable:\n",
        "$$\\mathbf{z} = \\boldsymbol{\\mu}_\\phi(\\mathbf{x}) + \\boldsymbol{\\sigma}_\\phi(\\mathbf{x}) \\odot \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$$\n",
        "\n",
        "**Gradient Computation:**\n",
        "$$\\nabla_\\phi \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})}[f(\\mathbf{z})] = \\mathbb{E}_{\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})}[\\nabla_\\phi f(\\boldsymbol{\\mu}_\\phi + \\boldsymbol{\\sigma}_\\phi \\odot \\boldsymbol{\\epsilon})]$$\n",
        "\n",
        "#### 7.5.3 VAE Loss Function\n",
        "\n",
        "$$\\mathcal{L}_{VAE} = -\\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] + D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z}))$$\n",
        "\n",
        "**Analytical KL for Gaussians:**\n",
        "$$D_{KL}(\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2) \\| \\mathcal{N}(\\mathbf{0}, \\mathbf{I})) = \\frac{1}{2}\\sum_{j=1}^{d}(\\mu_j^2 + \\sigma_j^2 - 1 - \\log \\sigma_j^2)$$\n",
        "\n",
        "### 7.6 Generative Adversarial Networks (GANs)\n",
        "\n",
        "#### 7.6.1 Framework\n",
        "\n",
        "**Generator:** $G_\\theta: \\mathbf{z} \\rightarrow \\mathbf{x}$\n",
        "**Discriminator:** $D_\\phi: \\mathbf{x} \\rightarrow [0, 1]$\n",
        "\n",
        "#### 7.6.2 Minimax Objective\n",
        "\n",
        "$$\\min_\\theta \\max_\\phi V(G_\\theta, D_\\phi) = \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}}[\\log D_\\phi(\\mathbf{x})] + \\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})}[\\log(1 - D_\\phi(G_\\theta(\\mathbf{z})))]$$\n",
        "\n",
        "#### 7.6.3 Optimal Discriminator\n",
        "\n",
        "$$D^*(\\mathbf{x}) = \\frac{p_{\\text{data}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_g(\\mathbf{x})}$$\n",
        "\n",
        "#### 7.6.4 Generator Objective at Optimal Discriminator\n",
        "\n",
        "$$\\min_\\theta V(G_\\theta, D^*) = -\\log 4 + 2 \\cdot D_{JS}(p_{\\text{data}} \\| p_g)$$\n",
        "\n",
        "where Jensen-Shannon divergence:\n",
        "$$D_{JS}(p \\| q) = \\frac{1}{2}D_{KL}(p \\| m) + \\frac{1}{2}D_{KL}(q \\| m), \\quad m = \\frac{p+q}{2}$$\n",
        "\n",
        "#### 7.6.5 Alternative GAN Losses\n",
        "\n",
        "**Wasserstein GAN (WGAN):**\n",
        "$$W(p_{\\text{data}}, p_g) = \\sup_{\\|f\\|_L \\leq 1} \\mathbb{E}_{p_{\\text{data}}}[f(\\mathbf{x})] - \\mathbb{E}_{p_g}[f(\\mathbf{x})]$$\n",
        "\n",
        "**WGAN-GP Loss:**\n",
        "$$\\mathcal{L} = \\mathbb{E}_{p_g}[D(\\mathbf{x})] - \\mathbb{E}_{p_{\\text{data}}}[D(\\mathbf{x})] + \\lambda \\mathbb{E}_{\\hat{\\mathbf{x}}}[(\\|\\nabla_{\\hat{\\mathbf{x}}} D(\\hat{\\mathbf{x}})\\|_2 - 1)^2]$$\n",
        "\n",
        "### 7.7 Diffusion Models\n",
        "\n",
        "#### 7.7.1 Score Matching\n",
        "\n",
        "**Definition:** Learn score function (gradient of log-density).\n",
        "\n",
        "$$\\mathbf{s}_\\theta(\\mathbf{x}) \\approx \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$$\n",
        "\n",
        "**Denoising Score Matching Objective:**\n",
        "$$\\mathcal{L}_{DSM} = \\mathbb{E}_{t, \\mathbf{x}_0, \\boldsymbol{\\epsilon}}\\left[\\|\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) - \\boldsymbol{\\epsilon}\\|^2\\right]$$\n",
        "\n",
        "where $\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1-\\bar{\\alpha}_t}\\boldsymbol{\\epsilon}$\n",
        "\n",
        "#### 7.7.2 Training Objective\n",
        "\n",
        "**Simplified Loss (DDPM):**\n",
        "$$\\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{t \\sim U(1,T), \\mathbf{x}_0, \\boldsymbol{\\epsilon}}\\left[\\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1-\\bar{\\alpha}_t}\\boldsymbol{\\epsilon}, t)\\|^2\\right]$$\n",
        "\n",
        "#### 7.7.3 Sampling Algorithms\n",
        "\n",
        "**DDPM Sampling:**\n",
        "$$\\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(\\mathbf{x}_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\right) + \\sigma_t \\mathbf{z}$$\n",
        "\n",
        "**DDIM Sampling (Deterministic):**\n",
        "$$\\mathbf{x}_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\left(\\frac{\\mathbf{x}_t - \\sqrt{1-\\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_\\theta}{\\sqrt{\\bar{\\alpha}_t}}\\right) + \\sqrt{1-\\bar{\\alpha}_{t-1}}\\boldsymbol{\\epsilon}_\\theta$$\n",
        "\n",
        "### 7.8 Autoregressive Models\n",
        "\n",
        "#### 7.8.1 Factorization\n",
        "\n",
        "$$p(\\mathbf{x}) = \\prod_{i=1}^{n} p(x_i | x_1, x_2, \\ldots, x_{i-1})$$\n",
        "\n",
        "#### 7.8.2 Transformer-based Language Models\n",
        "\n",
        "**Attention Mechanism:**\n",
        "$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n",
        "\n",
        "**Causal Masking:**\n",
        "$$M_{ij} = \\begin{cases} 0 & \\text{if } i \\geq j \\\\ -\\infty & \\text{if } i < j \\end{cases}$$\n",
        "\n",
        "$$\\text{CausalAttention} = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} + \\mathbf{M}\\right)\\mathbf{V}$$\n",
        "\n",
        "**Next-Token Prediction Loss:**\n",
        "$$\\mathcal{L} = -\\sum_{t=1}^{T} \\log p_\\theta(x_t | x_{<t})$$\n",
        "\n",
        "### 7.9 Normalizing Flows\n",
        "\n",
        "#### 7.9.1 Principle\n",
        "\n",
        "**Change of Variables Formula:**\n",
        "$$p_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\left|\\det \\frac{\\partial f^{-1}}{\\partial \\mathbf{x}}\\right|$$\n",
        "\n",
        "**Log-likelihood:**\n",
        "$$\\log p_X(\\mathbf{x}) = \\log p_Z(\\mathbf{z}) + \\log \\left|\\det \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\\right|$$\n",
        "\n",
        "#### 7.9.2 Flow Architectures\n",
        "\n",
        "**Coupling Layers (RealNVP):**\n",
        "$$\\mathbf{y}_{1:d} = \\mathbf{x}_{1:d}$$\n",
        "$$\\mathbf{y}_{d+1:D} = \\mathbf{x}_{d+1:D} \\odot \\exp(s(\\mathbf{x}_{1:d})) + t(\\mathbf{x}_{1:d})$$\n",
        "\n",
        "**Jacobian (Triangular):**\n",
        "$$\\det \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\exp\\left(\\sum_j s_j(\\mathbf{x}_{1:d})\\right)$$\n",
        "\n",
        "### 7.10 Information-Theoretic Measures\n",
        "\n",
        "#### 7.10.1 Entropy\n",
        "\n",
        "**Discrete:**\n",
        "$$H(X) = -\\sum_{x} p(x) \\log p(x)$$\n",
        "\n",
        "**Continuous (Differential Entropy):**\n",
        "$$h(X) = -\\int p(x) \\log p(x) dx$$\n",
        "\n",
        "#### 7.10.2 Mutual Information\n",
        "\n",
        "$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$\n",
        "\n",
        "$$I(X; Y) = D_{KL}(p(X, Y) \\| p(X)p(Y))$$\n",
        "\n",
        "**Application in VAEs (InfoVAE):**\n",
        "$$\\mathcal{L}_{InfoVAE} = \\mathcal{L}_{ELBO} - \\lambda I_q(X; Z)$$\n",
        "\n",
        "#### 7.10.3 Cross-Entropy\n",
        "\n",
        "$$H(p, q) = -\\sum_{x} p(x) \\log q(x) = H(p) + D_{KL}(p \\| q)$$\n",
        "\n",
        "**As Training Loss:**\n",
        "$$\\mathcal{L}_{CE} = -\\sum_{i} y_i \\log \\hat{y}_i$$\n",
        "\n",
        "### 7.11 Unified Framework: Score-Based Generative Models\n",
        "\n",
        "**Score Function Definition:**\n",
        "$$\\mathbf{s}(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$$\n",
        "\n",
        "**Langevin Dynamics Sampling:**\n",
        "$$\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\frac{\\epsilon}{2}\\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}_t) + \\sqrt{\\epsilon}\\mathbf{z}_t, \\quad \\mathbf{z}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$$\n",
        "\n",
        "**Connection to Diffusion:**\n",
        "$$\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) = -\\sqrt{1-\\bar{\\alpha}_t} \\cdot \\mathbf{s}_\\theta(\\mathbf{x}_t, t)$$\n",
        "\n",
        "---\n",
        "\n",
        "## Summary: Probabilistic Foundations → Generative Models\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────────┐\n",
        "│                    PROBABILITY FOUNDATIONS                          │\n",
        "├─────────────────────────────────────────────────────────────────────┤\n",
        "│ Random Variables → Distributions → Joint/Conditional → Bayes       │\n",
        "└────────────────────────────────────┬────────────────────────────────┘\n",
        "                                     │\n",
        "                                     ▼\n",
        "┌─────────────────────────────────────────────────────────────────────┐\n",
        "│                    STOCHASTIC PROCESSES                              │\n",
        "├─────────────────────────────────────────────────────────────────────┤\n",
        "│ Markov Chains → Gaussian Processes → SDEs → Diffusion Processes     │\n",
        "└────────────────────────────────────┬────────────────────────────────┘\n",
        "                                     │\n",
        "                                     ▼\n",
        "┌─────────────────────────────────────────────────────────────────────┐\n",
        "│                    GENERATIVE AI PRINCIPLES                          │\n",
        "├─────────────────────────────────────────────────────────────────────┤\n",
        "│ MLE → ELBO → KL Divergence → Score Matching → Reparameterization    │\n",
        "└────────────────────────────────────┬────────────────────────────────┘\n",
        "                                     │\n",
        "                                     ▼\n",
        "┌─────────────────────────────────────────────────────────────────────┐\n",
        "│                    GENERATIVE MODEL ARCHITECTURES                    │\n",
        "├─────────────────────────────────────────────────────────────────────┤\n",
        "│ VAE: Latent + ELBO                                                   │\n",
        "│ GAN: Adversarial + JS/Wasserstein                                   │\n",
        "│ Diffusion: Score + SDE                                              │\n",
        "│ Flow: Bijection + Change of Variables                               │\n",
        "│ Autoregressive: Chain Rule + Attention                              │\n",
        "└─────────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Key Mathematical Relationships\n",
        "\n",
        "| Concept | Equation | Gen-AI Application |\n",
        "|---------|----------|-------------------|\n",
        "| Bayes' Theorem | $p(\\mathbf{z}|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})}{p(\\mathbf{x})}$ | Posterior in VAEs |\n",
        "| Chain Rule | $p(\\mathbf{x}) = \\prod_i p(x_i|x_{<i})$ | Autoregressive LLMs |\n",
        "| ELBO | $\\log p(\\mathbf{x}) \\geq \\mathbb{E}_q[\\log p(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q\\|p)$ | VAE Training |\n",
        "| Score | $\\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$ | Diffusion Models |\n",
        "| Change of Variables | $p_x = p_z \\cdot |\\det J^{-1}|$ | Normalizing Flows |\n",
        "| Langevin Dynamics | $\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\frac{\\epsilon}{2}\\mathbf{s} + \\sqrt{\\epsilon}\\mathbf{z}$ | MCMC Sampling |"
      ],
      "metadata": {
        "id": "PopBB4wrg-p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probability and Statistics for Generative AI: Key Probability Distributions\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Foundational Probability Theory for Generative AI\n",
        "\n",
        "### 1.1 Definition of Generative Modeling from Probabilistic Perspective\n",
        "\n",
        "Generative AI fundamentally aims to learn the underlying probability distribution $p_{data}(x)$ of a dataset and generate new samples from an approximated distribution $p_{model}(x)$. The core objective is:\n",
        "\n",
        "$$p_{model}(x; \\theta) \\approx p_{data}(x)$$\n",
        "\n",
        "where $\\theta$ represents learnable parameters of the generative model.\n",
        "\n",
        "### 1.2 Probability Axioms (Kolmogorov Axioms)\n",
        "\n",
        "For a sample space $\\Omega$ and event $A$:\n",
        "\n",
        "1. **Non-negativity**: $P(A) \\geq 0$\n",
        "2. **Normalization**: $P(\\Omega) = 1$\n",
        "3. **Additivity**: For mutually exclusive events $A_1, A_2, ...$:\n",
        "\n",
        "$$P\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} P(A_i)$$\n",
        "\n",
        "### 1.3 Conditional Probability\n",
        "\n",
        "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) > 0$$\n",
        "\n",
        "**Relevance to Gen-AI**: Autoregressive models (GPT, LLaMA) generate sequences by modeling:\n",
        "\n",
        "$$P(x_1, x_2, ..., x_n) = \\prod_{t=1}^{n} P(x_t | x_1, x_2, ..., x_{t-1})$$\n",
        "\n",
        "### 1.4 Bayes' Theorem\n",
        "\n",
        "$$P(\\theta|X) = \\frac{P(X|\\theta) P(\\theta)}{P(X)} = \\frac{P(X|\\theta) P(\\theta)}{\\int P(X|\\theta) P(\\theta) d\\theta}$$\n",
        "\n",
        "| Component | Term | Role in Gen-AI |\n",
        "|-----------|------|----------------|\n",
        "| $P(\\theta\\|X)$ | Posterior | Updated belief about model parameters |\n",
        "| $P(X\\|\\theta)$ | Likelihood | How well model explains data |\n",
        "| $P(\\theta)$ | Prior | Initial belief about parameters |\n",
        "| $P(X)$ | Evidence/Marginal Likelihood | Normalization constant |\n",
        "\n",
        "### 1.5 Joint and Marginal Distributions\n",
        "\n",
        "**Joint Distribution**:\n",
        "$$P(X, Z) = P(X|Z)P(Z)$$\n",
        "\n",
        "**Marginalization**:\n",
        "$$P(X) = \\int P(X, Z) dZ = \\int P(X|Z)P(Z) dZ$$\n",
        "\n",
        "**Critical for Gen-AI**: Latent variable models (VAEs) rely on marginalization over latent space $Z$.\n",
        "\n",
        "### 1.6 Independence and Conditional Independence\n",
        "\n",
        "**Independence**:\n",
        "$$P(X, Y) = P(X)P(Y)$$\n",
        "\n",
        "**Conditional Independence** ($X \\perp Y | Z$):\n",
        "$$P(X, Y | Z) = P(X|Z)P(Y|Z)$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Statistical Estimators in Generative AI\n",
        "\n",
        "### 2.1 Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "**Definition**: Find parameters $\\theta$ that maximize the probability of observed data.\n",
        "\n",
        "$$\\theta_{MLE} = \\arg\\max_{\\theta} \\prod_{i=1}^{N} p(x_i | \\theta) = \\arg\\max_{\\theta} \\sum_{i=1}^{N} \\log p(x_i | \\theta)$$\n",
        "\n",
        "**Log-Likelihood Objective**:\n",
        "$$\\mathcal{L}(\\theta) = \\mathbb{E}_{x \\sim p_{data}}[\\log p_{model}(x; \\theta)]$$\n",
        "\n",
        "### 2.2 Maximum A Posteriori (MAP) Estimation\n",
        "\n",
        "$$\\theta_{MAP} = \\arg\\max_{\\theta} P(\\theta|X) = \\arg\\max_{\\theta} [\\log P(X|\\theta) + \\log P(\\theta)]$$\n",
        "\n",
        "The term $\\log P(\\theta)$ acts as regularization.\n",
        "\n",
        "### 2.3 Expectation and Variance\n",
        "\n",
        "**Expectation**:\n",
        "$$\\mathbb{E}[X] = \\int x \\cdot p(x) dx \\quad \\text{(continuous)}$$\n",
        "$$\\mathbb{E}[X] = \\sum_{x} x \\cdot P(x) \\quad \\text{(discrete)}$$\n",
        "\n",
        "**Variance**:\n",
        "$$\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$$\n",
        "\n",
        "**Covariance**:\n",
        "$$\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Information-Theoretic Foundations\n",
        "\n",
        "### 3.1 Entropy\n",
        "\n",
        "**Definition**: Measures uncertainty/randomness in a distribution.\n",
        "\n",
        "**Discrete Entropy**:\n",
        "$$H(X) = -\\sum_{x} P(x) \\log P(x)$$\n",
        "\n",
        "**Differential Entropy (Continuous)**:\n",
        "$$h(X) = -\\int p(x) \\log p(x) dx$$\n",
        "\n",
        "**Properties**:\n",
        "- $H(X) \\geq 0$ for discrete distributions\n",
        "- Maximum entropy for uniform distribution\n",
        "- Gaussian has maximum entropy among distributions with fixed variance\n",
        "\n",
        "### 3.2 Kullback-Leibler (KL) Divergence\n",
        "\n",
        "**Definition**: Measures dissimilarity between two distributions.\n",
        "\n",
        "$$D_{KL}(p \\| q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx = \\mathbb{E}_{x \\sim p}\\left[\\log \\frac{p(x)}{q(x)}\\right]$$\n",
        "\n",
        "**Properties**:\n",
        "- $D_{KL}(p \\| q) \\geq 0$ (Gibbs' inequality)\n",
        "- $D_{KL}(p \\| q) = 0$ iff $p = q$\n",
        "- **Asymmetric**: $D_{KL}(p \\| q) \\neq D_{KL}(q \\| p)$\n",
        "\n",
        "**Decomposition**:\n",
        "$$D_{KL}(p \\| q) = H(p, q) - H(p)$$\n",
        "\n",
        "where $H(p, q)$ is cross-entropy.\n",
        "\n",
        "### 3.3 Cross-Entropy\n",
        "\n",
        "$$H(p, q) = -\\mathbb{E}_{x \\sim p}[\\log q(x)] = -\\sum_{x} p(x) \\log q(x)$$\n",
        "\n",
        "**Connection to MLE**:\n",
        "$$\\min_{\\theta} H(p_{data}, p_{model}) = \\min_{\\theta} -\\mathbb{E}_{x \\sim p_{data}}[\\log p_{model}(x; \\theta)]$$\n",
        "\n",
        "### 3.4 Jensen-Shannon Divergence\n",
        "\n",
        "$$D_{JS}(p \\| q) = \\frac{1}{2} D_{KL}(p \\| m) + \\frac{1}{2} D_{KL}(q \\| m)$$\n",
        "\n",
        "where $m = \\frac{1}{2}(p + q)$\n",
        "\n",
        "**Properties**:\n",
        "- Symmetric: $D_{JS}(p \\| q) = D_{JS}(q \\| p)$\n",
        "- Bounded: $0 \\leq D_{JS} \\leq \\log 2$\n",
        "- Used in original GAN formulation\n",
        "\n",
        "### 3.5 Mutual Information\n",
        "\n",
        "$$I(X; Z) = D_{KL}(p(X, Z) \\| p(X)p(Z)) = H(X) - H(X|Z)$$\n",
        "\n",
        "**Application in Gen-AI**: InfoGAN maximizes mutual information between latent codes and generated samples.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Key Probability Distributions in Generative AI\n",
        "\n",
        "---\n",
        "\n",
        "### 4.1 Bernoulli Distribution\n",
        "\n",
        "**Definition**: Models binary random variable $X \\in \\{0, 1\\}$.\n",
        "\n",
        "**Probability Mass Function (PMF)**:\n",
        "$$P(X = x) = p^x (1-p)^{1-x}, \\quad x \\in \\{0, 1\\}$$\n",
        "\n",
        "**Parameters**: $p \\in [0, 1]$ (success probability)\n",
        "\n",
        "**Moments**:\n",
        "$$\\mathbb{E}[X] = p, \\quad \\text{Var}(X) = p(1-p)$$\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- Binary image generation (binary cross-entropy loss)\n",
        "- Dropout regularization during training\n",
        "- Bernoulli VAE decoders for binary data\n",
        "\n",
        "**Bernoulli VAE Reconstruction Loss**:\n",
        "$$\\mathcal{L}_{recon} = -\\sum_{i=1}^{D} [x_i \\log \\hat{x}_i + (1-x_i) \\log(1-\\hat{x}_i)]$$\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2 Categorical Distribution\n",
        "\n",
        "**Definition**: Generalization of Bernoulli to $K$ discrete outcomes.\n",
        "\n",
        "**PMF**:\n",
        "$$P(X = k) = \\pi_k, \\quad k \\in \\{1, 2, ..., K\\}$$\n",
        "\n",
        "where $\\sum_{k=1}^{K} \\pi_k = 1$ and $\\pi_k \\geq 0$\n",
        "\n",
        "**One-hot Encoding Formulation**:\n",
        "$$P(x | \\pi) = \\prod_{k=1}^{K} \\pi_k^{x_k}$$\n",
        "\n",
        "where $x = [x_1, ..., x_K]$ is one-hot vector.\n",
        "\n",
        "**Moments**:\n",
        "$$\\mathbb{E}[X_k] = \\pi_k, \\quad \\text{Var}(X_k) = \\pi_k(1-\\pi_k)$$\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- **Language Models**: Token prediction at each timestep\n",
        "- **VQ-VAE**: Discrete codebook selection\n",
        "- **Discrete Diffusion Models**: Categorical state transitions\n",
        "\n",
        "**Softmax Parameterization**:\n",
        "$$\\pi_k = \\frac{\\exp(z_k)}{\\sum_{j=1}^{K} \\exp(z_j)}$$\n",
        "\n",
        "where $z_k$ are logits from neural network.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.3 Multinomial Distribution\n",
        "\n",
        "**Definition**: Distribution over counts of $K$ categories in $n$ trials.\n",
        "\n",
        "**PMF**:\n",
        "$$P(x_1, ..., x_K) = \\frac{n!}{x_1! \\cdots x_K!} \\prod_{k=1}^{K} \\pi_k^{x_k}$$\n",
        "\n",
        "where $\\sum_{k=1}^{K} x_k = n$\n",
        "\n",
        "**Moments**:\n",
        "$$\\mathbb{E}[X_k] = n\\pi_k, \\quad \\text{Var}(X_k) = n\\pi_k(1-\\pi_k)$$\n",
        "$$\\text{Cov}(X_i, X_j) = -n\\pi_i\\pi_j \\quad (i \\neq j)$$\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- Bag-of-words text generation\n",
        "- Topic modeling (LDA)\n",
        "- Multi-label classification in conditional generation\n",
        "\n",
        "---\n",
        "\n",
        "### 4.4 Poisson Distribution\n",
        "\n",
        "**Definition**: Models count of events in fixed interval.\n",
        "\n",
        "**PMF**:\n",
        "$$P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad k \\in \\{0, 1, 2, ...\\}$$\n",
        "\n",
        "**Parameters**: $\\lambda > 0$ (rate parameter)\n",
        "\n",
        "**Moments**:\n",
        "$$\\mathbb{E}[X] = \\lambda, \\quad \\text{Var}(X) = \\lambda$$\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- Modeling sequence lengths in text generation\n",
        "- Point process generation\n",
        "- Sparse representation learning\n",
        "\n",
        "---\n",
        "\n",
        "### 4.5 Gaussian (Normal) Distribution\n",
        "\n",
        "**Definition**: Most fundamental continuous distribution in Gen-AI.\n",
        "\n",
        "#### 4.5.1 Univariate Gaussian\n",
        "\n",
        "**Probability Density Function (PDF)**:\n",
        "$$p(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
        "\n",
        "**Notation**: $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$\n",
        "\n",
        "**Parameters**:\n",
        "- $\\mu \\in \\mathbb{R}$: Mean\n",
        "- $\\sigma^2 > 0$: Variance\n",
        "\n",
        "**Moments**:\n",
        "$$\\mathbb{E}[X] = \\mu, \\quad \\text{Var}(X) = \\sigma^2$$\n",
        "\n",
        "**Standard Normal**: $Z \\sim \\mathcal{N}(0, 1)$\n",
        "\n",
        "**Standardization**:\n",
        "$$Z = \\frac{X - \\mu}{\\sigma}$$\n",
        "\n",
        "#### 4.5.2 Multivariate Gaussian\n",
        "\n",
        "**PDF**:\n",
        "$$p(x | \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{D/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right)$$\n",
        "\n",
        "**Notation**: $X \\sim \\mathcal{N}(\\mu, \\Sigma)$\n",
        "\n",
        "**Parameters**:\n",
        "- $\\mu \\in \\mathbb{R}^D$: Mean vector\n",
        "- $\\Sigma \\in \\mathbb{R}^{D \\times D}$: Covariance matrix (symmetric positive definite)\n",
        "\n",
        "**Mahalanobis Distance**:\n",
        "$$d_M(x, \\mu) = \\sqrt{(x-\\mu)^T \\Sigma^{-1} (x-\\mu)}$$\n",
        "\n",
        "**Properties**:\n",
        "1. **Marginals are Gaussian**: If $(X_1, X_2)^T \\sim \\mathcal{N}$, then $X_1 \\sim \\mathcal{N}$\n",
        "2. **Conditionals are Gaussian**: $p(X_1 | X_2) = \\mathcal{N}(\\mu_{1|2}, \\Sigma_{1|2})$\n",
        "3. **Linear transformations preserve Gaussianity**: $AX + b \\sim \\mathcal{N}(A\\mu + b, A\\Sigma A^T)$\n",
        "\n",
        "**Conditional Distribution Formulas**:\n",
        "\n",
        "For $\\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}\\right)$:\n",
        "\n",
        "$$\\mu_{1|2} = \\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(x_2 - \\mu_2)$$\n",
        "$$\\Sigma_{1|2} = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}$$\n",
        "\n",
        "#### 4.5.3 Applications in Gen-AI\n",
        "\n",
        "| Application | Role of Gaussian |\n",
        "|-------------|------------------|\n",
        "| **VAE Prior** | $p(z) = \\mathcal{N}(0, I)$ |\n",
        "| **VAE Encoder** | $q_\\phi(z\\|x) = \\mathcal{N}(\\mu_\\phi(x), \\sigma_\\phi^2(x))$ |\n",
        "| **Diffusion Forward Process** | $q(x_t\\|x_{t-1}) = \\mathcal{N}(\\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)$ |\n",
        "| **Reparameterization** | $z = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$ |\n",
        "\n",
        "#### 4.5.4 KL Divergence Between Two Gaussians\n",
        "\n",
        "For $p = \\mathcal{N}(\\mu_1, \\Sigma_1)$ and $q = \\mathcal{N}(\\mu_2, \\Sigma_2)$:\n",
        "\n",
        "$$D_{KL}(p \\| q) = \\frac{1}{2}\\left[\\log\\frac{|\\Sigma_2|}{|\\Sigma_1|} - D + \\text{tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\mu_2-\\mu_1)^T\\Sigma_2^{-1}(\\mu_2-\\mu_1)\\right]$$\n",
        "\n",
        "**Special Case (VAE)**: For $p = \\mathcal{N}(\\mu, \\sigma^2 I)$ and $q = \\mathcal{N}(0, I)$:\n",
        "\n",
        "$$D_{KL}(p \\| q) = \\frac{1}{2}\\sum_{j=1}^{D}\\left[\\sigma_j^2 + \\mu_j^2 - 1 - \\log\\sigma_j^2\\right]$$\n",
        "\n",
        "---\n",
        "\n",
        "### 4.6 Uniform Distribution\n",
        "\n",
        "#### 4.6.1 Continuous Uniform\n",
        "\n",
        "**PDF**:\n",
        "$$p(x | a, b) = \\frac{1}{b-a}, \\quad x \\in [a, b]$$\n",
        "\n",
        "**Moments**:\n",
        "$$\\mathbb{E}[X] = \\frac{a+b}{2}, \\quad \\text{Var}(X) = \\frac{(b-a)^2}{12}$$\n",
        "\n",
        "#### 4.6.2 Discrete Uniform\n",
        "\n",
        "**PMF**:\n",
        "$$P(X = k) = \\frac{1}{n}, \\quad k \\in \\{1, 2, ..., n\\}$$\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- **GAN Latent Space**: $z \\sim \\text{Uniform}(-1, 1)^D$\n",
        "- **Random Sampling**: Index selection for mini-batches\n",
        "- **Data Augmentation**: Random crop positions\n",
        "- **Noise Injection**: Uniform noise in certain architectures\n",
        "\n",
        "---\n",
        "\n",
        "### 4.7 Exponential Distribution\n",
        "\n",
        "**PDF**:\n",
        "$$p(x | \\lambda) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0$$\n",
        "\n",
        "**Parameters**: $\\lambda > 0$ (rate parameter)\n",
        "\n",
        "**Moments**:\n",
        "$$\\mathbb{E}[X] = \\frac{1}{\\lambda}, \\quad \\text{Var}(X) = \\frac{1}{\\lambda^2}$$\n",
        "\n",
        "**Memoryless Property**:\n",
        "$$P(X > s + t | X > s) = P(X > t)$$\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- Modeling inter-arrival times in temporal generation\n",
        "- Learning rate scheduling\n",
        "- Exponential family connections\n",
        "\n",
        "---\n",
        "\n",
        "### 4.8 Beta Distribution\n",
        "\n",
        "**Definition**: Distribution over probabilities $x \\in [0, 1]$.\n",
        "\n",
        "**PDF**:\n",
        "$$p(x | \\alpha, \\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)}$$\n",
        "\n",
        "where $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ is the Beta function.\n",
        "\n",
        "**Parameters**: $\\alpha > 0$, $\\beta > 0$ (shape parameters)\n",
        "\n",
        "**Moments**:\n",
        "$$\\mathbb{E}[X] = \\frac{\\alpha}{\\alpha+\\beta}, \\quad \\text{Var}(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$\n",
        "\n",
        "**Special Cases**:\n",
        "- $\\alpha = \\beta = 1$: Uniform distribution\n",
        "- $\\alpha = \\beta$: Symmetric around 0.5\n",
        "- $\\alpha > \\beta$: Skewed right\n",
        "- $\\alpha < \\beta$: Skewed left\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- **Prior for Bernoulli parameters** (conjugate prior)\n",
        "- **Mixup data augmentation**: $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$\n",
        "- **Dropout probability tuning**\n",
        "- **Interpolation weights in latent space**\n",
        "\n",
        "**Beta Distribution in Mixup**:\n",
        "$$\\tilde{x} = \\lambda x_i + (1-\\lambda) x_j, \\quad \\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$$\n",
        "\n",
        "---\n",
        "\n",
        "### 4.9 Gamma Distribution\n",
        "\n",
        "**PDF**:\n",
        "$$p(x | \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}, \\quad x > 0$$\n",
        "\n",
        "**Parameters**:\n",
        "- $\\alpha > 0$: Shape parameter\n",
        "- $\\beta > 0$: Rate parameter\n",
        "\n",
        "**Alternative Parameterization** (shape-scale):\n",
        "$$p(x | k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k} x^{k-1} e^{-x/\\theta}$$\n",
        "\n",
        "**Moments**:\n",
        "$$\\mathbb{E}[X] = \\frac{\\alpha}{\\beta}, \\quad \\text{Var}(X) = \\frac{\\alpha}{\\beta^2}$$\n",
        "\n",
        "**Relationships**:\n",
        "- Exponential is Gamma with $\\alpha = 1$\n",
        "- Chi-squared is Gamma with $\\alpha = k/2$, $\\beta = 1/2$\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- Prior for precision parameters\n",
        "- Modeling positive-valued latent variables\n",
        "- Bayesian neural network priors\n",
        "\n",
        "---\n",
        "\n",
        "### 4.10 Dirichlet Distribution\n",
        "\n",
        "**Definition**: Multivariate generalization of Beta; distribution over probability simplices.\n",
        "\n",
        "**PDF**:\n",
        "$$p(x | \\alpha) = \\frac{\\Gamma(\\sum_{k=1}^{K} \\alpha_k)}{\\prod_{k=1}^{K} \\Gamma(\\alpha_k)} \\prod_{k=1}^{K} x_k^{\\alpha_k - 1}$$\n",
        "\n",
        "**Constraint**: $\\sum_{k=1}^{K} x_k = 1$, $x_k \\geq 0$\n",
        "\n",
        "**Parameters**: $\\alpha = (\\alpha_1, ..., \\alpha_K)$, $\\alpha_k > 0$\n",
        "\n",
        "**Moments**:\n",
        "$$\\mathbb{E}[X_k] = \\frac{\\alpha_k}{\\alpha_0}, \\quad \\alpha_0 = \\sum_{j=1}^{K} \\alpha_j$$\n",
        "$$\\text{Var}(X_k) = \\frac{\\alpha_k(\\alpha_0 - \\alpha_k)}{\\alpha_0^2(\\alpha_0 + 1)}$$\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- **Topic Modeling (LDA)**: Prior over topic distributions\n",
        "- **Mixture Models**: Prior over mixture weights\n",
        "- **Attention Mechanism Analysis**: Modeling attention weight distributions\n",
        "\n",
        "---\n",
        "\n",
        "### 4.11 Laplace Distribution\n",
        "\n",
        "**PDF**:\n",
        "$$p(x | \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|x - \\mu|}{b}\\right)$$\n",
        "\n",
        "**Parameters**:\n",
        "- $\\mu$: Location (mean)\n",
        "- $b > 0$: Scale\n",
        "\n",
        "**Moments**:\n",
        "$$\\mathbb{E}[X] = \\mu, \\quad \\text{Var}(X) = 2b^2$$\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- **Sparse Latent Representations**: Encourages sparsity (L1 regularization connection)\n",
        "- **Robust Generation**: Heavier tails than Gaussian\n",
        "- **Flow-based Models**: Sometimes used as base distribution\n",
        "\n",
        "**Connection to L1 Regularization**:\n",
        "$$p(w) = \\frac{\\lambda}{2} \\exp(-\\lambda|w|) \\Rightarrow \\text{L1 penalty: } \\lambda|w|$$\n",
        "\n",
        "---\n",
        "\n",
        "### 4.12 Student's t-Distribution\n",
        "\n",
        "**PDF**:\n",
        "$$p(x | \\nu, \\mu, \\sigma) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\Gamma(\\frac{\\nu}{2})\\sqrt{\\nu\\pi}\\sigma} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)^{-\\frac{\\nu+1}{2}}$$\n",
        "\n",
        "**Parameters**:\n",
        "- $\\nu > 0$: Degrees of freedom\n",
        "- $\\mu$: Location\n",
        "- $\\sigma$: Scale\n",
        "\n",
        "**Moments** (for $\\nu > 2$):\n",
        "$$\\mathbb{E}[X] = \\mu, \\quad \\text{Var}(X) = \\frac{\\nu}{\\nu-2}\\sigma^2$$\n",
        "\n",
        "**Properties**:\n",
        "- Heavier tails than Gaussian\n",
        "- As $\\nu \\to \\infty$, converges to Gaussian\n",
        "- Robust to outliers\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- **Robust VAEs**: Replace Gaussian with t-distribution for robustness\n",
        "- **Outlier-resistant generation**\n",
        "- **Uncertainty quantification**\n",
        "\n",
        "---\n",
        "\n",
        "### 4.13 Log-Normal Distribution\n",
        "\n",
        "**Definition**: If $Y = \\log(X) \\sim \\mathcal{N}(\\mu, \\sigma^2)$, then $X \\sim \\text{LogNormal}(\\mu, \\sigma^2)$.\n",
        "\n",
        "**PDF**:\n",
        "$$p(x | \\mu, \\sigma) = \\frac{1}{x\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}\\right), \\quad x > 0$$\n",
        "\n",
        "**Moments**:\n",
        "$$\\mathbb{E}[X] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$$\n",
        "$$\\text{Var}(X) = \\left[\\exp(\\sigma^2) - 1\\right] \\exp(2\\mu + \\sigma^2)$$\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- Modeling positive-valued data (image intensities, word frequencies)\n",
        "- Variance modeling in heteroscedastic generation\n",
        "- Scale parameters in hierarchical models\n",
        "\n",
        "---\n",
        "\n",
        "### 4.14 Von Mises Distribution (Circular/Angular)\n",
        "\n",
        "**PDF**:\n",
        "$$p(\\theta | \\mu, \\kappa) = \\frac{\\exp(\\kappa \\cos(\\theta - \\mu))}{2\\pi I_0(\\kappa)}$$\n",
        "\n",
        "where $I_0(\\kappa)$ is the modified Bessel function of order 0.\n",
        "\n",
        "**Parameters**:\n",
        "- $\\mu \\in [-\\pi, \\pi]$: Mean direction\n",
        "- $\\kappa \\geq 0$: Concentration parameter\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- **Pose generation**: Modeling joint angles\n",
        "- **Directional data**: Wind direction, molecular conformations\n",
        "- **3D human motion generation**\n",
        "\n",
        "---\n",
        "\n",
        "### 4.15 Wishart Distribution\n",
        "\n",
        "**Definition**: Distribution over symmetric positive-definite matrices.\n",
        "\n",
        "**PDF**:\n",
        "$$p(W | V, n) = \\frac{|W|^{(n-D-1)/2} \\exp\\left(-\\frac{1}{2}\\text{tr}(V^{-1}W)\\right)}{2^{nD/2}|V|^{n/2}\\Gamma_D(n/2)}$$\n",
        "\n",
        "where $\\Gamma_D$ is the multivariate gamma function.\n",
        "\n",
        "**Parameters**:\n",
        "- $V$: Scale matrix ($D \\times D$ positive-definite)\n",
        "- $n \\geq D$: Degrees of freedom\n",
        "\n",
        "**Mean**:\n",
        "$$\\mathbb{E}[W] = nV$$\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- Prior for covariance matrices in Gaussian models\n",
        "- Bayesian deep learning for weight uncertainty\n",
        "- Structured covariance learning\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Mixture Distributions in Generative AI\n",
        "\n",
        "### 5.1 Gaussian Mixture Model (GMM)\n",
        "\n",
        "**PDF**:\n",
        "$$p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
        "\n",
        "**Constraints**:\n",
        "$$\\sum_{k=1}^{K} \\pi_k = 1, \\quad \\pi_k \\geq 0$$\n",
        "\n",
        "**Latent Variable Formulation**:\n",
        "$$z \\sim \\text{Categorical}(\\pi_1, ..., \\pi_K)$$\n",
        "$$x | z = k \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$$\n",
        "\n",
        "**Log-Likelihood**:\n",
        "$$\\log p(X | \\theta) = \\sum_{n=1}^{N} \\log \\left[\\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)\\right]$$\n",
        "\n",
        "**EM Algorithm for GMM**:\n",
        "\n",
        "**E-step** (Compute responsibilities):\n",
        "$$\\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)}$$\n",
        "\n",
        "**M-step** (Update parameters):\n",
        "$$N_k = \\sum_{n=1}^{N} \\gamma_{nk}$$\n",
        "$$\\mu_k^{new} = \\frac{1}{N_k} \\sum_{n=1}^{N} \\gamma_{nk} x_n$$\n",
        "$$\\Sigma_k^{new} = \\frac{1}{N_k} \\sum_{n=1}^{N} \\gamma_{nk} (x_n - \\mu_k^{new})(x_n - \\mu_k^{new})^T$$\n",
        "$$\\pi_k^{new} = \\frac{N_k}{N}$$\n",
        "\n",
        "**Applications in Gen-AI**:\n",
        "- VQ-VAE prior modeling\n",
        "- Multi-modal generation\n",
        "- Density estimation for likelihood-based models\n",
        "\n",
        "### 5.2 Mixture of Experts (MoE)\n",
        "\n",
        "$$p(y|x) = \\sum_{k=1}^{K} g_k(x) \\cdot p_k(y|x)$$\n",
        "\n",
        "where $g_k(x)$ is gating network output (softmax over experts).\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Probability Distributions in Specific Gen-AI Architectures\n",
        "\n",
        "### 6.1 Variational Autoencoders (VAEs)\n",
        "\n",
        "**Generative Model**:\n",
        "$$p_\\theta(x, z) = p_\\theta(x|z) p(z)$$\n",
        "\n",
        "**Prior**: $p(z) = \\mathcal{N}(0, I)$\n",
        "\n",
        "**Approximate Posterior**: $q_\\phi(z|x) = \\mathcal{N}(\\mu_\\phi(x), \\text{diag}(\\sigma_\\phi^2(x)))$\n",
        "\n",
        "**Evidence Lower Bound (ELBO)**:\n",
        "$$\\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p(z))$$\n",
        "\n",
        "**Reparameterization Trick**:\n",
        "$$z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
        "\n",
        "**Decoder Distribution Choices**:\n",
        "\n",
        "| Data Type | Distribution | Loss |\n",
        "|-----------|--------------|------|\n",
        "| Real-valued | Gaussian $\\mathcal{N}(\\mu_\\theta(z), \\sigma^2 I)$ | MSE |\n",
        "| Binary | Bernoulli | Binary Cross-Entropy |\n",
        "| Categorical | Categorical | Cross-Entropy |\n",
        "\n",
        "### 6.2 Generative Adversarial Networks (GANs)\n",
        "\n",
        "**Implicit Distribution**: GANs define distributions implicitly through transformation.\n",
        "\n",
        "$$z \\sim p_z(z) = \\mathcal{N}(0, I) \\text{ or } \\text{Uniform}(-1, 1)$$\n",
        "$$x = G_\\theta(z)$$\n",
        "\n",
        "The generated distribution $p_g(x)$ is implicitly defined.\n",
        "\n",
        "**Original GAN Objective**:\n",
        "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\n",
        "\n",
        "**Optimal Discriminator**:\n",
        "$$D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$$\n",
        "\n",
        "**At Optimality**:\n",
        "$$\\min_G V(G, D^*) = 2 \\cdot D_{JS}(p_{data} \\| p_g) - \\log 4$$\n",
        "\n",
        "### 6.3 Diffusion Models\n",
        "\n",
        "#### 6.3.1 Forward Process (Adding Noise)\n",
        "\n",
        "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)$$\n",
        "\n",
        "**Marginal at time $t$**:\n",
        "$$q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t) I)$$\n",
        "\n",
        "where $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$\n",
        "\n",
        "**Direct Sampling**:\n",
        "$$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
        "\n",
        "#### 6.3.2 Reverse Process (Denoising)\n",
        "\n",
        "$$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$$\n",
        "\n",
        "**True Posterior (Tractable)**:\n",
        "$$q(x_{t-1} | x_t, x_0) = \\mathcal{N}(x_{t-1}; \\tilde{\\mu}_t(x_t, x_0), \\tilde{\\beta}_t I)$$\n",
        "\n",
        "where:\n",
        "$$\\tilde{\\mu}_t(x_t, x_0) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1-\\bar{\\alpha}_t} x_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t} x_t$$\n",
        "$$\\tilde{\\beta}_t = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} \\beta_t$$\n",
        "\n",
        "#### 6.3.3 Training Objective\n",
        "\n",
        "**Simplified Loss** (DDPM):\n",
        "$$\\mathcal{L}_{simple} = \\mathbb{E}_{t, x_0, \\epsilon}\\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right]$$\n",
        "\n",
        "### 6.4 Autoregressive Language Models\n",
        "\n",
        "**Factorization**:\n",
        "$$p(x_1, x_2, ..., x_T) = \\prod_{t=1}^{T} p(x_t | x_{<t})$$\n",
        "\n",
        "**Token Distribution**:\n",
        "$$p(x_t | x_{<t}) = \\text{Categorical}(\\text{softmax}(W h_t + b))$$\n",
        "\n",
        "where $h_t$ is hidden state from transformer.\n",
        "\n",
        "**Temperature Scaling**:\n",
        "$$p(x_t = k | x_{<t}) = \\frac{\\exp(z_k / \\tau)}{\\sum_j \\exp(z_j / \\tau)}$$\n",
        "\n",
        "- $\\tau < 1$: Sharper distribution (more deterministic)\n",
        "- $\\tau > 1$: Flatter distribution (more random)\n",
        "- $\\tau = 1$: Original distribution\n",
        "\n",
        "**Top-k Sampling**:\n",
        "$$p'(x_t) \\propto \\begin{cases} p(x_t) & \\text{if } x_t \\in \\text{top-}k \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
        "\n",
        "**Nucleus (Top-p) Sampling**:\n",
        "$$p'(x_t) \\propto \\begin{cases} p(x_t) & \\text{if } x_t \\in V_p \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
        "\n",
        "where $V_p = \\min\\{V': \\sum_{x \\in V'} p(x) \\geq p\\}$\n",
        "\n",
        "### 6.5 Normalizing Flows\n",
        "\n",
        "**Definition**: Transform simple distribution to complex through invertible mappings.\n",
        "\n",
        "$$x = f_\\theta(z), \\quad z \\sim p_z(z)$$\n",
        "\n",
        "**Change of Variables**:\n",
        "$$p_x(x) = p_z(f_\\theta^{-1}(x)) \\left|\\det \\frac{\\partial f_\\theta^{-1}}{\\partial x}\\right|$$\n",
        "\n",
        "**Log-likelihood**:\n",
        "$$\\log p_x(x) = \\log p_z(z) + \\sum_{i=1}^{K} \\log \\left|\\det \\frac{\\partial f_i}{\\partial z_{i-1}}\\right|$$\n",
        "\n",
        "for composition $f = f_K \\circ ... \\circ f_1$\n",
        "\n",
        "**Flow Types and Their Jacobians**:\n",
        "\n",
        "| Flow Type | Jacobian Structure | Complexity |\n",
        "|-----------|-------------------|------------|\n",
        "| Planar | Rank-1 update | $O(D)$ |\n",
        "| Radial | Radial | $O(D)$ |\n",
        "| Coupling (RealNVP) | Triangular | $O(D)$ |\n",
        "| Autoregressive (MAF) | Triangular | $O(D)$ |\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Advanced Probabilistic Concepts for Gen-AI\n",
        "\n",
        "### 7.1 Reparameterization Trick\n",
        "\n",
        "**Problem**: Cannot backpropagate through stochastic sampling.\n",
        "\n",
        "**Solution**: Express random variable as deterministic function of parameters and noise.\n",
        "\n",
        "**General Form**:\n",
        "$$z = g(\\phi, \\epsilon), \\quad \\epsilon \\sim p(\\epsilon)$$\n",
        "\n",
        "**Examples**:\n",
        "\n",
        "| Distribution | Reparameterization |\n",
        "|--------------|-------------------|\n",
        "| Gaussian | $z = \\mu + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)$ |\n",
        "| Exponential | $z = -\\frac{1}{\\lambda} \\log(1 - u), \\quad u \\sim \\text{Uniform}(0, 1)$ |\n",
        "| Gamma | Rejection sampling based |\n",
        "\n",
        "### 7.2 Gumbel-Softmax for Discrete Distributions\n",
        "\n",
        "**Problem**: Categorical sampling is non-differentiable.\n",
        "\n",
        "**Gumbel-Max Trick**:\n",
        "$$z = \\arg\\max_k [g_k + \\log \\pi_k]$$\n",
        "\n",
        "where $g_k \\sim \\text{Gumbel}(0, 1)$\n",
        "\n",
        "**Gumbel-Softmax (Continuous Relaxation)**:\n",
        "$$y_k = \\frac{\\exp((\\log \\pi_k + g_k) / \\tau)}{\\sum_j \\exp((\\log \\pi_j + g_j) / \\tau)}$$\n",
        "\n",
        "As $\\tau \\to 0$, approaches one-hot categorical.\n",
        "\n",
        "**Gumbel Distribution**:\n",
        "$$p(g) = \\exp(-(g + \\exp(-g)))$$\n",
        "$$g = -\\log(-\\log(u)), \\quad u \\sim \\text{Uniform}(0, 1)$$\n",
        "\n",
        "### 7.3 Score Function and Score Matching\n",
        "\n",
        "**Score Function**:\n",
        "$$s_\\theta(x) = \\nabla_x \\log p_\\theta(x)$$\n",
        "\n",
        "**Score Matching Objective**:\n",
        "$$\\mathcal{L}_{SM} = \\mathbb{E}_{p_{data}}\\left[\\frac{1}{2}\\|s_\\theta(x)\\|^2 + \\text{tr}(\\nabla_x s_\\theta(x))\\right]$$\n",
        "\n",
        "**Denoising Score Matching**:\n",
        "$$\\mathcal{L}_{DSM} = \\mathbb{E}_{p_{data}(x), q_\\sigma(\\tilde{x}|x)}\\left[\\|s_\\theta(\\tilde{x}) - \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}|x)\\|^2\\right]$$\n",
        "\n",
        "### 7.4 Evidence Lower Bound (ELBO) Derivation\n",
        "\n",
        "Starting from log-likelihood:\n",
        "$$\\log p(x) = \\log \\int p(x, z) dz$$\n",
        "\n",
        "Introduce variational distribution $q(z|x)$:\n",
        "$$\\log p(x) = \\log \\int \\frac{p(x, z)}{q(z|x)} q(z|x) dz$$\n",
        "\n",
        "Apply Jensen's inequality:\n",
        "$$\\log p(x) \\geq \\int q(z|x) \\log \\frac{p(x, z)}{q(z|x)} dz = \\mathcal{L}_{ELBO}$$\n",
        "\n",
        "**Decomposition**:\n",
        "$$\\mathcal{L}_{ELBO} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) \\| p(z))$$\n",
        "\n",
        "**Gap Analysis**:\n",
        "$$\\log p(x) = \\mathcal{L}_{ELBO} + D_{KL}(q(z|x) \\| p(z|x))$$\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Probability Distribution Selection Guide for Gen-AI\n",
        "\n",
        "| Task | Recommended Distribution | Justification |\n",
        "|------|-------------------------|---------------|\n",
        "| **Continuous Latent Space** | Gaussian | Reparameterizable, closed-form KL |\n",
        "| **Binary Data** | Bernoulli | Natural for binary outputs |\n",
        "| **Token Generation** | Categorical | Discrete vocabulary |\n",
        "| **Image Generation (pixels)** | Gaussian/Discretized Logistic | Continuous approximation |\n",
        "| **Sparse Representations** | Laplace | Promotes sparsity |\n",
        "| **Robust Generation** | Student's t | Heavy tails handle outliers |\n",
        "| **Probability Parameters** | Beta/Dirichlet | Constrained to simplex |\n",
        "| **Covariance Learning** | Wishart | Positive-definite constraint |\n",
        "| **Mixture Modeling** | GMM | Multi-modal flexibility |\n",
        "| **Discrete Latent** | Gumbel-Softmax | Differentiable relaxation |\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Summary: Probabilistic Framework for Generative AI\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                    GENERATIVE AI PIPELINE                       │\n",
        "├─────────────────────────────────────────────────────────────────┤\n",
        "│                                                                 │\n",
        "│   DATA: x ~ p_data(x)                                          │\n",
        "│                                                                 │\n",
        "│           ↓                                                    │\n",
        "│                                                                 │\n",
        "│   LATENT SPACE: z ~ p(z)  [Prior: N(0,I), Uniform, etc.]       │\n",
        "│                                                                 │\n",
        "│           ↓                                                    │\n",
        "│                                                                 │\n",
        "│   TRANSFORMATION: x = f_θ(z) or p_θ(x|z)                       │\n",
        "│                                                                 │\n",
        "│           ↓                                                    │\n",
        "│                                                                 │\n",
        "│   OBJECTIVE: min D(p_data || p_model)                          │\n",
        "│              [KL, JS, Wasserstein, Score Matching]              │\n",
        "│                                                                 │\n",
        "│           ↓                                                    │\n",
        "│                                                                 │\n",
        "│   SAMPLING: x_new ~ p_θ(x)                                     │\n",
        "│                                                                 │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "**Core Distributions by Gen-AI Architecture**:\n",
        "\n",
        "| Architecture | Key Distributions Used |\n",
        "|--------------|----------------------|\n",
        "| **VAE** | Gaussian (prior, posterior, likelihood) |\n",
        "| **GAN** | Gaussian/Uniform (latent), Implicit (generated) |\n",
        "| **Diffusion** | Gaussian (forward/reverse process) |\n",
        "| **Autoregressive LM** | Categorical (token prediction) |\n",
        "| **Flow** | Base distribution → transformed complex distribution |\n",
        "| **VQ-VAE** | Categorical (codebook), Gaussian (encoder) |"
      ],
      "metadata": {
        "id": "7jdo6OrB3NQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expectation, Moments, Covariance, Correlation, and Convolution for Generative AI\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Expectation (Expected Value)\n",
        "\n",
        "### 1.1 Definition\n",
        "\n",
        "**Expectation** is the probability-weighted average of all possible values of a random variable, representing the central tendency or \"mean\" of a distribution.\n",
        "\n",
        "#### 1.1.1 Discrete Random Variable\n",
        "\n",
        "For a discrete random variable $X$ with probability mass function $P(X = x_i)$:\n",
        "\n",
        "$$\\mathbb{E}[X] = \\sum_{i} x_i \\cdot P(X = x_i) = \\sum_{i} x_i \\cdot p(x_i)$$\n",
        "\n",
        "#### 1.1.2 Continuous Random Variable\n",
        "\n",
        "For a continuous random variable $X$ with probability density function $p(x)$:\n",
        "\n",
        "$$\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot p(x) \\, dx$$\n",
        "\n",
        "#### 1.1.3 General Form (Lebesgue Integration)\n",
        "\n",
        "$$\\mathbb{E}[X] = \\int_{\\Omega} X(\\omega) \\, dP(\\omega)$$\n",
        "\n",
        "### 1.2 Expectation of a Function\n",
        "\n",
        "For a function $g(X)$:\n",
        "\n",
        "**Discrete**:\n",
        "$$\\mathbb{E}[g(X)] = \\sum_{i} g(x_i) \\cdot p(x_i)$$\n",
        "\n",
        "**Continuous**:\n",
        "$$\\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) \\cdot p(x) \\, dx$$\n",
        "\n",
        "### 1.3 Properties of Expectation\n",
        "\n",
        "| Property | Mathematical Expression |\n",
        "|----------|------------------------|\n",
        "| **Linearity** | $\\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]$ |\n",
        "| **Constant** | $\\mathbb{E}[c] = c$ |\n",
        "| **Non-negativity** | If $X \\geq 0$, then $\\mathbb{E}[X] \\geq 0$ |\n",
        "| **Independence** | If $X \\perp Y$: $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$ |\n",
        "| **Monotonicity** | If $X \\leq Y$, then $\\mathbb{E}[X] \\leq \\mathbb{E}[Y]$ |\n",
        "| **Tower Property** | $\\mathbb{E}[\\mathbb{E}[X|Y]] = \\mathbb{E}[X]$ |\n",
        "\n",
        "### 1.4 Conditional Expectation\n",
        "\n",
        "**Definition**: Expected value of $X$ given information about $Y$:\n",
        "\n",
        "$$\\mathbb{E}[X | Y = y] = \\int x \\cdot p(x|y) \\, dx$$\n",
        "\n",
        "**As a Random Variable**:\n",
        "$$\\mathbb{E}[X | Y] = g(Y)$$\n",
        "\n",
        "where $g(y) = \\mathbb{E}[X | Y = y]$\n",
        "\n",
        "**Law of Total Expectation (Tower Property)**:\n",
        "$$\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X | Y]] = \\int \\mathbb{E}[X | Y = y] \\cdot p(y) \\, dy$$\n",
        "\n",
        "### 1.5 Multivariate Expectation\n",
        "\n",
        "For a random vector $\\mathbf{X} = (X_1, X_2, ..., X_n)^T$:\n",
        "\n",
        "$$\\mathbb{E}[\\mathbf{X}] = \\begin{pmatrix} \\mathbb{E}[X_1] \\\\ \\mathbb{E}[X_2] \\\\ \\vdots \\\\ \\mathbb{E}[X_n] \\end{pmatrix} = \\boldsymbol{\\mu}$$\n",
        "\n",
        "**Matrix Expectation**: For random matrix $\\mathbf{A}$:\n",
        "$$\\mathbb{E}[\\mathbf{A}]_{ij} = \\mathbb{E}[A_{ij}]$$\n",
        "\n",
        "### 1.6 Applications in Generative AI\n",
        "\n",
        "#### 1.6.1 Training Objective (Empirical Expectation)\n",
        "\n",
        "**True Expectation**:\n",
        "$$\\mathcal{L}(\\theta) = \\mathbb{E}_{x \\sim p_{data}}[\\ell(x; \\theta)]$$\n",
        "\n",
        "**Monte Carlo Approximation**:\n",
        "$$\\hat{\\mathcal{L}}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(x_i; \\theta)$$\n",
        "\n",
        "#### 1.6.2 VAE ELBO\n",
        "\n",
        "$$\\mathcal{L}_{ELBO} = \\mathbb{E}_{z \\sim q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p(z))$$\n",
        "\n",
        "#### 1.6.3 GAN Generator Objective\n",
        "\n",
        "$$\\mathcal{L}_G = \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\n",
        "\n",
        "or non-saturating:\n",
        "$$\\mathcal{L}_G = -\\mathbb{E}_{z \\sim p_z}[\\log D(G(z))]$$\n",
        "\n",
        "#### 1.6.4 Diffusion Model Training\n",
        "\n",
        "$$\\mathcal{L}_{simple} = \\mathbb{E}_{t, x_0, \\epsilon}\\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right]$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Moments of a Distribution\n",
        "\n",
        "### 2.1 Definition of Moments\n",
        "\n",
        "Moments are quantitative measures that characterize the shape and properties of a probability distribution.\n",
        "\n",
        "### 2.2 Raw Moments (Moments about Origin)\n",
        "\n",
        "**Definition**: The $n$-th raw moment of random variable $X$:\n",
        "\n",
        "$$\\mu'_n = \\mathbb{E}[X^n] = \\int_{-\\infty}^{\\infty} x^n \\cdot p(x) \\, dx$$\n",
        "\n",
        "| Order | Name | Formula | Interpretation |\n",
        "|-------|------|---------|----------------|\n",
        "| $n=1$ | Mean | $\\mu'_1 = \\mathbb{E}[X]$ | Central location |\n",
        "| $n=2$ | Second raw moment | $\\mu'_2 = \\mathbb{E}[X^2]$ | Related to energy/power |\n",
        "| $n=k$ | k-th raw moment | $\\mu'_k = \\mathbb{E}[X^k]$ | Higher-order statistics |\n",
        "\n",
        "### 2.3 Central Moments (Moments about Mean)\n",
        "\n",
        "**Definition**: The $n$-th central moment:\n",
        "\n",
        "$$\\mu_n = \\mathbb{E}[(X - \\mu)^n] = \\mathbb{E}[(X - \\mathbb{E}[X])^n]$$\n",
        "\n",
        "| Order | Name | Formula | Interpretation |\n",
        "|-------|------|---------|----------------|\n",
        "| $n=1$ | First central moment | $\\mu_1 = 0$ | Always zero |\n",
        "| $n=2$ | Variance | $\\mu_2 = \\mathbb{E}[(X-\\mu)^2] = \\sigma^2$ | Spread/dispersion |\n",
        "| $n=3$ | Third central moment | $\\mu_3 = \\mathbb{E}[(X-\\mu)^3]$ | Asymmetry (unnormalized) |\n",
        "| $n=4$ | Fourth central moment | $\\mu_4 = \\mathbb{E}[(X-\\mu)^4]$ | Tail heaviness (unnormalized) |\n",
        "\n",
        "### 2.4 Standardized Moments\n",
        "\n",
        "**Definition**: Central moments normalized by appropriate power of standard deviation:\n",
        "\n",
        "$$\\tilde{\\mu}_n = \\frac{\\mu_n}{\\sigma^n} = \\frac{\\mathbb{E}[(X-\\mu)^n]}{\\sigma^n}$$\n",
        "\n",
        "### 2.5 Key Standardized Moments\n",
        "\n",
        "#### 2.5.1 Skewness (Third Standardized Moment)\n",
        "\n",
        "$$\\gamma_1 = \\frac{\\mu_3}{\\sigma^3} = \\frac{\\mathbb{E}[(X-\\mu)^3]}{(\\mathbb{E}[(X-\\mu)^2])^{3/2}}$$\n",
        "\n",
        "**Interpretation**:\n",
        "- $\\gamma_1 > 0$: Right-skewed (positive skew), tail extends right\n",
        "- $\\gamma_1 < 0$: Left-skewed (negative skew), tail extends left\n",
        "- $\\gamma_1 = 0$: Symmetric distribution\n",
        "\n",
        "```\n",
        "Left-skewed (γ₁ < 0)     Symmetric (γ₁ = 0)     Right-skewed (γ₁ > 0)\n",
        "        ___                    ___                    ___\n",
        "       /   \\                  /   \\                  /   \\\n",
        "      /     \\__              /     \\                _/     \\\n",
        "    _/         \\_           /       \\              /        \\_\n",
        "```\n",
        "\n",
        "#### 2.5.2 Kurtosis (Fourth Standardized Moment)\n",
        "\n",
        "**Kurtosis**:\n",
        "$$\\gamma_2 = \\frac{\\mu_4}{\\sigma^4} = \\frac{\\mathbb{E}[(X-\\mu)^4]}{(\\mathbb{E}[(X-\\mu)^2])^2}$$\n",
        "\n",
        "**Excess Kurtosis** (relative to Gaussian):\n",
        "$$\\kappa = \\gamma_2 - 3 = \\frac{\\mu_4}{\\sigma^4} - 3$$\n",
        "\n",
        "**Interpretation**:\n",
        "- $\\kappa > 0$: Leptokurtic (heavier tails than Gaussian)\n",
        "- $\\kappa < 0$: Platykurtic (lighter tails than Gaussian)\n",
        "- $\\kappa = 0$: Mesokurtic (Gaussian-like tails)\n",
        "\n",
        "| Distribution | Excess Kurtosis |\n",
        "|--------------|-----------------|\n",
        "| Gaussian | $\\kappa = 0$ |\n",
        "| Uniform | $\\kappa = -1.2$ |\n",
        "| Laplace | $\\kappa = 3$ |\n",
        "| Student's t ($\\nu > 4$) | $\\kappa = \\frac{6}{\\nu - 4}$ |\n",
        "\n",
        "### 2.6 Relationship Between Raw and Central Moments\n",
        "\n",
        "$$\\mu_n = \\sum_{k=0}^{n} \\binom{n}{k} (-1)^{n-k} \\mu'^k_1 \\mu'_{n-k}$$\n",
        "\n",
        "**Common Relationships**:\n",
        "$$\\mu_2 = \\mu'_2 - (\\mu'_1)^2 = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$$\n",
        "$$\\mu_3 = \\mu'_3 - 3\\mu'_1\\mu'_2 + 2(\\mu'_1)^3$$\n",
        "$$\\mu_4 = \\mu'_4 - 4\\mu'_1\\mu'_3 + 6(\\mu'_1)^2\\mu'_2 - 3(\\mu'_1)^4$$\n",
        "\n",
        "### 2.7 Moment Generating Function (MGF)\n",
        "\n",
        "**Definition**:\n",
        "$$M_X(t) = \\mathbb{E}[e^{tX}] = \\int_{-\\infty}^{\\infty} e^{tx} p(x) \\, dx$$\n",
        "\n",
        "**Property**: The $n$-th derivative at $t=0$ gives the $n$-th raw moment:\n",
        "$$\\mu'_n = \\frac{d^n M_X(t)}{dt^n}\\bigg|_{t=0}$$\n",
        "\n",
        "**Taylor Expansion**:\n",
        "$$M_X(t) = \\sum_{n=0}^{\\infty} \\frac{\\mu'_n t^n}{n!} = 1 + \\mu'_1 t + \\frac{\\mu'_2 t^2}{2!} + \\frac{\\mu'_3 t^3}{3!} + ...$$\n",
        "\n",
        "### 2.8 Characteristic Function\n",
        "\n",
        "**Definition** (Fourier transform of PDF):\n",
        "$$\\phi_X(t) = \\mathbb{E}[e^{itX}] = \\int_{-\\infty}^{\\infty} e^{itx} p(x) \\, dx$$\n",
        "\n",
        "**Property**: Always exists (unlike MGF), and:\n",
        "$$\\mu'_n = \\frac{1}{i^n} \\frac{d^n \\phi_X(t)}{dt^n}\\bigg|_{t=0}$$\n",
        "\n",
        "### 2.9 Cumulants\n",
        "\n",
        "**Cumulant Generating Function**:\n",
        "$$K_X(t) = \\log M_X(t) = \\sum_{n=1}^{\\infty} \\kappa_n \\frac{t^n}{n!}$$\n",
        "\n",
        "**Key Cumulants**:\n",
        "| Cumulant | Expression | Name |\n",
        "|----------|------------|------|\n",
        "| $\\kappa_1$ | $\\mu$ | Mean |\n",
        "| $\\kappa_2$ | $\\sigma^2$ | Variance |\n",
        "| $\\kappa_3$ | $\\mu_3$ | Third cumulant |\n",
        "| $\\kappa_4$ | $\\mu_4 - 3\\sigma^4$ | Excess kurtosis × $\\sigma^4$ |\n",
        "\n",
        "**Property**: For independent $X, Y$:\n",
        "$$K_{X+Y}(t) = K_X(t) + K_Y(t)$$\n",
        "\n",
        "### 2.10 Applications of Moments in Gen-AI\n",
        "\n",
        "#### 2.10.1 Batch Normalization\n",
        "\n",
        "$$\\hat{x}_i = \\frac{x_i - \\mathbb{E}[X]}{\\sqrt{\\text{Var}(X) + \\epsilon}}$$\n",
        "\n",
        "Uses first moment (mean) and second central moment (variance).\n",
        "\n",
        "#### 2.10.2 Layer Normalization\n",
        "\n",
        "$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad \\mu = \\frac{1}{D}\\sum_{j=1}^{D} x_j, \\quad \\sigma^2 = \\frac{1}{D}\\sum_{j=1}^{D}(x_j - \\mu)^2$$\n",
        "\n",
        "#### 2.10.3 Feature Matching in GANs\n",
        "\n",
        "Match statistics of real and generated distributions:\n",
        "$$\\mathcal{L}_{FM} = \\|\\mathbb{E}_{x \\sim p_{data}}[f(x)] - \\mathbb{E}_{z \\sim p_z}[f(G(z))]\\|^2$$\n",
        "\n",
        "#### 2.10.4 Maximum Mean Discrepancy (MMD)\n",
        "\n",
        "$$\\text{MMD}^2(p, q) = \\mathbb{E}_{x,x' \\sim p}[k(x,x')] - 2\\mathbb{E}_{x \\sim p, y \\sim q}[k(x,y)] + \\mathbb{E}_{y,y' \\sim q}[k(y,y')]$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Variance and Standard Deviation\n",
        "\n",
        "### 3.1 Variance\n",
        "\n",
        "**Definition**: Second central moment measuring spread around the mean.\n",
        "\n",
        "$$\\text{Var}(X) = \\sigma^2 = \\mathbb{E}[(X - \\mu)^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$$\n",
        "\n",
        "**Discrete**:\n",
        "$$\\text{Var}(X) = \\sum_{i} (x_i - \\mu)^2 \\cdot p(x_i)$$\n",
        "\n",
        "**Continuous**:\n",
        "$$\\text{Var}(X) = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 \\cdot p(x) \\, dx$$\n",
        "\n",
        "### 3.2 Standard Deviation\n",
        "\n",
        "$$\\sigma = \\sqrt{\\text{Var}(X)} = \\sqrt{\\mathbb{E}[(X - \\mu)^2]}$$\n",
        "\n",
        "### 3.3 Properties of Variance\n",
        "\n",
        "| Property | Formula |\n",
        "|----------|---------|\n",
        "| Non-negativity | $\\text{Var}(X) \\geq 0$ |\n",
        "| Constant | $\\text{Var}(c) = 0$ |\n",
        "| Scaling | $\\text{Var}(aX) = a^2 \\text{Var}(X)$ |\n",
        "| Translation invariance | $\\text{Var}(X + c) = \\text{Var}(X)$ |\n",
        "| Sum (independent) | $\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)$ if $X \\perp Y$ |\n",
        "| General sum | $\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X, Y)$ |\n",
        "\n",
        "### 3.4 Law of Total Variance\n",
        "\n",
        "$$\\text{Var}(X) = \\mathbb{E}[\\text{Var}(X|Y)] + \\text{Var}(\\mathbb{E}[X|Y])$$\n",
        "\n",
        "**Interpretation**:\n",
        "- $\\mathbb{E}[\\text{Var}(X|Y)]$: Average variance within groups\n",
        "- $\\text{Var}(\\mathbb{E}[X|Y])$: Variance between group means\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Covariance\n",
        "\n",
        "### 4.1 Definition\n",
        "\n",
        "**Covariance** measures the joint variability of two random variables.\n",
        "\n",
        "$$\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]$$\n",
        "\n",
        "**Alternative Formula**:\n",
        "$$\\text{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$$\n",
        "\n",
        "**Integral Form**:\n",
        "$$\\text{Cov}(X, Y) = \\iint (x - \\mu_X)(y - \\mu_Y) \\cdot p(x, y) \\, dx \\, dy$$\n",
        "\n",
        "### 4.2 Properties of Covariance\n",
        "\n",
        "| Property | Formula |\n",
        "|----------|---------|\n",
        "| Symmetry | $\\text{Cov}(X, Y) = \\text{Cov}(Y, X)$ |\n",
        "| Self-covariance | $\\text{Cov}(X, X) = \\text{Var}(X)$ |\n",
        "| With constant | $\\text{Cov}(X, c) = 0$ |\n",
        "| Linearity (first arg) | $\\text{Cov}(aX + b, Y) = a \\cdot \\text{Cov}(X, Y)$ |\n",
        "| Bilinearity | $\\text{Cov}(aX + bY, cZ + dW) = ac\\text{Cov}(X,Z) + ad\\text{Cov}(X,W) + bc\\text{Cov}(Y,Z) + bd\\text{Cov}(Y,W)$ |\n",
        "| Independence | If $X \\perp Y$: $\\text{Cov}(X, Y) = 0$ |\n",
        "\n",
        "**Note**: $\\text{Cov}(X, Y) = 0$ does NOT imply independence (only uncorrelated).\n",
        "\n",
        "### 4.3 Covariance Matrix\n",
        "\n",
        "For random vector $\\mathbf{X} = (X_1, X_2, ..., X_n)^T$:\n",
        "\n",
        "$$\\boldsymbol{\\Sigma} = \\text{Cov}(\\mathbf{X}) = \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T]$$\n",
        "\n",
        "**Matrix Form**:\n",
        "$$\\boldsymbol{\\Sigma} = \\begin{pmatrix}\n",
        "\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_n) \\\\\n",
        "\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\cdots & \\text{Cov}(X_2, X_n) \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\text{Cov}(X_n, X_1) & \\text{Cov}(X_n, X_2) & \\cdots & \\text{Var}(X_n)\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "**Element-wise**:\n",
        "$$\\Sigma_{ij} = \\text{Cov}(X_i, X_j) = \\mathbb{E}[(X_i - \\mu_i)(X_j - \\mu_j)]$$\n",
        "\n",
        "**Alternative Form**:\n",
        "$$\\boldsymbol{\\Sigma} = \\mathbb{E}[\\mathbf{X}\\mathbf{X}^T] - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T$$\n",
        "\n",
        "### 4.4 Properties of Covariance Matrix\n",
        "\n",
        "1. **Symmetric**: $\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}^T$\n",
        "\n",
        "2. **Positive Semi-Definite**: For any vector $\\mathbf{a}$:\n",
        "$$\\mathbf{a}^T \\boldsymbol{\\Sigma} \\mathbf{a} = \\text{Var}(\\mathbf{a}^T \\mathbf{X}) \\geq 0$$\n",
        "\n",
        "3. **Linear Transformation**: For $\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{b}$:\n",
        "$$\\text{Cov}(\\mathbf{Y}) = \\mathbf{A} \\boldsymbol{\\Sigma}_X \\mathbf{A}^T$$\n",
        "\n",
        "4. **Eigendecomposition**:\n",
        "$$\\boldsymbol{\\Sigma} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^T$$\n",
        "where $\\boldsymbol{\\Lambda} = \\text{diag}(\\lambda_1, ..., \\lambda_n)$ with $\\lambda_i \\geq 0$\n",
        "\n",
        "### 4.5 Precision Matrix (Inverse Covariance)\n",
        "\n",
        "$$\\boldsymbol{\\Omega} = \\boldsymbol{\\Sigma}^{-1}$$\n",
        "\n",
        "**Significance**: $\\Omega_{ij} = 0$ implies conditional independence:\n",
        "$$X_i \\perp X_j | \\mathbf{X}_{\\setminus\\{i,j\\}}$$\n",
        "\n",
        "### 4.6 Applications of Covariance in Gen-AI\n",
        "\n",
        "#### 4.6.1 Multivariate Gaussian in VAE\n",
        "\n",
        "$$p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$$\n",
        "$$q_\\phi(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu}_\\phi(\\mathbf{x}), \\text{diag}(\\boldsymbol{\\sigma}_\\phi^2(\\mathbf{x})))$$\n",
        "\n",
        "#### 4.6.2 KL Divergence with Covariance\n",
        "\n",
        "$$D_{KL}(\\mathcal{N}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_1) \\| \\mathcal{N}(\\boldsymbol{\\mu}_2, \\boldsymbol{\\Sigma}_2)) = \\frac{1}{2}\\left[\\log\\frac{|\\boldsymbol{\\Sigma}_2|}{|\\boldsymbol{\\Sigma}_1|} - D + \\text{tr}(\\boldsymbol{\\Sigma}_2^{-1}\\boldsymbol{\\Sigma}_1) + (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}_2^{-1} (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)\\right]$$\n",
        "\n",
        "#### 4.6.3 Whitening Transformation\n",
        "\n",
        "**Objective**: Transform data to have identity covariance.\n",
        "\n",
        "$$\\mathbf{X}_{white} = \\boldsymbol{\\Sigma}^{-1/2}(\\mathbf{X} - \\boldsymbol{\\mu})$$\n",
        "\n",
        "**Result**: $\\text{Cov}(\\mathbf{X}_{white}) = \\mathbf{I}$\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Cross-Covariance\n",
        "\n",
        "### 5.1 Definition\n",
        "\n",
        "**Cross-covariance** measures the covariance between elements of two different random vectors.\n",
        "\n",
        "For random vectors $\\mathbf{X} \\in \\mathbb{R}^m$ and $\\mathbf{Y} \\in \\mathbb{R}^n$:\n",
        "\n",
        "$$\\boldsymbol{\\Sigma}_{XY} = \\text{Cov}(\\mathbf{X}, \\mathbf{Y}) = \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu}_X)(\\mathbf{Y} - \\boldsymbol{\\mu}_Y)^T]$$\n",
        "\n",
        "### 5.2 Matrix Form\n",
        "\n",
        "$$\\boldsymbol{\\Sigma}_{XY} \\in \\mathbb{R}^{m \\times n}$$\n",
        "\n",
        "$$(\\boldsymbol{\\Sigma}_{XY})_{ij} = \\text{Cov}(X_i, Y_j) = \\mathbb{E}[(X_i - \\mu_{X_i})(Y_j - \\mu_{Y_j})]$$\n",
        "\n",
        "### 5.3 Alternative Formula\n",
        "\n",
        "$$\\boldsymbol{\\Sigma}_{XY} = \\mathbb{E}[\\mathbf{X}\\mathbf{Y}^T] - \\boldsymbol{\\mu}_X \\boldsymbol{\\mu}_Y^T$$\n",
        "\n",
        "### 5.4 Properties\n",
        "\n",
        "1. **Relationship to Transpose**: $\\boldsymbol{\\Sigma}_{YX} = \\boldsymbol{\\Sigma}_{XY}^T$\n",
        "\n",
        "2. **Joint Covariance Matrix**:\n",
        "$$\\text{Cov}\\begin{pmatrix} \\mathbf{X} \\\\ \\mathbf{Y} \\end{pmatrix} = \\begin{pmatrix} \\boldsymbol{\\Sigma}_{XX} & \\boldsymbol{\\Sigma}_{XY} \\\\ \\boldsymbol{\\Sigma}_{YX} & \\boldsymbol{\\Sigma}_{YY} \\end{pmatrix}$$\n",
        "\n",
        "3. **Linear Transformation**:\n",
        "$$\\text{Cov}(\\mathbf{A}\\mathbf{X}, \\mathbf{B}\\mathbf{Y}) = \\mathbf{A} \\boldsymbol{\\Sigma}_{XY} \\mathbf{B}^T$$\n",
        "\n",
        "### 5.5 Cross-Covariance Function (Stochastic Processes)\n",
        "\n",
        "For stochastic processes $\\{X_t\\}$ and $\\{Y_t\\}$:\n",
        "\n",
        "$$C_{XY}(t_1, t_2) = \\text{Cov}(X_{t_1}, Y_{t_2}) = \\mathbb{E}[(X_{t_1} - \\mu_X(t_1))(Y_{t_2} - \\mu_Y(t_2))]$$\n",
        "\n",
        "**For Stationary Processes** (depends only on lag $\\tau = t_2 - t_1$):\n",
        "$$C_{XY}(\\tau) = \\mathbb{E}[(X_t - \\mu_X)(Y_{t+\\tau} - \\mu_Y)]$$\n",
        "\n",
        "### 5.6 Applications in Gen-AI\n",
        "\n",
        "#### 5.6.1 Canonical Correlation Analysis (CCA)\n",
        "\n",
        "Find projections maximizing correlation between two views:\n",
        "$$\\max_{\\mathbf{w}_x, \\mathbf{w}_y} \\frac{\\mathbf{w}_x^T \\boldsymbol{\\Sigma}_{XY} \\mathbf{w}_y}{\\sqrt{\\mathbf{w}_x^T \\boldsymbol{\\Sigma}_{XX} \\mathbf{w}_x} \\sqrt{\\mathbf{w}_y^T \\boldsymbol{\\Sigma}_{YY} \\mathbf{w}_y}}$$\n",
        "\n",
        "#### 5.6.2 Multi-Modal Generative Models\n",
        "\n",
        "Cross-covariance captures relationships between different modalities (text-image, audio-video).\n",
        "\n",
        "#### 5.6.3 Attention Mechanism Analysis\n",
        "\n",
        "Cross-covariance between query and key representations:\n",
        "$$\\boldsymbol{\\Sigma}_{QK} = \\text{Cov}(\\mathbf{Q}, \\mathbf{K})$$\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Correlation\n",
        "\n",
        "### 6.1 Pearson Correlation Coefficient\n",
        "\n",
        "**Definition**: Normalized covariance measuring linear relationship.\n",
        "\n",
        "$$\\rho_{XY} = \\text{Corr}(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} = \\frac{\\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)]}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}}$$\n",
        "\n",
        "**Range**: $-1 \\leq \\rho_{XY} \\leq 1$\n",
        "\n",
        "**Interpretation**:\n",
        "- $\\rho = 1$: Perfect positive linear relationship\n",
        "- $\\rho = -1$: Perfect negative linear relationship\n",
        "- $\\rho = 0$: No linear relationship (uncorrelated)\n",
        "\n",
        "### 6.2 Correlation Matrix\n",
        "\n",
        "$$\\mathbf{R} = \\mathbf{D}^{-1/2} \\boldsymbol{\\Sigma} \\mathbf{D}^{-1/2}$$\n",
        "\n",
        "where $\\mathbf{D} = \\text{diag}(\\sigma_1^2, \\sigma_2^2, ..., \\sigma_n^2)$\n",
        "\n",
        "**Element-wise**:\n",
        "$$R_{ij} = \\frac{\\Sigma_{ij}}{\\sqrt{\\Sigma_{ii} \\Sigma_{jj}}} = \\frac{\\text{Cov}(X_i, X_j)}{\\sigma_{X_i} \\sigma_{X_j}}$$\n",
        "\n",
        "**Properties**:\n",
        "- Diagonal elements: $R_{ii} = 1$\n",
        "- Off-diagonal: $-1 \\leq R_{ij} \\leq 1$\n",
        "- Symmetric: $\\mathbf{R} = \\mathbf{R}^T$\n",
        "- Positive semi-definite\n",
        "\n",
        "### 6.3 Cross-Correlation Matrix\n",
        "\n",
        "For random vectors $\\mathbf{X}$ and $\\mathbf{Y}$:\n",
        "\n",
        "$$\\mathbf{R}_{XY} = \\mathbf{D}_X^{-1/2} \\boldsymbol{\\Sigma}_{XY} \\mathbf{D}_Y^{-1/2}$$\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Auto-Correlation\n",
        "\n",
        "### 7.1 Definition\n",
        "\n",
        "**Auto-correlation** measures the correlation of a signal/process with a delayed version of itself.\n",
        "\n",
        "### 7.2 For Stochastic Processes\n",
        "\n",
        "#### 7.2.1 Auto-Correlation Function (ACF)\n",
        "\n",
        "$$R_{XX}(t_1, t_2) = \\mathbb{E}[X_{t_1} X_{t_2}]$$\n",
        "\n",
        "#### 7.2.2 Auto-Covariance Function\n",
        "\n",
        "$$C_{XX}(t_1, t_2) = \\mathbb{E}[(X_{t_1} - \\mu(t_1))(X_{t_2} - \\mu(t_2))] = R_{XX}(t_1, t_2) - \\mu(t_1)\\mu(t_2)$$\n",
        "\n",
        "### 7.3 For Stationary Processes\n",
        "\n",
        "**Wide-Sense Stationary (WSS)**: Statistics depend only on time difference.\n",
        "\n",
        "$$R_{XX}(\\tau) = \\mathbb{E}[X_t X_{t+\\tau}]$$\n",
        "\n",
        "$$C_{XX}(\\tau) = \\mathbb{E}[(X_t - \\mu)(X_{t+\\tau} - \\mu)] = R_{XX}(\\tau) - \\mu^2$$\n",
        "\n",
        "**Normalized Auto-Correlation**:\n",
        "$$\\rho_{XX}(\\tau) = \\frac{C_{XX}(\\tau)}{C_{XX}(0)} = \\frac{C_{XX}(\\tau)}{\\sigma^2}$$\n",
        "\n",
        "### 7.4 Properties of Auto-Correlation\n",
        "\n",
        "1. **Symmetry**: $R_{XX}(\\tau) = R_{XX}(-\\tau)$\n",
        "\n",
        "2. **Maximum at Zero**: $R_{XX}(0) \\geq |R_{XX}(\\tau)|$ for all $\\tau$\n",
        "\n",
        "3. **At Zero**: $R_{XX}(0) = \\mathbb{E}[X^2]$ (mean squared value)\n",
        "\n",
        "4. **Positive Semi-Definite**: The auto-correlation matrix is PSD\n",
        "\n",
        "5. **Wiener-Khinchin Theorem**: Power spectral density is Fourier transform of ACF:\n",
        "$$S_{XX}(f) = \\mathcal{F}\\{R_{XX}(\\tau)\\} = \\int_{-\\infty}^{\\infty} R_{XX}(\\tau) e^{-j2\\pi f \\tau} d\\tau$$\n",
        "\n",
        "### 7.5 Discrete Auto-Correlation\n",
        "\n",
        "For discrete signal $x[n]$:\n",
        "\n",
        "**Deterministic**:\n",
        "$$R_{xx}[k] = \\sum_{n=-\\infty}^{\\infty} x[n] x[n+k]$$\n",
        "\n",
        "**Normalized (Biased Estimator)**:\n",
        "$$\\hat{R}_{xx}[k] = \\frac{1}{N} \\sum_{n=0}^{N-1-|k|} x[n] x[n+|k|]$$\n",
        "\n",
        "**Unbiased Estimator**:\n",
        "$$\\hat{R}_{xx}[k] = \\frac{1}{N-|k|} \\sum_{n=0}^{N-1-|k|} x[n] x[n+|k|]$$\n",
        "\n",
        "### 7.6 Applications in Gen-AI\n",
        "\n",
        "#### 7.6.1 Temporal Modeling in Sequence Generation\n",
        "\n",
        "Auto-correlation reveals temporal dependencies:\n",
        "- High auto-correlation at lag $k$ → strong dependency between $x_t$ and $x_{t+k}$\n",
        "\n",
        "#### 7.6.2 Audio Generation\n",
        "\n",
        "$$R_{audio}(\\tau) = \\mathbb{E}[s(t) s(t+\\tau)]$$\n",
        "\n",
        "Captures periodic structure (pitch, rhythm).\n",
        "\n",
        "#### 7.6.3 Positional Encoding Analysis\n",
        "\n",
        "Analyze auto-correlation of positional embeddings:\n",
        "$$R_{PE}(\\Delta pos) = \\mathbb{E}[PE(pos) \\cdot PE(pos + \\Delta pos)]$$\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Cross-Correlation\n",
        "\n",
        "### 8.1 Definition\n",
        "\n",
        "**Cross-correlation** measures similarity between two signals as a function of displacement/lag.\n",
        "\n",
        "### 8.2 Continuous Cross-Correlation\n",
        "\n",
        "**For Functions**:\n",
        "$$(f \\star g)(\\tau) = \\int_{-\\infty}^{\\infty} \\overline{f(t)} g(t + \\tau) dt$$\n",
        "\n",
        "where $\\overline{f(t)}$ is complex conjugate (for real signals, just $f(t)$).\n",
        "\n",
        "**For Stochastic Processes**:\n",
        "$$R_{XY}(\\tau) = \\mathbb{E}[X_t Y_{t+\\tau}]$$\n",
        "\n",
        "### 8.3 Discrete Cross-Correlation\n",
        "\n",
        "$$(x \\star y)[k] = \\sum_{n=-\\infty}^{\\infty} x[n] y[n+k]$$\n",
        "\n",
        "**Finite Length Signals**:\n",
        "$$(x \\star y)[k] = \\sum_{n=0}^{N-1} x[n] y[n+k]$$\n",
        "\n",
        "### 8.4 Properties of Cross-Correlation\n",
        "\n",
        "1. **Relationship to Auto-Correlation**:\n",
        "$$R_{XX}(\\tau) = (x \\star x)(\\tau)$$\n",
        "\n",
        "2. **Non-Commutative**:\n",
        "$$R_{XY}(\\tau) \\neq R_{YX}(\\tau)$$\n",
        "$$R_{XY}(\\tau) = R_{YX}(-\\tau)$$\n",
        "\n",
        "3. **Relationship to Convolution**:\n",
        "$$(f \\star g)(\\tau) = (f(-t) * g)(\\tau)$$\n",
        "\n",
        "where $*$ denotes convolution.\n",
        "\n",
        "4. **Fourier Domain**:\n",
        "$$\\mathcal{F}\\{x \\star y\\} = \\overline{\\mathcal{F}\\{x\\}} \\cdot \\mathcal{F}\\{y\\}$$\n",
        "\n",
        "### 8.5 Normalized Cross-Correlation (NCC)\n",
        "\n",
        "$$\\text{NCC}(\\tau) = \\frac{(f \\star g)(\\tau)}{\\sqrt{\\sum f^2 \\cdot \\sum g^2}}$$\n",
        "\n",
        "**Range**: $-1 \\leq \\text{NCC} \\leq 1$\n",
        "\n",
        "### 8.6 Cross-Correlation Matrix (for Random Vectors)\n",
        "\n",
        "$$\\mathbf{R}_{XY} = \\mathbb{E}[\\mathbf{X}\\mathbf{Y}^T]$$\n",
        "\n",
        "**Relationship to Cross-Covariance**:\n",
        "$$\\boldsymbol{\\Sigma}_{XY} = \\mathbf{R}_{XY} - \\boldsymbol{\\mu}_X \\boldsymbol{\\mu}_Y^T$$\n",
        "\n",
        "### 8.7 Applications in Gen-AI\n",
        "\n",
        "#### 8.7.1 Template Matching in Vision\n",
        "\n",
        "$$\\text{Match}(i, j) = \\sum_{m,n} T(m, n) \\cdot I(i+m, j+n)$$\n",
        "\n",
        "where $T$ is template and $I$ is image.\n",
        "\n",
        "#### 8.7.2 Attention Mechanism\n",
        "\n",
        "Cross-correlation between queries and keys:\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "$QK^T$ represents cross-correlation/similarity.\n",
        "\n",
        "#### 8.7.3 Audio-Visual Alignment\n",
        "\n",
        "$$R_{AV}(\\tau) = \\mathbb{E}[A_t \\cdot V_{t+\\tau}]$$\n",
        "\n",
        "Find temporal offset between audio and video.\n",
        "\n",
        "#### 8.7.4 Text-Image Similarity (CLIP)\n",
        "\n",
        "$$\\text{similarity}(t, i) = \\frac{\\mathbf{e}_t^T \\mathbf{e}_i}{\\|\\mathbf{e}_t\\| \\|\\mathbf{e}_i\\|}$$\n",
        "\n",
        "Normalized cross-correlation between text and image embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Convolution\n",
        "\n",
        "### 9.1 Definition\n",
        "\n",
        "**Convolution** is a mathematical operation that combines two functions to produce a third function, expressing how the shape of one is modified by the other.\n",
        "\n",
        "### 9.2 Continuous Convolution\n",
        "\n",
        "$$(f * g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau) g(t - \\tau) d\\tau$$\n",
        "\n",
        "**Alternative Form**:\n",
        "$$(f * g)(t) = \\int_{-\\infty}^{\\infty} f(t - \\tau) g(\\tau) d\\tau$$\n",
        "\n",
        "### 9.3 Discrete Convolution\n",
        "\n",
        "$$(f * g)[n] = \\sum_{k=-\\infty}^{\\infty} f[k] g[n - k]$$\n",
        "\n",
        "**Finite Signals**:\n",
        "$$(f * g)[n] = \\sum_{k=0}^{K-1} f[k] g[n - k]$$\n",
        "\n",
        "where $K$ is kernel size.\n",
        "\n",
        "### 9.4 2D Convolution (Images)\n",
        "\n",
        "$$(I * K)(i, j) = \\sum_{m} \\sum_{n} I(i-m, j-n) K(m, n)$$\n",
        "\n",
        "**Cross-Correlation Form** (used in deep learning):\n",
        "$$(I \\star K)(i, j) = \\sum_{m} \\sum_{n} I(i+m, j+n) K(m, n)$$\n",
        "\n",
        "**Note**: Deep learning \"convolution\" is actually cross-correlation (no kernel flip).\n",
        "\n",
        "### 9.5 Properties of Convolution\n",
        "\n",
        "| Property | Mathematical Expression |\n",
        "|----------|------------------------|\n",
        "| **Commutativity** | $f * g = g * f$ |\n",
        "| **Associativity** | $(f * g) * h = f * (g * h)$ |\n",
        "| **Distributivity** | $f * (g + h) = f * g + f * h$ |\n",
        "| **Scalar Multiplication** | $a(f * g) = (af) * g = f * (ag)$ |\n",
        "| **Identity** | $f * \\delta = f$ where $\\delta$ is Dirac delta |\n",
        "| **Differentiation** | $\\frac{d}{dt}(f * g) = \\frac{df}{dt} * g = f * \\frac{dg}{dt}$ |\n",
        "\n",
        "### 9.6 Convolution Theorem\n",
        "\n",
        "**Statement**: Convolution in time/spatial domain equals multiplication in frequency domain.\n",
        "\n",
        "$$\\mathcal{F}\\{f * g\\} = \\mathcal{F}\\{f\\} \\cdot \\mathcal{F}\\{g\\}$$\n",
        "\n",
        "$$f * g = \\mathcal{F}^{-1}\\{\\mathcal{F}\\{f\\} \\cdot \\mathcal{F}\\{g\\}\\}$$\n",
        "\n",
        "**Dual**:\n",
        "$$\\mathcal{F}\\{f \\cdot g\\} = \\mathcal{F}\\{f\\} * \\mathcal{F}\\{g\\}$$\n",
        "\n",
        "### 9.7 Relationship: Convolution vs Cross-Correlation\n",
        "\n",
        "$$(f * g)(t) = (f(-t) \\star g)(t) = (f \\star g(-t))(t)$$\n",
        "\n",
        "**Implication**: Convolution = Cross-correlation with flipped kernel\n",
        "\n",
        "```\n",
        "Cross-Correlation:    Convolution:\n",
        "K = [1 2 3]           K_flipped = [3 2 1]\n",
        "    \n",
        "   Signal: [a b c d e]\n",
        "   \n",
        "Cross-Corr: 1·a + 2·b + 3·c    Conv: 3·a + 2·b + 1·c\n",
        "```\n",
        "\n",
        "### 9.8 Types of Convolution in Deep Learning\n",
        "\n",
        "#### 9.8.1 Standard 2D Convolution\n",
        "\n",
        "**Input**: $\\mathbf{X} \\in \\mathbb{R}^{C_{in} \\times H \\times W}$\n",
        "**Kernel**: $\\mathbf{K} \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times k_h \\times k_w}$\n",
        "**Output**: $\\mathbf{Y} \\in \\mathbb{R}^{C_{out} \\times H' \\times W'}$\n",
        "\n",
        "$$Y_{c_{out}, i, j} = \\sum_{c_{in}=1}^{C_{in}} \\sum_{m=0}^{k_h-1} \\sum_{n=0}^{k_w-1} K_{c_{out}, c_{in}, m, n} \\cdot X_{c_{in}, i+m, j+n}$$\n",
        "\n",
        "**Output Size**:\n",
        "$$H' = \\left\\lfloor \\frac{H + 2P - k_h}{S} \\right\\rfloor + 1$$\n",
        "$$W' = \\left\\lfloor \\frac{W + 2P - k_w}{S} \\right\\rfloor + 1$$\n",
        "\n",
        "where $P$ = padding, $S$ = stride.\n",
        "\n",
        "#### 9.8.2 Depthwise Separable Convolution\n",
        "\n",
        "**Depthwise**: Apply separate kernel per channel\n",
        "$$Y^{(c)}_{i,j} = \\sum_{m,n} K^{(c)}_{m,n} \\cdot X^{(c)}_{i+m, j+n}$$\n",
        "\n",
        "**Pointwise**: 1×1 convolution to mix channels\n",
        "$$Z_{c_{out}, i, j} = \\sum_{c=1}^{C_{in}} W_{c_{out}, c} \\cdot Y_{c, i, j}$$\n",
        "\n",
        "**Parameter Reduction**:\n",
        "- Standard: $C_{out} \\times C_{in} \\times k^2$\n",
        "- Depthwise Separable: $C_{in} \\times k^2 + C_{out} \\times C_{in}$\n",
        "\n",
        "#### 9.8.3 Transposed Convolution (Deconvolution)\n",
        "\n",
        "Used in generators for upsampling:\n",
        "\n",
        "$$Y = \\mathbf{K}^T \\mathbf{X}$$\n",
        "\n",
        "**Output Size**:\n",
        "$$H' = (H - 1) \\times S - 2P + k_h$$\n",
        "\n",
        "#### 9.8.4 Dilated (Atrous) Convolution\n",
        "\n",
        "Kernel with holes (dilation rate $d$):\n",
        "\n",
        "$$Y_{i,j} = \\sum_{m,n} K_{m,n} \\cdot X_{i + d \\cdot m, j + d \\cdot n}$$\n",
        "\n",
        "**Effective Kernel Size**: $k' = k + (k-1)(d-1)$\n",
        "\n",
        "**Receptive Field**: Grows exponentially with stacked dilated convolutions.\n",
        "\n",
        "### 9.9 1D Convolution in Sequence Models\n",
        "\n",
        "**Input**: $\\mathbf{X} \\in \\mathbb{R}^{C_{in} \\times L}$\n",
        "**Kernel**: $\\mathbf{K} \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times k}$\n",
        "\n",
        "$$Y_{c_{out}, t} = \\sum_{c_{in}=1}^{C_{in}} \\sum_{i=0}^{k-1} K_{c_{out}, c_{in}, i} \\cdot X_{c_{in}, t+i}$$\n",
        "\n",
        "**Causal Convolution** (for autoregressive models):\n",
        "$$Y_t = \\sum_{i=0}^{k-1} K_i \\cdot X_{t-i}$$\n",
        "\n",
        "Only uses past and current inputs (no future leakage).\n",
        "\n",
        "### 9.10 Convolution in Generative AI Architectures\n",
        "\n",
        "#### 9.10.1 U-Net (Diffusion Models)\n",
        "\n",
        "```\n",
        "Encoder (Downsampling):\n",
        "    Conv → Conv → Pool → Conv → Conv → Pool → ...\n",
        "\n",
        "Decoder (Upsampling):  \n",
        "    TransposedConv → Concat(skip) → Conv → Conv → ...\n",
        "```\n",
        "\n",
        "#### 9.10.2 StyleGAN Generator\n",
        "\n",
        "Progressive growing with transposed convolutions:\n",
        "$$\\mathbf{Y} = \\text{Conv}^T(\\text{AdaIN}(\\mathbf{X}, \\mathbf{style}))$$\n",
        "\n",
        "#### 9.10.3 WaveNet (Audio Generation)\n",
        "\n",
        "Dilated causal convolutions:\n",
        "$$Y_t = \\sum_{k=0}^{K-1} W_k \\cdot X_{t - d \\cdot k}$$\n",
        "\n",
        "with exponentially increasing dilation: $d \\in \\{1, 2, 4, 8, ..., 512\\}$\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Relationship Summary\n",
        "\n",
        "### 10.1 Mathematical Relationships\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│                    RELATIONSHIP DIAGRAM                         │\n",
        "├─────────────────────────────────────────────────────────────────┤\n",
        "│                                                                 │\n",
        "│   EXPECTATION: E[X]                                            │\n",
        "│        │                                                       │\n",
        "│        ▼                                                       │\n",
        "│   RAW MOMENTS: μ'_n = E[X^n]                                   │\n",
        "│        │                                                       │\n",
        "│        ▼                                                       │\n",
        "│   CENTRAL MOMENTS: μ_n = E[(X - E[X])^n]                       │\n",
        "│        │                                                       │\n",
        "│        ├──► VARIANCE: σ² = μ_2                                 │\n",
        "│        │                                                       │\n",
        "│        └──► SKEWNESS, KURTOSIS                                 │\n",
        "│                                                                 │\n",
        "│   COVARIANCE: Cov(X,Y) = E[(X-μ_X)(Y-μ_Y)]                     │\n",
        "│        │                                                       │\n",
        "│        ├──► CORRELATION: ρ = Cov(X,Y)/(σ_X σ_Y)                │\n",
        "│        │                                                       │\n",
        "│        └──► AUTO-CORRELATION: R_XX(τ) = E[X_t X_{t+τ}]         │\n",
        "│             CROSS-CORRELATION: R_XY(τ) = E[X_t Y_{t+τ}]        │\n",
        "│                                                                 │\n",
        "│   CONVOLUTION: (f * g)(t) = ∫ f(τ)g(t-τ)dτ                     │\n",
        "│        │                                                       │\n",
        "│        └──► Cross-Corr = Conv with flipped kernel               │\n",
        "│             (f ⋆ g)(t) = (f * g(-t))(t)                        │\n",
        "│                                                                 │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### 10.2 Fourier Domain Connections\n",
        "\n",
        "| Operation | Time Domain | Frequency Domain |\n",
        "|-----------|-------------|------------------|\n",
        "| Convolution | $f * g$ | $F \\cdot G$ |\n",
        "| Cross-Correlation | $f \\star g$ | $\\bar{F} \\cdot G$ |\n",
        "| Auto-Correlation | $f \\star f$ | $|F|^2$ (Power Spectrum) |\n",
        "| Product | $f \\cdot g$ | $F * G$ |\n",
        "\n",
        "### 10.3 Summary Table for Gen-AI Applications\n",
        "\n",
        "| Concept | Definition | Gen-AI Application |\n",
        "|---------|------------|-------------------|\n",
        "| **Expectation** | $\\mathbb{E}[X] = \\int x \\cdot p(x) dx$ | Loss functions, training objectives |\n",
        "| **Variance** | $\\text{Var}(X) = \\mathbb{E}[(X-\\mu)^2]$ | Normalization, uncertainty quantification |\n",
        "| **Covariance** | $\\text{Cov}(X,Y) = \\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)]$ | Multivariate Gaussians, VAE |\n",
        "| **Cross-Covariance** | $\\boldsymbol{\\Sigma}_{XY} = \\mathbb{E}[(\\mathbf{X}-\\boldsymbol{\\mu}_X)(\\mathbf{Y}-\\boldsymbol{\\mu}_Y)^T]$ | Multi-modal learning, CCA |\n",
        "| **Auto-Correlation** | $R_{XX}(\\tau) = \\mathbb{E}[X_t X_{t+\\tau}]$ | Temporal modeling, audio/speech |\n",
        "| **Cross-Correlation** | $R_{XY}(\\tau) = \\mathbb{E}[X_t Y_{t+\\tau}]$ | Attention, similarity matching |\n",
        "| **Convolution** | $(f*g)(t) = \\int f(\\tau)g(t-\\tau)d\\tau$ | CNNs, U-Net, generators |\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Computational Considerations\n",
        "\n",
        "### 11.1 Efficient Convolution via FFT\n",
        "\n",
        "**Direct Convolution**: $O(N \\cdot K)$ for signal length $N$, kernel size $K$\n",
        "\n",
        "**FFT-based Convolution**: $O(N \\log N)$\n",
        "\n",
        "$$f * g = \\mathcal{F}^{-1}\\{\\mathcal{F}\\{f\\} \\cdot \\mathcal{F}\\{g\\}\\}$$\n",
        "\n",
        "Efficient when $K > \\log N$.\n",
        "\n",
        "### 11.2 Batch Statistics Computation\n",
        "\n",
        "**Batch Mean**:\n",
        "$$\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i$$\n",
        "\n",
        "**Batch Variance**:\n",
        "$$\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2$$\n",
        "\n",
        "**Welford's Online Algorithm** (numerically stable):\n",
        "$$\\mu_n = \\mu_{n-1} + \\frac{x_n - \\mu_{n-1}}{n}$$\n",
        "$$M_n = M_{n-1} + (x_n - \\mu_{n-1})(x_n - \\mu_n)$$\n",
        "$$\\sigma_n^2 = \\frac{M_n}{n}$$\n",
        "\n",
        "### 11.3 Covariance Matrix Estimation\n",
        "\n",
        "**Sample Covariance**:\n",
        "$$\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N-1} \\sum_{i=1}^{N} (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})^T$$\n",
        "\n",
        "**Matrix Form**:\n",
        "$$\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N-1} \\mathbf{X}_c^T \\mathbf{X}_c$$\n",
        "\n",
        "where $\\mathbf{X}_c$ is centered data matrix."
      ],
      "metadata": {
        "id": "8ZIlhXr06Xoi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5624fTSkWb7h"
      },
      "outputs": [],
      "source": []
    }
  ]
}